{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS\n",
    "\n",
    "- hide: true\n",
    "- toc: true\n",
    "- comments: true\n",
    "- categories: [tools]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frequently used interaction patterns with AWS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To create a new bucket, use `aws s3 mb bucketname`.\n",
    "\n",
    "- To add a subfolder to a bucket, use `aws s3api put-object --bucket bucketname\n",
    "  --key foldername`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "- There are multiple ways to access your AWS account. I store config and credential files in `~/.aws` as discussed [here](https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html). AWS access methods find these files automatically so I don't have to worry about that.\n",
    "\n",
    "- What I do have to worry about is choosing the appropriate profile depending on what AWS account I want to interact with (e.g. my personal one or one for work). This is different for each library, so I cover this below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `s3fs`\n",
    "\n",
    "Built by people at Dask, [`s3fs`](https://github.com/dask/s3fs) is built on top of `botocore` and provides a convenient way to interact with S3. It can read and -- I think -- write data, but there are easier ways to do that, and I use the library mainly to navigate buckets and list content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Navigate buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fgu-samples/transactions.parquet']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import s3fs\n",
    "\n",
    "# establish connection\n",
    "fs = s3fs.S3FileSystem()\n",
    "\n",
    "# count items in s3 root bucket\n",
    "fs.ls('/fgu-samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To choose a profile other than `default`, use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# connect using a different profile\n",
    "fs = s3fs.S3FileSystem(profile='tracker-fgu')\n",
    "len(fs.ls('/'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Â Read and write directly from `Pandas`\n",
    "\n",
    "- Pandas can read and write files to and from S3 directly if you provide the file name as `s3://<bucket>/<filename>`.\n",
    "\n",
    "- By default, `Pandas` uses the default profile to access S3. Recent versions of `Pandas` have a `storage_options` parameter that can be used to provide, among other things, a profile name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1168943, 21)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# read using default profile \n",
    "\n",
    "fp = 's3://fgu-samples/transactions.parquet'\n",
    "df = pd.read_parquet(fp)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9388334, 23)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read using custom profile\n",
    "\n",
    "fp = 's3://temp-mdb/data_XX7.parquet'\n",
    "df = pd.read_parquet(fp, storage_options = dict(profile='tracker-fgu'))\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works well for simple jobs, but in a large project, passing the profile information to each read and write call is cumbersome and ugly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple improvement using `functools.partial` \n",
    "\n",
    "`functools.partial`provides a simple solution, as it allows me to create a custom function with a frozen storage options argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9388334, 23)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import functools\n",
    "\n",
    "options = dict(storage_options=dict(profile='tracker-fgu'))\n",
    "read_parquet_s3 = functools.partial(pd.read_parquet, **options)\n",
    "df = read_parquet_s3(fp)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More flexible solution with custom function\n",
    "\n",
    "Often, I run projects on my Mac for testing and a virtual machine to run the full code. In this case, I need a way to automatically provide the correct profile name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['raw-mdb/data_777.csv', 'raw-mdb/data_X77.csv']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3 = s3fs.S3FileSystem(profile='tracker-fgu')\n",
    "s3.ls('/raw-mdb/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'options'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-09c6801ecbd9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m's3://raw-mdb/data_777.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0ms3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-09c6801ecbd9>\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'options'"
     ]
    }
   ],
   "source": [
    "import functools\n",
    "import platform\n",
    "\n",
    "def get_aws_profile():\n",
    "    \"\"\"\n",
    "    Return access point specific AWS profile to use for S3 access.\n",
    "    \"\"\"\n",
    "    if platform.node() == 'FabsMacBook.local':\n",
    "        profile = 'tracker-fgu'\n",
    "    else:\n",
    "        profile = 'default'\n",
    "\n",
    "    return profile\n",
    "\n",
    "\n",
    "class s3:\n",
    "    \"\"\"\n",
    "    Create read and write functions with frozen AWS profile.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.profile = get_aws_profile()\n",
    "        self.options = dict(storage_options=dict(profile=self.profile))\n",
    "        \n",
    "    def read_csv(self):\n",
    "        return functools.partial(pd.read_csv, **self.options)\n",
    "    \n",
    "    \n",
    "\n",
    "def make_s3_funcs():\n",
    "    \"\"\"\n",
    "    Return readers and writers with frozen AWS profile name.\n",
    "    \"\"\"\n",
    "    # identify required profile (depends on project)\n",
    "    if platform.node() == 'FabsMacBook.local':\n",
    "        profile = 'tracker-fgu'\n",
    "    else:\n",
    "        profile = 'default'\n",
    "        \n",
    "    # create partial readers and writers\n",
    "    options = dict(storage_options=dict(profile=profile))\n",
    "    read_csv_s3 = functools.partial(pd.read_csv, **options)\n",
    "    write_csv_s3 = functools.partial(pd.write_csv, **options)\n",
    "    read_parquet_s3 = functools.partial(pd.read_parquet, **options)\n",
    "    write_parquet_s3 = functools.partial(pd.write_parquet, **options)\n",
    "    \n",
    "fp = 's3://raw-mdb/data_777.csv'\n",
    "    \n",
    "s3.read_csv(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is not ideal, as it requires cumbersome unpacking of return. Maybe using decorator is better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `awswrangler`\n",
    "\n",
    "A new [library](https://github.com/awslabs/aws-data-wrangler) from AWS labs for Pandas interaction with a number of AWS services. Looks very promising, but haven't had any use for it thus far."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blog",
   "language": "python",
   "name": "blog"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
