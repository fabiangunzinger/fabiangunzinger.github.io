{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"AWS\"\n",
    "date: \"2020-03-14\"\n",
    "tags:\n",
    "    - tools\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frequently used interaction patterns with AWS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To create a new bucket, use `aws s3 mb bucketname`.\n",
    "\n",
    "- To add a subfolder to a bucket, use `aws s3api put-object --bucket bucketname\n",
    "  --key foldername`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "- There are multiple ways to access your AWS account. I store config and credential files in `~/.aws` as discussed [here](https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html). AWS access methods find these files automatically so I don't have to worry about that.\n",
    "\n",
    "- What I do have to worry about is choosing the appropriate profile depending on what AWS account I want to interact with (e.g. my personal one or one for work). This is different for each library, so I cover this below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `s3fs`\n",
    "\n",
    "Built by people at Dask, [`s3fs`](https://github.com/dask/s3fs) is built on top of `botocore` and provides a convenient way to interact with S3. It can read and -- I think -- write data, but there are easier ways to do that, and I use the library mainly to navigate buckets and list content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Navigate buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import s3fs\n",
    "\n",
    "# establish connection\n",
    "fs = s3fs.S3FileSystem()\n",
    "\n",
    "# count items in s3 root bucket\n",
    "fs.ls('/fgu-samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To choose a profile other than `default`, use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect using a different profile\n",
    "fs = s3fs.S3FileSystem(profile='tracker-fgu')\n",
    "len(fs.ls('/'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Â Read and write directly from `Pandas`\n",
    "\n",
    "- Pandas can read and write files to and from S3 directly if you provide the file name as `s3://<bucket>/<filename>`.\n",
    "\n",
    "- By default, `Pandas` uses the default profile to access S3. Recent versions of `Pandas` have a `storage_options` parameter that can be used to provide, among other things, a profile name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# read using default profile \n",
    "\n",
    "fp = 's3://fgu-samples/transactions.parquet'\n",
    "df = pd.read_parquet(fp)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read using custom profile\n",
    "\n",
    "fp = 's3://temp-mdb/data_XX7.parquet'\n",
    "df = pd.read_parquet(fp, storage_options = dict(profile='tracker-fgu'))\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works well for simple jobs, but in a large project, passing the profile information to each read and write call is cumbersome and ugly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple improvement using `functools.partial` \n",
    "\n",
    "`functools.partial`provides a simple solution, as it allows me to create a custom function with a frozen storage options argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "options = dict(storage_options=dict(profile='tracker-fgu'))\n",
    "read_parquet_s3 = functools.partial(pd.read_parquet, **options)\n",
    "df = read_parquet_s3(fp)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More flexible solution with custom function\n",
    "\n",
    "Often, I run projects on my Mac for testing and a virtual machine to run the full code. In this case, I need a way to automatically provide the correct profile name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = s3fs.S3FileSystem(profile='tracker-fgu')\n",
    "s3.ls('/raw-mdb/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import platform\n",
    "\n",
    "def get_aws_profile():\n",
    "    \"\"\"\n",
    "    Return access point specific AWS profile to use for S3 access.\n",
    "    \"\"\"\n",
    "    if platform.node() == 'FabsMacBook.local':\n",
    "        profile = 'tracker-fgu'\n",
    "    else:\n",
    "        profile = 'default'\n",
    "\n",
    "    return profile\n",
    "\n",
    "\n",
    "class s3:\n",
    "    \"\"\"\n",
    "    Create read and write functions with frozen AWS profile.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.profile = get_aws_profile()\n",
    "        self.options = dict(storage_options=dict(profile=self.profile))\n",
    "        \n",
    "    def read_csv(self):\n",
    "        return functools.partial(pd.read_csv, **self.options)\n",
    "    \n",
    "    \n",
    "\n",
    "def make_s3_funcs():\n",
    "    \"\"\"\n",
    "    Return readers and writers with frozen AWS profile name.\n",
    "    \"\"\"\n",
    "    # identify required profile (depends on project)\n",
    "    if platform.node() == 'FabsMacBook.local':\n",
    "        profile = 'tracker-fgu'\n",
    "    else:\n",
    "        profile = 'default'\n",
    "        \n",
    "    # create partial readers and writers\n",
    "    options = dict(storage_options=dict(profile=profile))\n",
    "    read_csv_s3 = functools.partial(pd.read_csv, **options)\n",
    "    write_csv_s3 = functools.partial(pd.write_csv, **options)\n",
    "    read_parquet_s3 = functools.partial(pd.read_parquet, **options)\n",
    "    write_parquet_s3 = functools.partial(pd.write_parquet, **options)\n",
    "    \n",
    "fp = 's3://raw-mdb/data_777.csv'\n",
    "    \n",
    "s3.read_csv(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is not ideal, as it requires cumbersome unpacking of return. Maybe using decorator is better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `awswrangler`\n",
    "\n",
    "A new [library](https://github.com/awslabs/aws-data-wrangler) from AWS labs for Pandas interaction with a number of AWS services. Looks very promising, but haven't had any use for it thus far."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blog",
   "language": "python",
   "name": "blog"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
