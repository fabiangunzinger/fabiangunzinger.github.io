{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Testing and validating\"\n",
    "date: \"2021-08-12\"\n",
    "tags:\n",
    "    - python, datascience\n",
    "execute:\n",
    "    enabled: false\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes on testing and data validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "What I currently do:\n",
    "\n",
    "- I use `assert` to test small ad-hoc pieces of code, `pytest` to test crucial pieces of code, and a series of validating functions that check certain assumptions about the data \n",
    "\n",
    "What I need:\n",
    "\n",
    "- A process to ensure that my data preprocessing pipeline produces the intended data.\n",
    "\n",
    "- I still don't fully understand how good data scientists test their code. Unit testing seems incomplete because while it ensures that a piece of code works as expected, the main challenge when working with large datasets is often that there are special cases in the data that I don't know in advance and can't think of. To catch these, I need to perform tests on the full data. In addition to that, manually creating a dataframe for testing is a huge pain when I need to test functions that create more complex data patterns (e.g. checking whether, in a financial transactions dataset, certain individuals in the data have a at least a certain number of monthly transactions for a certain type of bank account).\n",
    "\n",
    "- I currently mainly use validating functions that operate on the entire dataset at the end of the pipeline (e.g. transaction data production in entropy project), which already has proven invaluable in catching errors. Using a decorator to add each validator function to a list of validators, and then running the data through each validator in that list works well and is fairly convenient.\n",
    "\n",
    "- `pandera` seems potentially useful in that the defined schema can be used both to validate data and -- excitingly -- can also be used to generate sample datasets for `hypothesis` and `pytest`, which could go a long way towards solving the above problem. But specifying a data schema for a non-trivial dataset is not easy, and I can't see how to write one for a dataset like MDB, where I need constraints such as a certain number of financial accounts of a certain type per user. So, for now, I just use my own validation functions. The testing branch in the entropy project has a `schema.py` file that expriments with the library.\n",
    "\n",
    "- [This](https://www.peterbaumgartner.com/blog/testing-for-data-science/?utm_campaign=Data_Elixir&utm_source=Data_Elixir_368/) article has been very useful, suggesting the following approach to testing: `assert` statements for ad-hoc pieces of code in Jupyter Lab, `pytest` for pieces of code others user, `hypothesis` for code that operates on the data, and `pandera` or other validator libraries for overall data validation. I basically do the first and last of these, and am still looking for ways to do "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `pytest` notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic test for return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_int(s):\n",
    "    return int(s.replace(\",\", \"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very basic test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_convert_to_int():\n",
    "    assert convert_to_int(\"1,200\") == 1200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More transparent test with message, which will show when AssertionError is raised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_convert_to_int():\n",
    "    actual = convert_to_int(\"1,200\")\n",
    "    expected = 1200\n",
    "    message = f\"convert_to_int('1,200') returned {actual} instead of {expected}.\"\n",
    "    assert actual == expected, message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Careful with floats: because of this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.1 + 0.1 + 0.1 == 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.1 + 0.1 + 0.1 == pytest.approx(0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing for exceptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use context manager that silences expected error if raised within context and raises an assertion error if expected error isn't raised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pytest.raises(ValueError):\n",
    "    raise ValueError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "Failed",
     "evalue": "DID NOT RAISE <class 'ValueError'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFailed\u001b[0m                                    Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/xg/n9p73cf50s52twlnz7z778vr0000gn/T/ipykernel_3961/1768107249.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mpytest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraises\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/blog/lib/python3.9/site-packages/_pytest/outcomes.py\u001b[0m in \u001b[0;36mfail\u001b[0;34m(msg, pytrace)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \"\"\"\n\u001b[1;32m    152\u001b[0m     \u001b[0m__tracebackhide__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mFailed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpytrace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpytrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFailed\u001b[0m: DID NOT RAISE <class 'ValueError'>"
     ]
    }
   ],
   "source": [
    "with pytest.raises(ValueError):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def incrementer(x):\n",
    "    if isinstance(x, int):\n",
    "        return x + 1\n",
    "    elif isinstance(x, str):\n",
    "        raise TypeError(\"Please enter a number\")\n",
    "\n",
    "\n",
    "def test_valueerror_on_string():\n",
    "    example_argument = \"hello\"\n",
    "    with pytest.raises(TypeError):\n",
    "        incrementer(example_argument)\n",
    "\n",
    "\n",
    "test_valueerror_on_string()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test for correct error message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_valueerror_on_string():\n",
    "    example_argument = \"hello\"\n",
    "    with pytest.raises(TypeError) as exception_info:\n",
    "        incrementer(example_argument)\n",
    "    assert exception_info.match(\"Please enter a number\")\n",
    "\n",
    "\n",
    "test_valueerror_on_string()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's a \"well-tested\" function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Argument types:\n",
    "- Bad arguments\n",
    "    - Examples: incomplete args, wrong dimensions, wrong type, etc.\n",
    "    - Return value: exception\n",
    "- Special arguments\n",
    "    - Examples: values triggering special logic, boundary value (value between bad and good arguments and before or after values that raise special logic)\n",
    "    - Return value: expected value\n",
    "- Normal arguments\n",
    "    - Examples: All other values, test 2 or 3\n",
    "    - Return value: expected value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keeping tests organised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principles to follow:\n",
    "\n",
    "- Mirror structure of `src` directory in `tests` directory\n",
    "- Name test modules as `test_<name of src module>`\n",
    "- Within test module, collect all tests for a single function in a class named `TestNameOfFunction` (from DataCamp: 'Test classes are containers inside test modules. They help separate tests for different functions within the test module, and serve as a structuring tool in the pytest framework.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test class layout\n",
    "\n",
    "\n",
    "class TestNameOfFunction(object):\n",
    "    def test_first_thing(self):\n",
    "        pass\n",
    "\n",
    "    def test_second_thing(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Marking tests as expected to fail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes we might want to differentiate between failing code and tests that we know won't run yet or under certain conditions (e.g. we might follow TDD and haven't written a test yet, or we know a function only runs in Python 3). In this case, we can apply decorators to either functions or classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expect to fail always (e.g. because not implemented yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestNameOfFunction(object):\n",
    "    @pytest.mark.xfail\n",
    "    def test_first_thing(self):\n",
    "        pass\n",
    "\n",
    "    @pytest.mark.xfail(reason=\"Not implemented yet.\")\n",
    "    def test_first_thing(self):\n",
    "        \"\"\"With optional reason arg.\"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "# or\n",
    "\n",
    "\n",
    "@pytest.mark.xfail(reason=\"Not implemented yet.\")\n",
    "class TestNameOfFunction(object):\n",
    "    def test_first_thing(self):\n",
    "        pass\n",
    "\n",
    "    def test_first_thing(self):\n",
    "        \"\"\"With optional reason arg.\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expect to fail under certain conditions (e.g. certain Python versions, operating systems, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "\n",
    "class TestNameOfFunction(object):\n",
    "    @pytest.mark.skipif(sys.version_info < (3, 0), reason=\"Requires Python 3\")\n",
    "    def test_first_thing(self):\n",
    "        \"\"\"Only runs in Python 3.\"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "# or\n",
    "\n",
    "\n",
    "@pytest.mark.skipif(sys.version_info < (3, 0), reason=\"Requires Python 3\")\n",
    "class TestNameOfFunction(object):\n",
    "    def test_first_thing(self):\n",
    "        \"\"\"Only runs in Python 3.\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running pytests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `pytest` runs all tests\n",
    "- `pytest -x` stops after first failure\n",
    "- `pytest <path to test module>` runs all tests in test module\n",
    "- `pytest <path to test module>::<test class name>` runs all tests in test module with specified node id\n",
    "- `pytest <path to test module>::<test class name>::<test name>` runs test with specified node id\n",
    "- `pytest -k <patter>` runds tests that fit pattern\n",
    "    - `pytest -k <TestNameOfFunction>` runs all tests in specified class\n",
    "    - `pytest -k <NameOf and not second thing>` runs all tests in specified class except for `test_second_thing`\n",
    "- `pytest -r` show reasons\n",
    "- `pytest -rs` show reasons for skipped tests\n",
    "- `pytest -rx` show reasons for xfailed tests\n",
    "- `pytest -rsx` show reasons for skipped and xfailed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixtures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create raw and clean data files in fixture\n",
    "@pytest.fixture\n",
    "def raw_and_clean_data():\n",
    "    raw_path = \"raw.csv\"\n",
    "    clean_path = \"clean.csv\"\n",
    "    with open(raw_path, \"w\") as f:\n",
    "        f.write(\"1000, 40\\n\" \"2000, 50\\n\")\n",
    "    yield raw_path, clean_path\n",
    "\n",
    "    # teardown code so we start with clean env in next test\n",
    "    os.remove(raw_path)\n",
    "    os.remove(clean_path)\n",
    "\n",
    "\n",
    "# use fixture in test\n",
    "def test_on_raw_data(raw_and_clean_data):\n",
    "    raw_path, clean_path = raw_and_clean_data\n",
    "    preprocess(raw_path, clean_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful alternative using `tempdir()` and fixture chaining:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pytest' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/xg/n9p73cf50s52twlnz7z778vr0000gn/T/ipykernel_2101/1056953037.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0mpytest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfixture\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mraw_and_clean_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtempdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mraw_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtempdir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'raw.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mclean_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtempdir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'clean.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pytest' is not defined"
     ]
    }
   ],
   "source": [
    "@pytest.fixture\n",
    "def raw_and_clean_data(tempdir):\n",
    "    raw_path = tempdir.join(\"raw.csv\")\n",
    "    clean_path = tempdir.join(\"clean.csv\")\n",
    "    with open(raw_path, \"w\") as f:\n",
    "        f.write(\"1000, 40\\n\" \"2000, 50\\n\")\n",
    "    yield raw_path, clean_path\n",
    "\n",
    "    # no teardown needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mocking\n",
    "\n",
    "Testing functions independently of dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test models, use toy datasets for which I know the correct results and perform sanity-checks using assertions I can know."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests for training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import train_model\n",
    "\n",
    "\n",
    "def test_on_linear_data():\n",
    "    \"\"\"Can easily predict results precisely.\"\"\"\n",
    "    test_arg = np.array([[1, 3], [2, 5], [3, 7]])\n",
    "    expected_slope = 2\n",
    "    expected_intercept = 1\n",
    "\n",
    "    slope, intercept = train_model(test_arg)\n",
    "    assert slope == pytest.approx(expected_slope)\n",
    "    assert intercept == pytest.approx(expected_intercept)\n",
    "\n",
    "\n",
    "def test_on_positively_correlated_data():\n",
    "    \"\"\"Cannot easily predict result precisely,\n",
    "    but can still assert that slope is positive\n",
    "    as a sanity-check.\n",
    "    \"\"\"\n",
    "    test_arg = np.array([[1, 4], [2, 3], [4, 8], [3, 7]])\n",
    "    slope, intercept = train_model(test_arg)\n",
    "    assert slope > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests for final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_test(test_set, slope, intercept):\n",
    "    \"\"\"Assert that R^2 is between 0 and 1.\"\"\"\n",
    "    rsq = ...\n",
    "    assert 0 <= rsq <= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing plots\n",
    "\n",
    "Overall approach:\n",
    "1. Create baseline plot using plotting function and store as PNG image\n",
    "2. Test plotting function and compare to baseline\n",
    "\n",
    "Install `pytest-mpl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_best_fit_line(slope, intercept, x_array, y_array, title):\n",
    "    \"\"\"Plotting function to be tested.\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "@pytest.mark.mpl_image_compare\n",
    "def test_plot_for_linear_data():\n",
    "    \"\"\"Testing function.\n",
    "    Under the hood, creates baseline and comparisons.\n",
    "    \"\"\"\n",
    "    slope = 2\n",
    "    intercept = 1\n",
    "    x_array = np.array([1, 2, 3])\n",
    "    y_array = np.array([3, 5, 7])\n",
    "    title = \"Test plot for linear data\"\n",
    "    return plot_best_fit_line(slope, intercept, x_array, y_array, title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "baseline image needs to be stored in `baseline` subfolder of the plot module testing directory.\n",
    "\n",
    "To create baseline image, do following:\n",
    "\n",
    "`>pytest -k 'test_plot_for_linear_data' --mpl-generate-path <path-to-baseline-folder>`\n",
    "\n",
    "To compare future tests with baseline image run:\n",
    "\n",
    "`> pytest -k 'test_plot_for_linear_data' --mpl`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Travis CI](https://www.travis-ci.com)\n",
    "- [Python stuff](https://docs.travis-ci.com/user/languages/python/)\n",
    "- [Using Conda](https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/use-conda-with-travis-ci.html)\n",
    "\n",
    "- [Travis CI build info](https://app.travis-ci.com/github/fabiangunzinger/entropy/jobs/545437220)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "- [Peter Baumgartner, Ways I use testing as a data scientist](https://www.peterbaumgartner.com/blog/testing-for-data-science/?utm_campaign=Data_Elixir&utm_source=Data_Elixir_368/)\n",
    "- [Code from DataCamp course](https://github.com/gutfeeling/univariate-linear-regression)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sandbox",
   "language": "python",
   "name": "sandbox"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
