[{"categories":["growth"],"content":"My wife and I are about to become parents for the first time. In this space, I want to collect a limited number of core pieces of advice I want to remember. They are all from Philippa Perry‚Äôs wonderful book The book you wish your parents had read, which I can‚Äôt recommend highly enough to any parent. Stuff I want to remember: A child is not a thing I need to manage and train to behave well, but an individual I‚Äôm building a relationship with. All behaviour is communication. Instead of reacting to the behaviour, try to understand the feelings or needs that cause it, and help my child identify and articulate them, so that they learn doing it themselves. To help me do that, I want to get into the habit of seeing the world from my child‚Äôs perspective. I want to be a container for my child‚Äôs feelings ‚Äì to accept all their feelings, validate them, and help them identify and express them. I want to talk to my child about what is going to happen so they can mentally prepare for it instead of just having things done to them. I want to praise them for effort, not achievements, and be curious about how they feel, instead of telling them how they should feel and how I feel about things they do. I want to remember that strong emotional reactions to my child‚Äôs behaviour are likely to be a reaction to my own past. In such moments I want to ask myself what used to happen to me as a child when I behaved in this way. I want to talk about my child as the individual they are, not as ‚Äúthe child‚Äù or ‚Äì worse ‚Äì ‚Äúthe children‚Äù. I want to poblem-solve with them, not for them: to not rescue them unnecessarily, but help them solve their own problems as needed; to be a sounding board, not an oracle. I want to invest positive time early instead of negative time later. ","date":"2023-05-14","objectID":"/parenting-principles/:0:0","tags":["parenting"],"title":"Parenting advice I want to remember","uri":"/parenting-principles/"},{"categories":null,"content":"Drills to practice working with Python dictionaries. ","date":"2022-02-28","objectID":"/python-dictionaries/:0:0","tags":["python"],"title":"Python dicts","uri":"/python-dictionaries/"},{"categories":null,"content":"Built-in dict Create a dictionary, d, from the two lists below using the dict() constructor. keys = [\"a\", \"b\", \"c\"] values = range(3) d = dict(zip(keys, values)) Recreate the same dictionary using dictionary comprehension and check that the result is identical to d. dd = {key: value for key, value in zip(keys, values)} dd == d What does the below return and why? {key: value for key in keys for value in values} It returns 2 as the value for every key because for each key it iterates through all the values and returns the last one. Return a list of all dictionary keys. list(d) Return the number of items in d. len(d) Return the value for key b in two different ways. d[\"b\"] d.get(\"b\") Replace the above value with 5. d[\"b\"] = 5 Remove the entry c. del d[\"c\"] Check that c is no longer present. \"c\" not in d Create a shallow copy, dd, of d. dd = d.copy() Try to return the value for the non-existend key x and intead return default. d.get(\"x\", \"default\") Return a view of all key-value pairs. d.items() Return a view of all keys. d.keys() Return a view of all values. d.values() Remove key a and return its value, ensuring that if a isn‚Äôt in d, default is returned. d.pop(\"a\", \"default\") Add keys m and n with values 10 and 11 to the dict. d[\"m\"] = 10 d[\"n\"] = 11 Remove the last added key-value pair from the dict and return it. d.popitem() Return a reversed iterator over the keys and use it to print the values in reverse order. for i in reversed(d): print(d[i]) Add key z to the dictionary initialising it with a default value of 0. d.setdefault(\"z\", 0) Print the dictionary. d Check equality of d.values() with itself. What do you get and why? It‚Äôs False, like all comparisons of view objects. Delete all elements from the dictionary. d.clear() d Create dictionaries m and n with keys from the string below, and with all values initialised as 1 and 2, repectively. keys_m = \"abcd\" keys_n = \"cdef\" m = dict.fromkeys(keys_m, 1) n = dict.fromkeys(keys_n, 2) Create a new dict, o, with the combined keys of m and n, with the keys of the former taking precedent in the case of duplicate keys. o = n | m Update the values in n with those of m in place. n.update(m) Now update m with n using a different method. m |= n ","date":"2022-02-28","objectID":"/python-dictionaries/:1:0","tags":["python"],"title":"Python dicts","uri":"/python-dictionaries/"},{"categories":null,"content":"Counter Create a dictionary d that stores the number of occurrences for each item in items. items = \"aabbcdeaabdd\" from collections import Counter d = Counter(items) Use d to produce a sorted version of items. list(d.elements()) Find the two most common element in d and their counts. d.most_common(2) Find the three least common elements in d (in ascending order of frequency) and their counts. d.most_common()[:-4:-1] Add the counts of more_items to d. How does the method to be used compare to when called on a regular dictionary? more_items = \"aabcxxyzz\" d.update(more_items) Based on the below two counters, create a new counter containing the minimum of all counts (with zero counts deleted from the dict). a = Counter(\"aab\") b = Counter(\"bbc\") a \u0026 b Now create one that contains the maximum of all counts. a | b Drop all items with negative counts from counter a below and explain what‚Äôs going on. a = Counter({\"a\": 3, \"b\": -4}) a Counter({'a': 3, 'b': -4}) +a There are two things going on here: First, mathematical operations on counters drop items with negative counts (counters are built for counting positive things), and, second, unary operations +a or -a are shorthand for adding to or subtracting from an empty counter. ","date":"2022-02-28","objectID":"/python-dictionaries/:2:0","tags":["python"],"title":"Python dicts","uri":"/python-dictionaries/"},{"categories":null,"content":"Ordered dict Since Python 3.7, dict has been declared to maintain the order of keys as they are encountered. Hence, the use of OrderedDict is more limited (see here for a good discussion on when you might still use it), and Counter inherits the new behaviour as it is a dict subclass üí•. ","date":"2022-02-28","objectID":"/python-dictionaries/:3:0","tags":["python"],"title":"Python dicts","uri":"/python-dictionaries/"},{"categories":null,"content":"Topics ","date":"2022-02-28","objectID":"/python-dictionaries/:4:0","tags":["python"],"title":"Python dicts","uri":"/python-dictionaries/"},{"categories":null,"content":"Setting default values Create a dict d from the below items, concatenating the list values for duplicate keys. items = [(\"a\", [1, 2]), (\"b\", [3, 4]), (\"a\", [3, 4]), (\"c\", [5])] First, what happens if we call dict(items)? The first set of values for a get overwritten by the second set of values. Now, create d using the built-in dict. d = {} for key, value in items: d.setdefault(key, []).extend(value) d Do the same but in a different way. d = {} for key, value in items: if key not in d: d[key] = [] d[key].extend(value) d Now do the same using the collections module. from collections import defaultdict d = defaultdict(list) for key, data in items: d[key].extend(data) d Now, using any additional module you like, create d again but this time with values ([items], occurrences), where the first element is the value items as before, and the second element is a count for the number of times the key occurres in items (e.g.¬†for key a we want ([1, 2, 3, 4], 2).) d = collections.defaultdict(lambda: [[], 0]) for key, value in items: d[key][0].extend(value) d[key][1] += 1 d Create a greeting function that takes a user ID as input and returns Hi name if the user ID has an entry in name_for_userid and Hi there otherwise. name_for_userid = { 382: \"Alice\", 590: \"Bob\", 951: \"Dilbert\", } def greeting(user_id): return \"Hi {}\".format(name_for_userid.get(user_id, \"there\")) ","date":"2022-02-28","objectID":"/python-dictionaries/:4:1","tags":["python"],"title":"Python dicts","uri":"/python-dictionaries/"},{"categories":null,"content":"Setting default values in nested dictionaries From Khuyen Tran Get a list of the taste attribute for each fruit in list below, substituting unknown for fruits with no taste information. fruits = [ { 'name': 'apple', 'attr': {'colour': 'red', 'taste': 'sweet'}, }, { 'name': 'orange', 'attr': {'colour': 'orange'}, }, { 'name': 'banana', }, ] Complete the task without using any dictionary methods. [fruit['attr']['taste'] if 'attr' in fruit and 'taste' in fruit['attr'] else 'unknown' for fruit in fruits] Complete the task more elegantly than above. [fruit.get('attr', {}).get('taste', 'unknown') for fruit in fruits] ","date":"2022-02-28","objectID":"/python-dictionaries/:4:2","tags":["python"],"title":"Python dicts","uri":"/python-dictionaries/"},{"categories":null,"content":"Calculating with dics Based on recipe 1.8 in the Python Cookbook prices = {\"ACME\": 45.23, \"AAPL\": 612.78, \"IBM\": 205.55, \"HPQ\": 37.20, \"FB\": 10.75} Return the name of the stock with the highest price. max(prices, key=lambda x: prices[x]) Return the key-value pair for the stock with the lowest price. min(prices.items(), key=lambda x: x[1]) Return the items in decreasing order of price. sorted(prices.items(), key=lambda x: x[1], reverse=True) ","date":"2022-02-28","objectID":"/python-dictionaries/:4:3","tags":["python"],"title":"Python dicts","uri":"/python-dictionaries/"},{"categories":null,"content":"Mapping dict values to list items Create a list containing the dict values of the elements in a in two different ways. d = {1: 'a', 2: 'b', 3: 'c'} a = [1, 2, 3, 1, 2] list(map(d.get, a)) [d.get(item) for item in a] Now do the same for the new list a, and use 99 as the value for items that aren‚Äôt in d. Use again two different ways. a = [1, 2, 3, 1, 100] list(map(lambda x: d.get(x, 99), a)) [d.get(item, 99) for item in a] ","date":"2022-02-28","objectID":"/python-dictionaries/:4:4","tags":["python"],"title":"Python dicts","uri":"/python-dictionaries/"},{"categories":null,"content":"Basics Define a heap. Heaps are binary trees for which every parent node has a value less than or equal to any of its children. Load the standard library module that implements heaps. What kind of heaps are supported? # heapq implements min heaps. Push *-item* to implement max heap. import heapq Turn the below list into a min-heap. heap = [1, -4, 7, 50] heapq.heapify(heap) Add -1 to the heap. heapq.heappush(heap, -1) Remove and return the smallest element from the heap. heapq.heappop(heap) -4 Add -20 to the heap, then remove and return the smalles element. heapq.heappushpop(heap, -20) -20 Remove and return the smallest element and add 3 to the heap. heapq.heapreplace(heap, 3) -1 Display the heap. heap [1, 3, 7, 50] Return the two largest elements on the heap. heapq.nlargest(2, heap) [50, 7] For the heap below, return the two elements with the smallest digits. heap = [(\"a\", 3), (\"b\", 2), (\"c\", 1)] heapq.nsmallest(2, heap, key=lambda x: x[1]) [('c', 1), ('b', 2)] ","date":"2022-02-25","objectID":"/heaps/:1:0","tags":["python"],"title":"Heaps","uri":"/heaps/"},{"categories":null,"content":"Applications Accessing shares For the shares portfolio below, return the data for the three most expensive shares, sorted in descending order. portfolio = [ {\"name\": \"IBM\", \"shares\": 100, \"price\": 91.1}, {\"name\": \"AAPL\", \"shares\": 50, \"price\": 543.22}, {\"name\": \"FB\", \"shares\": 200, \"price\": 21.09}, {\"name\": \"HPQ\", \"shares\": 35, \"price\": 31.75}, ] heapq.nlargest(3, portfolio, lambda x: x[\"price\"]) [{'name': 'AAPL', 'shares': 50, 'price': 543.22}, {'name': 'IBM', 'shares': 100, 'price': 91.1}, {'name': 'HPQ', 'shares': 35, 'price': 31.75}] Do the same using operator.itemgetter(). from operator import itemgetter heapq.nlargest(3, portfolio, key=itemgetter(\"price\")) Use a faster method that doesn‚Äôt rely on heapq to achieve the same result. %timeit sorted(portfolio, key=lambda x: x['price'], reverse=True)[:3] 588 ns ¬± 16.9 ns per loop (mean ¬± std. dev. of 7 runs, 1000000 loops each) %timeit heapq.nlargest(3, portfolio, key=lambda x: x['price']) 2.1 ¬µs ¬± 6.86 ns per loop (mean ¬± std. dev. of 7 runs, 100000 loops each) Calculate the running median for the list below. Building a priority queue Use heapq to build a priority queue that supports push() and pop() operations (based on Python Cookbook recipe 1.5 and heapq docs). Specifically, build a queue that takes in items of the form (value, priority), returns items in order or priority with higher values signifying higher priority, and breaks priority ties by returning items in the order they were added. To test the list, push the items (foo, 1), (bar, 3), and (baz, 3). The first pop should return (bar, 3). import heapq class Item: def __init__(self, name): self.name = name def __repr__(self): return \"Item({!r})\".format(self.name) class PriorityQueue: def __init__(self): self._queue = [] self._index = 0 def push(self, item, priority): heapq.heappush(self._queue, (-priority, self._index, item)) self._index += 1 def pop(self): return heapq.heappop(self._queue)[-1] q = PriorityQueue() q.push(Item(\"foo\"), 1) q.push(Item(\"bar\"), 3) q.push(Item(\"baz\"), 3) q.pop() Item('bar') Running median Write a program that calculates the running median and test it on the list below. def running_median(sequence): min_heap, max_heap, result = [], [], [] for x in sequence: heapq.heappush(max_heap, -heapq.heappushpop(min_heap, x)) if len(max_heap) \u003e len(min_heap): heapq.heappush(min_heap, -heapq.heappop(max_heap)) result.append( (min_heap[0] + -max_heap[0]) / 2 if len(min_heap) == len(max_heap) else min_heap[0] ) return result a = [1, 30, 2, -7, 99, 10] running_median(a) [1, 15.5, 2, 1.5, 2, 6.0] ","date":"2022-02-25","objectID":"/heaps/:2:0","tags":["python"],"title":"Heaps","uri":"/heaps/"},{"categories":null,"content":"Sources Fluent Python Python Cookbook Learning Python The Hitchhiker‚Äôs Guide to Python Effective Python ","date":"2022-02-25","objectID":"/heaps/:3:0","tags":["python"],"title":"Heaps","uri":"/heaps/"},{"categories":null,"content":"My evolving notes and on how to effectively use Git and GitHub. ","date":"2022-02-02","objectID":"/git/:0:0","tags":["tools"],"title":"Git","uri":"/git/"},{"categories":null,"content":"Workflow I code in Neovim in the terminal, within tmux sessions. I use gitgutter to show me git diff markers in the sign column and to access hunks of changes easily. I use gitgutter to preview (,hp), stage (,hs), and undo (,hu) hunks. ","date":"2022-02-02","objectID":"/git/:1:0","tags":["tools"],"title":"Git","uri":"/git/"},{"categories":null,"content":"Git Evolution of version control systems (and their shortcomings): from having differently named files and folders (error prone and inefficient) to local version control systems (LVCs) where files are checked out from a locally hosted database that contains the entire history (single point of failure and impractical for collaboration) to centralized version control systems (CVCSs) where files are checked out from a server-hosted database (single point of failure) to distributed version control systems (DVCs) where files and a database containing the entire history are checked out from a served-hosted database, so that each local node contains all information stored on the server (content on server can easily be restored in case of failure). Git stores data as snapshots: at each new commit, modified files are replaced with a snapshot of their new state, while unmodified files are replaced with a link to the previous snapshot. In a basic workflow I edit a file in the working tree (the locally checked out version of the project), add them to the staging area (also called index), commit them to the local database, and finally push them to the remote database on GitHub. Neither main (or, formerly, master), nor origin have any special significance in Git. The reason they are widely used is that main is the default name for the starting branch when running git init and origin is the default name for the remote repository when running git clone. Git stores a single file as a blob, which is a backronym for binary large object and is a collection of binary data. It can store any type of data including multimedia files and images. Blobs in the git object database are stored named with a SHA-1 hash key of their content and containing the exact same content as the file would on my filesystem. ","date":"2022-02-02","objectID":"/git/:2:0","tags":["tools"],"title":"Git","uri":"/git/"},{"categories":null,"content":"Interacting with remotes A remote repository is a version of the project that‚Äôs hosted on a server, in my case always on GitHub. The default name Git assigns to the remote when I clone it is origin. (Technically, the remote could also be hosted on my machine but in a separate location from my working copy.) git fetch \u003cremote\u003e downloads all new objects from the remote repository (e.g. including references to new branches) but does not automatically merge these objects into my local work. git pull fetches and automatically merges the remote version of the local branch I‚Äôm currently on and merges it if the current branch is set up to track the remote. By default, git clone sets up my local main branch to track the remote version, named origin/main. To share work with the remote, I can use git push \u003cremote\u003e \u003cbranch\u003e (e.g. to share work from my local main branch with origin/main, I can do git push origin main). ","date":"2022-02-02","objectID":"/git/:3:0","tags":["tools"],"title":"Git","uri":"/git/"},{"categories":null,"content":"Undoing things Cardinal rule: don‚Äôt push stuff before you‚Äôre fully happy with it. Changing your local history is easy, changing the history on the server isn‚Äôt. Changing message of last commit: (with an empty index) run git commit --amend. Changing the content of the last commit: stage changes you want to add, then run git commit --amend. If you don‚Äôt want to change the commit message, append --no-edit. Unstaging a staged file: git restore --staged \u003cpathspec\u003e. Undoing changes in the working directory and reverting a file to its state after the last commit: git restore \u003cpathspec\u003e. Changing multiple commits (edit, reorder, squash, delete, split, etc.): git rebase -i HEAD~#, where # is the parent of the last commit you want to edit (e.g. if you want to edit the last 3 commits, HEAD~3 will select commits HEAD, HEAD~1, and HEAD~2). More in docs I‚Äôve accidentally overwritten a file with content I meant to place in a new file, and now want to 1) save the new content under a different name and 2) restore the file I‚Äôve overwritten. Solution: Just save the new content under a new name (temporarily deleting the file I‚Äôve accidentally overwritten, and then use git restore \u003cname_of_overwritten_file\u003e to restore the old version of the overwritten file. I‚Äôve deleted one or more commits (e.g. by using a hard reset) that I need to recover. First thing to try: run git reflog to get a log of commits HEAD pointed to. Once I‚Äôve identified the commit I need (ab1af) I can create a new branch that points to it using git branch recover-branch ab1af. If there is no reflog, I can run git fsck --full to check the database for integrity and get a list of objects that aren‚Äôt reacheable. The commit I‚Äôm looking for will be labelled with dangling commit, and I can create a branch pointing to it. Removing a file from every commit (e.g. accidentally committed large data file): git filter-branch --index-filter 'rm --ignore-unmatch --cached data.csv' HEAD. This can take a long time. One way to speed things up is to find the commit that added the file to the history and only filter downstream from there. git log --oneline --branches -- data.csv will list all commits that contain the file from latest to earliest. If the file was added in commit a34s5, then I only want to rewrite commits a34s5^..HEAD, which I can substitute for HEAD in the filter-branch command. (In case I don‚Äôt know the name of the large file I want to remove, see here for how to find it. Finally, as advised in relevant section here, it‚Äôs best to first do this on a separate branch to test the behaviour before running it on main. Undoing a (pushed) commit: git revert \u003ccommit_hash\u003e. Creates a new commit undoing the specified commit. To undo a range of commits, use git revert \u003coldest_commit_hash\u003e..\u003clatest_commit_hash\u003e. (More here). ","date":"2022-02-02","objectID":"/git/:4:0","tags":["tools"],"title":"Git","uri":"/git/"},{"categories":null,"content":"Frequently used stuff and best practices I‚Äôve started to work on an issue I decide I don‚Äôt want to work on yet but I want to save that work. Just stash the work. I‚Äôm working on a topic branch and discover another issue I need to fix first. What to do? Create local version of remote branch that automatically tracks remote branch (if branch has unique remote counterpart): git switch \u003cbranch\u003e. Reset vs restore vs revert vs rebase (docs here): reset is about updating your branch by adding or removing commits from the branch, restore is about unstading files from the index or undoing changes in the working directory, reset is about making new commits to undo changes made by other commits, rebase, like merge, is a way to integrate work from two different branches. But unlike merge, which takes the endpoints of two branches and merges them together, rebase applies changes from the branch you merge onto the branch you merge to in the order they happened and thus creates a linear history. Adding a local repository to GitHub: Initialise the local repo as a Git repository: git init. Add and commit all content of the local repo: git add --all; git commit -m 'Intial commit'. Create a GitHub repo and follow prompts: gh repo create. Done. ","date":"2022-02-02","objectID":"/git/:5:0","tags":["tools"],"title":"Git","uri":"/git/"},{"categories":null,"content":"Understanding reset Think of HEAD as the last commit on the current branch (it‚Äôs a pointer to the current branch which is a pointer to the last commit on that branch), the index as the proposed next commit, and the working directory as a sandbox. Think of all of them as collections of files, or file trees. When I switch to a branch, Git makes HEAD point to the new branch ref, populates the index with the snapshot of the last commit on that branch, and copies the contents of the index into the working directory. Changing a file updates it in the working directory, staging it updates the version in the index with that of the working directory, and committing it updates the version HEAD points to with that of the index. git reset --soft HEAD~ moves the branch that HEAD points to to the parent of the last commit. The version of the file that HEAD points to now differs from the versions in the index and the working directory, which still contain the version of the last commit on the branch. Effectively, we‚Äôve undone the last commit. You could now make changes to the index and then commit them, accomplishing the same as git commit --amend. So, --soft undoes git commit. git reset [--mixed] HEAD~ moves the branch HEAD points to (just as --soft above) but then also updates index with the content of the snapshot HEAD now points to. So, --mixed undoes git commit and git add. git reset --hard HEAD~, does what the above does, but then continues and also updates the working directory with the content of the index. This forcibly overwrites files, which, if they haven‚Äôt been committed (in which case they can be recovered using the reflog), is unrecoverable. So, --hard undoes git commit, git add, and all changes made in the working directory since the last commit. We can undo multiple commits: to undo all commits since commit 9e5bf, simply run git reset \u003coption\u003e 9e5bf. reset is also handy to squash commits together. To squash the last three unpushed commits, use git reset --soft HEAD~3. This moves the branch ref to the great-grandparent of the latest commit. Because the index remains unchanged, all changes committed after HEAD~3 now appear as staged and can be committed in a single new commit. ","date":"2022-02-02","objectID":"/git/:6:0","tags":["tools"],"title":"Git","uri":"/git/"},{"categories":null,"content":"Writing good commits Writing good commits (more here): No whitespace errors (git diff --check, probs integrated in fugitive somehow). Each commit is a logically separate changeset. Useful commit message using capitalisation and written in imperative style (‚ÄúFix bug‚Äù instead of ‚ÄúFixed bug‚Äù or ‚ÄúFixes bug‚Äù, to be compatible with Git‚Äôs auto generated messages) comprising a short summary (no longer than 50 characters so it fits on one line in log) followed by a blank line followed by a motivation for the changes and a description of the contrast between old and new behaviour. ","date":"2022-02-02","objectID":"/git/:7:0","tags":["tools"],"title":"Git","uri":"/git/"},{"categories":null,"content":"Selecting commits Docs Short SHA-1 hashes: git show d921 shows the commit with abbreviated SHA-1 hash d921. Branch references: git show iss3 shows the commit on the tip of the branch iss3. Ancestry references: There are two different ways of ancestry selection which I think of as ‚Äúhorizontal‚Äù and ‚Äúvertical‚Äù selection. ^ selects different parents for merge commits: git show d921^ shows the first parent of the merge commit. By default, this parent is from the branch from which the merge was performed (frequently main). git show d921^2 shows the second parent, which is from the branch that was merged. I think of this as horizontal ancestor selection. Conversely, ~ performs vertical ancestor selection in that it iteratively selects the first parent a specified number of times. git show d921~ shows the parent of d921, git show d921~2 the parent of the parent, git show d921~3 the parent of the parent of the parent, and so on (the previous could also be written as git show d921~~~). The two syntaxes can be combined: git show d921~2^2 shows the second parent of the grandparent of d921, assuming the grandparent is a merge commit. Double dot: git log main..iss3 lists all commits that are reachable from the iss3 but not the main branch. Git substitues HEAD if one side of .. is empty, so to list local commits that aren‚Äôt yet on the remote, you could do git log origin/main... Similarly, to select the last three commits, use HEAD~3..HEAD, which selects all commits reachable from HEAD but not HEAD~3, which are commits HEAD~2, HEAD~, and HEAD. Triple dot: to list commits that occur either on master or iss3 but not both, I can do git log master...iss3, and to get arrows to indicate whether a commit is reachable from the right or left branch, git log --left-right master...iss3. ","date":"2022-02-02","objectID":"/git/:8:0","tags":["tools"],"title":"Git","uri":"/git/"},{"categories":null,"content":"Misc. To search for commit message in log history: git log --grep='regex' ","date":"2022-02-02","objectID":"/git/:9:0","tags":["tools"],"title":"Git","uri":"/git/"},{"categories":null,"content":".gitignore The section on ignoring files in this ProGit book chapter is excellently clear and provides very useful examples. ","date":"2022-02-02","objectID":"/git/:10:0","tags":["tools"],"title":"Git","uri":"/git/"},{"categories":null,"content":"Iterators A Python object is described as iterable (adjective) or as an iterable (noun) when it can be iterated over ‚Äì when we can process the elements it contains in turn. An iterator is a value-producing object that returns the content of an iterable on demand one by one as we call next(). We can create an iterator from an iterable using the built-in iter() function. One (the?) main feature of iterators is that they are lazy: they produce the next item in the sequence only once it is required, which, for large sequences, can save a lot of memory and allow us to process data that doesn‚Äôt fit into memory all at once. Iterator example: iterable = \"hello world\" iterator = iter(iterable) next(iterator), next(iterator) ('h', 'e') ","date":"2022-01-22","objectID":"/iterators-and-generators/:1:0","tags":["python"],"title":"Iterators and generators","uri":"/iterators-and-generators/"},{"categories":null,"content":"Generators Generators are a tool to easily create iterators (docs) Similarly to return, the yield statement indicates that a value is returned to the caller, but unlike it, function execution is not terminated but merely suspended with the current state of the function saved, and function execution will pick up right after the yield statement on the next call to a generator method. import inspect def gen(x): yield x a = gen(5) print(inspect.getgeneratorstate(a)) next(a) print(inspect.getgeneratorstate(a)) try: next(a) except StopIteration: print(inspect.getgeneratorstate(a)) GEN_CREATED GEN_SUSPENDED GEN_CLOSED Example: creating a generator from an iterator using generator comprehension a = [1, 2, 3] reversed(a), (i for i in reversed(a)) (\u003clist_reverseiterator at 0x15f191b80\u003e, \u003cgenerator object \u003cgenexpr\u003e at 0x15f5e1200\u003e) Example: creating an infinite sequence def infinite_sequence(): num = 0 while True: yield num num += 1 gen = infinite_sequence() next(gen), next(gen) (0, 1) Example: generator expressions (They are well suited in cases where memory is an issue, but then can be slower than list expressions.) import sys nums_squared_lc = [i ** 2 for i in range(10000)] print(sys.getsizeof(nums_squared_lc)) nums_squared_gc = (i ** 2 for i in range(10000)) sys.getsizeof(nums_squared_gc) 85176 112 Example: reading a file line by line def csv_reader(filepath): for row in open(filepath, \"r\"): yield row filepath = \"/Users/fgu/example.csv\" csv_gen = csv_reader(filepath) # or: csv_gen = (row for row in open(filepath)) row_count = 0 for row in csv_gen: row_count += 1 print(f\"Row count: {row_count}\") Row count: 2017 Example: data processing pipeline filepath = \"/Users/fgu/example.csv\" # exctract lines lines = (line for line in open(filepath)) # turn lines into lists of items list_line = (line.rstrip().split(\",\") for line in lines) # extract column names col_names = next(list_line) # create dict with col name keys for each line line_dicts = (dict(zip(col_names, data)) for data in list_line) # extract needed data sf_temps = (float(d[\"temperature\"]) for d in line_dicts if d[\"city\"] == \"San Francisco\") # start iteration to calculate result sf_min_temp = min(sf_temps) print(f\"SF min temperature was {sf_min_temp}\") SF min temperature was 15.4496008956306 Example: implementing a for loop items = [\"a\", \"b\", \"c\"] it = iter(items) while True: try: item = next(it) except StopIteration: break print(item) a b c Example: implementing range() (From the Python Cookbook recipee 4.3) def frange(start, stop, increment): x = start while x \u003c stop: yield x x += increment rng = frange(1, 10, 2) list(rng) [1, 3, 5, 7, 9] Example: creating an arithmetic progression sequence def aritprog(begin, step, end=None): result = type(begin + step)(begin) forever = end is None index = 0 while forever or result \u003c end: yield result index += 1 result = begin + step * index a = aritprog(0, 5, 20) for a in a: print(a) 0 5 10 15 ","date":"2022-01-22","objectID":"/iterators-and-generators/:2:0","tags":["python"],"title":"Iterators and generators","uri":"/iterators-and-generators/"},{"categories":null,"content":"yield from yield from allows for easily splitting up a generator into multiple generators. def gen(): for i in range(5): yield i for j in range(5, 10): yield j We can split it into two def gen1(): for i in range(5): yield i def gen2(): for j in range(5, 10): yield j def gen(): for i in gen1(): yield i for j in gen2(): yield j list(gen()) [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] yield from allows us to simplify the above. def gen(): yield from gen1() yield from gen2() list(gen()) [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] Merging two sorted collections def sorted_merge(left, right): \"\"\"Returns two sorted collections as a single sorted collection.\"\"\" def _merge(): while left and right: yield (left if left[0] \u003c right[0] else right).pop(0) yield from left yield from right return _merge() list(sorted_merge([1, 3, 4, 99, 111], [2, 5, 7, 88])) [1, 2, 3, 4, 5, 7, 88, 99, 111] ","date":"2022-01-22","objectID":"/iterators-and-generators/:3:0","tags":["python"],"title":"Iterators and generators","uri":"/iterators-and-generators/"},{"categories":null,"content":"Sources Real Python, Python ‚Äúfor‚Äù loops Fluent Python Python Cookbook Simeon Visser, Using yield from in generators ","date":"2022-01-22","objectID":"/iterators-and-generators/:4:0","tags":["python"],"title":"Iterators and generators","uri":"/iterators-and-generators/"},{"categories":null,"content":" import math import matplotlib.pyplot as plt import numpy as np ","date":"2022-01-09","objectID":"/entropy/:0:0","tags":["cs, stats"],"title":"Entropy","uri":"/entropy/"},{"categories":null,"content":"Intro Entropy is a measure of the amount of information contained in an event or a random variable. It is a cornerstone of information theory, a subfield of mathematics concerned with the transmission of data across a noisy channel. ","date":"2022-01-09","objectID":"/entropy/:1:0","tags":["cs, stats"],"title":"Entropy","uri":"/entropy/"},{"categories":null,"content":"Information of an event Key intuition: learning that a low probability event has occurred is more informative than learning that a high probability event has occurred. Hence, the information of an event $E$ is inversely proportional to its probability $p(E)$. We could capture this using $I(E) = \\frac{1}{p(E)}$. But this implied that when $E$ is certain to occur (and thus $p(E) = 1$), the information would be 1, when it would make more sense for it to be 0. A way to achieve this is to use $log\\left(\\frac{1}{p(E)}\\right)$ (the log is actually the only function that also satisfies a number of other desirable characteristics). Hence, the information (or surprise) of E, often called Shannon information, self-information, or just information, is defined as $$I(E) = log\\left(\\frac{1}{p(E)}\\right) = -log(p(E)).$$ As a reminder to myself: the second equality holds because $log(\\frac{1}{p(E)}) = log(p(E)^{-1}) = -log(p(E))$, where the second step is true because (using base $e$ for simplicity of notation) $e^{ln(a^b)} = a^b = (e^{ln(a)})^b = e^{bln(a)}$, where the last step is true because $(e^a)^b = e^{ab}$, and which implies that $ln(a^b) = bln(a)$. The choice of the base for the logarithm varies by application and determines the units of $I(E)$. Base 2 means that information is expressed in bits. The natural logarithm, another popular choice, expresses information in nats. ","date":"2022-01-09","objectID":"/entropy/:2:0","tags":["cs, stats"],"title":"Entropy","uri":"/entropy/"},{"categories":null,"content":"Tossing a head example p = 0.5 I = -math.log2(p) print( f\"If p(head) is {p:.2f}, the amount of information of flipping a head is {I:.2f} bit.\" ) If p(head) is 0.50, the amount of information of flipping a head is 1.00 bit. Getting head when it is unlikely is more informative. p = 0.1 I = -math.log2(p) print( f\"If p(head) is {p:.2f}, the information conveyed by flipping a head is {I:.2f} bit.\" ) If p(head) is 0.10, the information conveyed by flipping a head is 3.32 bit. ###¬†Rolling a 6 example p = 1 / 6 I = -math.log2(p) print(f\"Rolling a 6 with a fair die has {I:.2f} bits of information.\") Rolling a 6 with a fair die has 2.58 bits of information. ","date":"2022-01-09","objectID":"/entropy/:2:1","tags":["cs, stats"],"title":"Entropy","uri":"/entropy/"},{"categories":null,"content":"Probability vs information Generally, the higher the probability of an event $E$, the lower the information content revealed when it comes to pass. probs = np.linspace(0.1, 1, 10) info = [-math.log2(p) for p in probs] fig, ax = plt.subplots() ax.plot(probs, info, marker=\".\") ax.set(xlabel=\"p(E)\", ylabel=\"I(E)\"); ","date":"2022-01-09","objectID":"/entropy/:2:2","tags":["cs, stats"],"title":"Entropy","uri":"/entropy/"},{"categories":null,"content":"Information of a random variable The information of a random variable X is called Information entropy, Shannon entropy, or just entropy, and denoted by $H(X)$ (named, by Shannon, after Boltzmann‚Äôs H-theorem in statistical mechanics). Calculating the information of a random variable is the same as calculating the information of the probability distribution of the events for the random variables. It is calculated as $$H(X) = -\\sum_x p(x) \\times log(p(x)) = \\sum_x p(x)I(x) = \\mathbb{E} I(x).$$ Intuitively, the entropy of a random variable captures the expected amount of information conveyed by an event drawn from the probability distribution of the random variable. Hence, it‚Äôs the expected value of self-information of a variable, which can be seen by the last equality above. Why does entropy (when calculated using base 2 logarithms) also represent the number of bits required to convey the average outcome of a distribution? Wikipedia (in the second to last paragraph of the introduction) explains this well. Basically, its because if there are some events with very high probability, then these events could be transmitted with short codes of only a few bits, so that most of the time, only a few bits have to be transmitted to send the message. ","date":"2022-01-09","objectID":"/entropy/:3:0","tags":["cs, stats"],"title":"Entropy","uri":"/entropy/"},{"categories":null,"content":"Fair die example Given that all six sides of a fair die have equal probability of being thrown, and given that the entropy for a random variable captures the average amount of information of its events, we‚Äôd expect the entropy of throwing a fair die to be identical to the amount of information of throwing a 6, as calculated above. The below shows that this is indeed the case. probs = [1 / 6] * 6 H = -sum(p * math.log2(p) for p in probs) print(f\"Entropy of rolling a fair die is {H:.2f}.\") Entropy of rolling a fair die is 2.58. We can also use the entropy() function from scipy. from scipy.stats import entropy H1 = entropy(probs, base=2) assert H == H1 ","date":"2022-01-09","objectID":"/entropy/:3:1","tags":["cs, stats"],"title":"Entropy","uri":"/entropy/"},{"categories":null,"content":"Skewness vs information The below shows the binary entropy function, the relationship between entropy and the binary outcome probability in a Bernoulli process. probs = np.linspace(0, 1, 11) H = [entropy([p, 1 - p], base=2) for p in probs] fig, ax = plt.subplots() ax.plot(probs, H, marker=\".\") ax.set(xlabel=\"p\", ylabel=\"Entropy\"); ","date":"2022-01-09","objectID":"/entropy/:3:2","tags":["cs, stats"],"title":"Entropy","uri":"/entropy/"},{"categories":null,"content":"References Claude E. Shannon, A mathematical theory of communication Wikipedia, Entropy Machine Learning Mastery, A gentle introduction to information entropy Aerin Kim, The intuition behind Shannon‚Äôs entropy ","date":"2022-01-09","objectID":"/entropy/:4:0","tags":["cs, stats"],"title":"Entropy","uri":"/entropy/"},{"categories":null,"content":"Operator (docs) ","date":"2021-12-13","objectID":"/python-built-in-heroes/:1:0","tags":["python"],"title":"Python built-in heroes","uri":"/python-built-in-heroes/"},{"categories":null,"content":"itemgetter() Basic use: from operator import itemgetter print(itemgetter(1, 3, 5)(\"Watermelon\")) print(itemgetter(slice(5, None))(\"Watermelon\")) print(itemgetter(\"name\")(dict(name=\"Paul\", age=44))) ('a', 'e', 'm') melon Paul Application (from docs): inventory = [(\"apple\", 3), (\"banana\", 2), (\"pear\", 5), (\"orange\", 1)] getcount = itemgetter(1) # get second item from list print(getcount(inventory)) # get second item from each element in list list(map(getcount, inventory)) ('banana', 2) [3, 2, 5, 1] Application: sorting list of dictionaries (from Python Cookbook recipe 1.13) data = [ {\"fname\": \"Brian\", \"lname\": \"Jones\", \"uid\": 1003}, {\"fname\": \"David\", \"lname\": \"Beazley\", \"uid\": 1002}, {\"fname\": \"John\", \"lname\": \"Cleese\", \"uid\": 1001}, {\"fname\": \"Big\", \"lname\": \"Jones\", \"uid\": 1004}, ] # get second row from data print(itemgetter(2)(data)) # get ids from all rows print(list(map(itemgetter(\"uid\"), data))) # sort data by fname and lname sorted(data, key=itemgetter(\"fname\", \"lname\")) {'fname': 'John', 'lname': 'Cleese', 'uid': 1001} [1003, 1002, 1001, 1004] [{'fname': 'Big', 'lname': 'Jones', 'uid': 1004}, {'fname': 'Brian', 'lname': 'Jones', 'uid': 1003}, {'fname': 'David', 'lname': 'Beazley', 'uid': 1002}, {'fname': 'John', 'lname': 'Cleese', 'uid': 1001}] lambda alternative to the above: sorted(data, key=lambda r: (r[\"fname\"], r[\"lname\"])) [{'fname': 'Big', 'lname': 'Jones', 'uid': 1004}, {'fname': 'Brian', 'lname': 'Jones', 'uid': 1003}, {'fname': 'David', 'lname': 'Beazley', 'uid': 1002}, {'fname': 'John', 'lname': 'Cleese', 'uid': 1001}] Beazley notes that itemgetter is often a bit faster. I also find itemgetter easier to read. But I usually use lambda as it‚Äôs built it. ","date":"2021-12-13","objectID":"/python-built-in-heroes/:1:1","tags":["python"],"title":"Python built-in heroes","uri":"/python-built-in-heroes/"},{"categories":null,"content":"attrgetter() Basic use: from operator import attrgetter def greeter(): print(\"Hello\") func_name = attrgetter(\"__name__\") func_name(greeter) 'greeter' Application: sort objects without native support (from Python Cookbook recipe 1.14) class User: def __init__(self, name, id): self.name = name self.id = id def __repr__(self): return f'User({self.name!r}, {self.id!r})' users = [User('Trev', 1), User('Rodika', 33), User('Gab', 2), User('Claire', 9)] sorted(users, key=attrgetter('name', 'id')) [User('Claire', 9), User('Gab', 2), User('Rodika', 33), User('Trev', 1)] ","date":"2021-12-13","objectID":"/python-built-in-heroes/:1:2","tags":["python"],"title":"Python built-in heroes","uri":"/python-built-in-heroes/"},{"categories":null,"content":"Itertools import itertools ","date":"2021-12-13","objectID":"/python-built-in-heroes/:2:0","tags":["python"],"title":"Python built-in heroes","uri":"/python-built-in-heroes/"},{"categories":null,"content":"Infinite iterators count() creates evenly spaced value cycle() repeates elements of an iterable repeat() repeats an object Application: create custom tuple patterns count = itertools.count(1, 2) cycle = itertools.cycle(\"abc\") repeat = itertools.repeat(\"ho\") a = [1, 2, 3, 4, 5] list(zip(a, count, cycle, repeat)) [(1, 1, 'a', 'ho'), (2, 3, 'b', 'ho'), (3, 5, 'c', 'ho'), (4, 7, 'a', 'ho'), (5, 9, 'b', 'ho')] Application: create sequence of functions with custom arguments count = itertools.count(1, 2) repeat = itertools.repeat(\"ho\") def repeater(word, count): print(word * count) for _ in range(3): next(map(repeater, count, repeat)) ho hohoho hohohohoho ","date":"2021-12-13","objectID":"/python-built-in-heroes/:2:1","tags":["python"],"title":"Python built-in heroes","uri":"/python-built-in-heroes/"},{"categories":null,"content":"islice() Automatically stops if sequence is exhausted instead of throwing an error. import itertools s = [1, 2, 3, 4, 5] for x in itertools.islice(s, 10): print(x) 1 2 3 4 5 ","date":"2021-12-13","objectID":"/python-built-in-heroes/:2:2","tags":["python"],"title":"Python built-in heroes","uri":"/python-built-in-heroes/"},{"categories":null,"content":"groupby() Sorting addresses (Python Cookbook recipe 1.15) rows = [ {\"address\": \"5412 N CLARK\", \"date\": \"07/01/2012\"}, {\"address\": \"5148 N CLARK\", \"date\": \"07/04/2012\"}, {\"address\": \"5800 E 58TH\", \"date\": \"07/02/2012\"}, {\"address\": \"2122 N CLARK\", \"date\": \"07/03/2012\"}, {\"address\": \"5645 N RAVENSWOOD\", \"date\": \"07/02/2012\"}, {\"address\": \"1060 W ADDISON\", \"date\": \"07/02/2012\"}, {\"address\": \"4801 N BROADWAY\", \"date\": \"07/01/2012\"}, {\"address\": \"1039 W GRANVILLE\", \"date\": \"07/04/2012\"}, ] Iterate over the entries grouped by date (use groupby) from itertools import groupby from operator import itemgetter rows.sort(key=itemgetter(\"date\")) for date, items in groupby(rows, lambda x: x[\"date\"]): print(date) for i in items: print(i) 07/01/2012 {'address': '5412 N CLARK', 'date': '07/01/2012'} {'address': '4801 N BROADWAY', 'date': '07/01/2012'} 07/02/2012 {'address': '5800 E 58TH', 'date': '07/02/2012'} {'address': '5645 N RAVENSWOOD', 'date': '07/02/2012'} {'address': '1060 W ADDISON', 'date': '07/02/2012'} 07/03/2012 {'address': '2122 N CLARK', 'date': '07/03/2012'} 07/04/2012 {'address': '5148 N CLARK', 'date': '07/04/2012'} {'address': '1039 W GRANVILLE', 'date': '07/04/2012'} Repeat the above but without using groupby. import collections sorted_rows = collections.defaultdict(list) for row in rows: sorted_rows[row[\"date\"]].append(row) for date, items in sorted_rows.items(): print(date) for item in items: print(item) 07/01/2012 {'address': '5412 N CLARK', 'date': '07/01/2012'} {'address': '4801 N BROADWAY', 'date': '07/01/2012'} 07/02/2012 {'address': '5800 E 58TH', 'date': '07/02/2012'} {'address': '5645 N RAVENSWOOD', 'date': '07/02/2012'} {'address': '1060 W ADDISON', 'date': '07/02/2012'} 07/03/2012 {'address': '2122 N CLARK', 'date': '07/03/2012'} 07/04/2012 {'address': '5148 N CLARK', 'date': '07/04/2012'} {'address': '1039 W GRANVILLE', 'date': '07/04/2012'} Creating look-and-say sequence (EPI 6.7) Return the nth entry of the look-and-say sequence def look_and_say(n): s = \"1\" for _ in range(1, n): s = \"\".join(key + str(len(list(group))) for key, group in groupby(s)) return s look_and_say(3) '12' ","date":"2021-12-13","objectID":"/python-built-in-heroes/:2:3","tags":["python"],"title":"Python built-in heroes","uri":"/python-built-in-heroes/"},{"categories":null,"content":"compress() from itertools import compress addresses = [ \"5412 N CLARK\", \"5148 N CLARK\", \"5800 E 58TH\", \"2122 N CLARK\" \"5645 N RAVENSWOOD\", \"1060 W ADDISON\", \"4801 N BROADWAY\", \"1039 W GRANVILLE\", ] counts = [0, 3, 10, 4, 1, 7, 6, 1] # keep all popular addresses (with \u003e 5 counts) popular = [x \u003e 5 for x in counts] list(compress(addresses, popular)) ['5800 E 58TH', '4801 N BROADWAY', '1039 W GRANVILLE'] ","date":"2021-12-13","objectID":"/python-built-in-heroes/:2:4","tags":["python"],"title":"Python built-in heroes","uri":"/python-built-in-heroes/"},{"categories":null,"content":"starmap() Using starmap() to calculate a running average: from itertools import accumulate, starmap numbers = range(0, 21, 5) list(starmap(lambda a, b: b / a, enumerate(accumulate(numbers), start=1))) [0.0, 2.5, 5.0, 7.5, 10.0] How this works: list(numbers) [0, 5, 10, 15, 20] list(accumulate(numbers)) [0, 5, 15, 30, 50] list(enumerate(accumulate(numbers), start=1)) [(1, 0), (2, 5), (3, 15), (4, 30), (5, 50)] import operator name = \"Emily\" list(starmap(operator.mul, enumerate(name, 1))) ['E', 'mm', 'iii', 'llll', 'yyyyy'] ","date":"2021-12-13","objectID":"/python-built-in-heroes/:2:5","tags":["python"],"title":"Python built-in heroes","uri":"/python-built-in-heroes/"},{"categories":null,"content":"dropwhile() From docs def dropwhile(predicate, iterable): iterable = iter(iterable) for x in iterable: if not predicate(x): yield x break for x in iterable: yield x predicate = lambda x: x \u003c 5 iterable = [1, 2, 3, 6, 7, 3] list(dropwhile(predicate, iterable)) [6, 7, 3] What happens here? iter() is used so that the iterable becomes an iterator (which gets emptied as it‚Äôs being iterated over). The first for loop moves until the first element fails the condition in predicate, at which point that element is yielded and the program breakes out of that for loop, advancing to the next. Because of step 1, iterable now only contains all elements after the element that caused the previous for loop to break, and all of these are yielded. def sensemaker(predicate, iterable): iterable = iter(iterable) for x in iterable: if not predicate(x): print(\"First loop\") print(x) break print(\"Second loop\") for x in iterable: print(x) sensemaker(predicate, iterable) First loop 6 Second loop 7 3 def sensemaker(predicate, iterable): # iterable = iter(iterable) for x in iterable: if not predicate(x): print(\"First loop\") print(x) break print(\"Second loop\") for x in iterable: print(x) sensemaker(predicate, iterable) First loop 6 Second loop 1 2 3 6 7 3 If we don‚Äôt turn the iterable into an iterator, it doesn‚Äôt get exhausted and the second loop simply loops over all its objects. ","date":"2021-12-13","objectID":"/python-built-in-heroes/:2:6","tags":["python"],"title":"Python built-in heroes","uri":"/python-built-in-heroes/"},{"categories":null,"content":"More itertools From more itertools import more_itertools more_itertools.take(4, more_itertools.pairwise(itertools.count())) [(0, 1), (1, 2), (2, 3), (3, 4)] ","date":"2021-12-13","objectID":"/python-built-in-heroes/:2:7","tags":["python"],"title":"Python built-in heroes","uri":"/python-built-in-heroes/"},{"categories":null,"content":"Functools ","date":"2021-12-13","objectID":"/python-built-in-heroes/:3:0","tags":["python"],"title":"Python built-in heroes","uri":"/python-built-in-heroes/"},{"categories":null,"content":"partial print(operator.mul(2, 3)) tripple = partial(mul, 3) tripple(2) 6 6 ","date":"2021-12-13","objectID":"/python-built-in-heroes/:3:1","tags":["python"],"title":"Python built-in heroes","uri":"/python-built-in-heroes/"},{"categories":null,"content":"reduce() from functools import reduce Basic use: reduce(operator.mul, [1, 2, 3, 4]) 24 Application: import pandas as pd df = pd.DataFrame( {\"AAA\": [4, 5, 6, 7], \"BBB\": [10, 20, 30, 40], \"CCC\": [100, 50, -30, -50]} ) What I usually do crit1 = df.AAA \u003e 5 crit2 = df.BBB \u003e 30 crits = crit1 \u0026 crit2 df[crits] AAA BBB CCC 3 7 40 -50 Alternative using functools.reduce() import functools crit1 = df.AAA \u003e 5 crit2 = df.BBB \u003e 30 crits = [crit1, crit2] mask = functools.reduce(lambda x, y: x \u0026 y, crits) df[mask] AAA BBB CCC 3 7 40 -50 ","date":"2021-12-13","objectID":"/python-built-in-heroes/:3:2","tags":["python"],"title":"Python built-in heroes","uri":"/python-built-in-heroes/"},{"categories":null,"content":"Misc. ","date":"2021-12-13","objectID":"/python-built-in-heroes/:4:0","tags":["python"],"title":"Python built-in heroes","uri":"/python-built-in-heroes/"},{"categories":null,"content":"Iterable unpacking When looping over a list of records (maybe of unequal length), we can access each records elements directly using star expressions. (From Python Cookbook recipe 1.2.) records = [(\"foo\", 1, 2), (\"bar\", \"hello\")] # conventional loop for record in records: print(record) ('foo', 1, 2) ('bar', 'hello') # accessign items for a, *b in records: print(a) print(b) foo [1, 2] bar ['hello'] # example use def do_foo(x, y): print(f\"foo: args are {x} and {y}.\") def do_bar(x): print(f\"bar: arg is {x}.\") for tag, *args in records: if tag == \"foo\": do_foo(*args) elif tag == \"bar\": do_bar(*args) foo: args are 1 and 2. bar: arg is hello. ","date":"2021-12-13","objectID":"/python-built-in-heroes/:4:1","tags":["python"],"title":"Python built-in heroes","uri":"/python-built-in-heroes/"},{"categories":null,"content":"iter() Basic use a = iter([1, 2, 3]) next(a), next(a), next(a) (1, 2, 3) Creating a callable_iterator: roll a die until a 6 is rolled import random def roll(): return random.randint(1, 6) roll_iter = iter(roll, 6) roll_iter \u003ccallable_iterator at 0x112c75850\u003e for r in roll_iter: print(r) 1 2 4 2 list(iter(roll, 4)) [3, 1, 2, 5, 3, 5, 3] To read file until an empty line: with open(\"filepath\") as f: for line in iter(f.readline, \"\"): process_line(line) ","date":"2021-12-13","objectID":"/python-built-in-heroes/:4:2","tags":["python"],"title":"Python built-in heroes","uri":"/python-built-in-heroes/"},{"categories":null,"content":"filter() a = [0, 1, 0, 2, 3] non_zero = list(filter(lambda x: x != 0, a)) non_zero [1, 2, 3] a = [[1, 2, 3], [4, 5, 6], [7, 8, 9]] [a[i] for i in range(3)] [[1, 2, 3], [4, 5, 6], [7, 8, 9]] def unique(array): return len(array) == len(set(array)) cols = [[a[row][col] for row in range(3)] for col in range(3)] all(unique(a) for a in cols) True ","date":"2021-12-13","objectID":"/python-built-in-heroes/:4:3","tags":["python"],"title":"Python built-in heroes","uri":"/python-built-in-heroes/"},{"categories":null,"content":"String import string string.ascii_letters 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ' string.punctuation '!\"#$%\u0026\\'()*+,-./:;\u003c=\u003e?@[\\\\]^_`{|}~' ","date":"2021-12-13","objectID":"/python-built-in-heroes/:4:4","tags":["python"],"title":"Python built-in heroes","uri":"/python-built-in-heroes/"},{"categories":null,"content":"Sources David Beazley talk (the inspiration for the title) Fluent Python Pandas cookbook Elements of programming interviews in Python (EPI) ","date":"2021-12-13","objectID":"/python-built-in-heroes/:5:0","tags":["python"],"title":"Python built-in heroes","uri":"/python-built-in-heroes/"},{"categories":null,"content":"Solve the below tasks and state their time and space complexities. ","date":"2021-11-23","objectID":"/python-lists/:0:0","tags":["python"],"title":"Python lists","uri":"/python-lists/"},{"categories":null,"content":"Basics Define a list. A list is a finite, ordered, and mutable sequence of elements. Create a list a containing the letters a, b, and c. a = list(\"abc\") Append z. a.append(\"z\") Insert x at the second position. a.insert(1, \"x\") Append the characters m, n. a.extend(\"mn\") Remove the first occurrence of x from the list. a.remove(\"x\") Remove the second to last element from the list. del a[-2] Remove and return the last item. a.pop() Remove and return the second item. a.pop(1) Check whether c is in the list. \"c\" in a Return the index of c. a.index(\"c\") Count occurrences of ‚Äòc‚Äô. a.count(\"c\") Sort the list in place. a.sort() Insert ‚Äòz‚Äô in the first position. a.insert(0, \"z\") Replace the just inserted z with ‚Äòk‚Äô a[0] = \"k\" Create a sorted copy of the list. sorted(a) Reverse the order in place. a.reverse() Create a reversed iterator. reversed(a) Delete all elements from the list. a.clear() ","date":"2021-11-23","objectID":"/python-lists/:1:0","tags":["python"],"title":"Python lists","uri":"/python-lists/"},{"categories":null,"content":"Deep and shallow copies a = [1, 2, [3]] Create shallow copies b, c, d, and e of a, all in different ways. import copy b = list(a) c = a[:] d = a.copy() e = copy.copy(a) Check that the new lists are indeed shallow copies. all(a is not copy and a[2] is copy[2] for copy in [b, c, d]) True Create a deep copy e of a. e = copy.deepcopy(a) Check that the new list is a deep copy. e is not a and e[2] is not a[2] ","date":"2021-11-23","objectID":"/python-lists/:2:0","tags":["python"],"title":"Python lists","uri":"/python-lists/"},{"categories":null,"content":"List comprehensions colors = [\"blue\", \"yellow\"] sizes = \"SML\" Reproduce the output of the below using a list comprehension. results = [] for color in colors: for size in sizes: results.append((color, size)) results results = [(color, size) for color in colors for size in sizes] results Create a copy of results sorted by size in ascending order. sorted(results, key=lambda x: x[1], reverse=True) ","date":"2021-11-23","objectID":"/python-lists/:3:0","tags":["python"],"title":"Python lists","uri":"/python-lists/"},{"categories":null,"content":"Summing elements a = [1, 2, 3, 4, 5] Sum a using the built-in method. sum(a) Sum the list using a recursive algorithm. def rec_sum(items): if not items: return 0 head, *tail = items return head + rec_sum(tail) if tail else head 8 Sum the list using a for loop. def for_sum(items): result = 0 for item in items: result += item return result for_sum(a) Sum the list using a while loop without altering the input. def while_sum(items): result = i = 0 while i \u003c len(items): result += items[i] i += 1 return result while_sum(a) ","date":"2021-11-23","objectID":"/python-lists/:4:0","tags":["python"],"title":"Python lists","uri":"/python-lists/"},{"categories":null,"content":"Misc. What is c? a = 1 b = 2 c = [a, b] a = 2 c [1, 2] Find the indices of the min and max elements in the list below. a = [1, 3, 4, 9, 9, 10, 2, 4, 2, 33] a.index(min(a)), a.index(max(a)) ","date":"2021-11-23","objectID":"/python-lists/:5:0","tags":["python"],"title":"Python lists","uri":"/python-lists/"},{"categories":null,"content":"Removing duplicates Write an algorithm to remove duplicates from a list while maintaining it‚Äôs original order (from Python cookbook recipe 1.10). def dedupe(items): seen = set() deduped = [] for item in items: if item not in seen: deduped.append(item) seen.add(item) return deduped dedupe([1, 1, 2, 3, 2]) Use a generator function to achieve the same. def dedupe_gen(items): seen = set() for item in items: if item not in seen: seen.add(item) yield item for item in dedupe_gen([1, 1, 2, 3, 2]): print(item) Remove duplicates from the below sorted list by updating the original list a = [1, 2, 2, 3, 4, 4, 4, 5, 6, 6, 7] def dedupe_inplace(items): write_index = 1 for i in range(2, len(items)): if items[i] != items[write_index - 1]: items[write_index] = items[i] write_index += 1 return items[:write_index] dedupe_inplace(a) [1, 2, 3, 4, 5, 6, 7] ","date":"2021-11-23","objectID":"/python-lists/:5:1","tags":["python"],"title":"Python lists","uri":"/python-lists/"},{"categories":null,"content":"Implementing a stack Implement a stack with push(), pop(), peek(), and is_empty() methods. class Stack: def __init__(self): self.data = [] def push(self, item): self.data.append(item) def pop(self): self.data.pop() def peek(self): return self.data[-1] def is_empty(self): return len(self.data) == 0 ##¬†Implementing a queue Implement a queue with basic operations enqueue(), dequeue(), and is_empty(). class Queue: def __init__(self): self.data = [] def enqueue(self, item): self.data.append(item) def dequeue(self): return self.data.pop(0) def is_empty(self): return len(self.data) == 0 ","date":"2021-11-23","objectID":"/python-lists/:6:0","tags":["python"],"title":"Python lists","uri":"/python-lists/"},{"categories":null,"content":" The goal of algorithm analysis is to study the efficiency of an algorithm in a language and machine-independent way. The two most important tools for this are (1) the RAM model of computation and (2) the asymptotic analysis of worst-case complexity. The Random Access Machine (RAM) model of computation is a simple model of a generic computer that is based on three main assumptions: (1) each simple operation takes exactly one time step, (2) loops and subroutines are considered composites of all simple operations they perform, and (3) memory access from cache and RAM takes one time unit. None of these hold in practice, but the model is extremely useful because it captures the essence of algorithm behaviour while being very simple to work with. Best, worst, and average-case complexity are functions defined by the minimum, maximum, and average number of steps taken for any instance of size n of the input string. (Think about a graph with n on the x-axis and number of steps on the y-axis, with number of steps for each instance of a problem of size n forming columns of dots with increasing variation as n ‚Äì and thus the number of possible instances ‚Äì increases. The three functions trace the lowest, highest, and middle dots at each input size n. See Fig 2.1 in ADM.) Using these functions to analyse algorithms is impractical, however, because they are not smooth and require lots of detail about the algorithm and its implementation. Big O notation ignores such details and focuses on the essentials to capture the rate at which runtime (or space requirements) grow as a function of the input size (the letter O is used because the growth rate of a function is also called its order). In essence, this means only focusing on the higest order term and ignoring constants (which depend on things like hardware and programming language used to run the algorithm). A function $f(n)$ is $O(g(n))$ if there exist constants $c$ and $n_0$ such that $f(n) \\leq cg(n)$ for any $n \u003e n_0$. Intuitively, this means that $f(n)$ grows no faster than $cg(n)$ above a certain input size. For example: $T(n) = 2n^2 + 3n$ is $O(n^2)$, since $5n^2 \\geq 2n^2 + 3n$ for all positive values of $n$. Amortised worse-case complexity takes into account that the running time of a given operation in an algorithm may take a very long or a very short time depending on the situation, and averages those different running times of the operation in a sequence over that sequence. Adding an element to an array that is dynamically resized takes $O(1)$ time until the array is full, when the array needs to create a new array of twice its original size, copy all elements over to the new array, and add the new element, which takes $O(n)$ time. Average worst-case complexity averages these runtimes to find that pushing elements onto a dynamically resized array takes: $\\frac{nO(1) + O(n)}{n + 1} = O(1)$, constant time. (Source) Exercises: Is $2^{n+1} = O(2^n)$? Is $(x + y)^2 = O(x^2 + y^2)$? What‚Äôs the time complexity of $f(n) = min(n, 100)$? Solutions: The way to go is to start from the definition. The statement is true if there is a $c$ and $n_0$ for which $c2^n \\geq 2^{n+1}$ for $n \u003e n_0$. The key is to rewrite the right hand side to $c2^n \\geq 2 \\times 2^n$, which makes it obvious that the statement holds whenever $c \\geq 2$. Starting from the definition, the statement is true if there exist constants $c$ and $n_0$ for which $c(x^2 + y^2) \\geq (x + y)^2$ for $n \u003e n_0$. Expanding the right hand side, we get $c(x^2 + y^2) \\geq x^2 + 2xy + y^2$. Ignoring the middle term, the statement holds for $c = 1$; considering only the middle term, we see that it is largest when $x = y$, in which case the statement holds for $c = 2$. Thus, $3(x^2 + y^2) \\geq (x + y)^2$, so the statement is true. I reflexively answered $n$. Thinking for a moment (an embarassingly long one, admittedly), I realised that $n$ here refers not to the length of an array but to a single number. So the operation is $O(1)$. ","date":"2021-11-21","objectID":"/algorithm-complexity-analysis/:0:0","tags":["cs"],"title":"Algorithm complexity analysis","uri":"/algorithm-complexity-analysis/"},{"categories":null,"content":"Sources Steven Skiena, The Algorithm Design Manual (ADM) MIT, Big O notation Wikipedia, Big O notation ","date":"2021-11-21","objectID":"/algorithm-complexity-analysis/:1:0","tags":["cs"],"title":"Algorithm complexity analysis","uri":"/algorithm-complexity-analysis/"},{"categories":null,"content":"My regular expression cheatsheet, focused on the Python engine. A lot of content is heavily based on (and often shamelessly copied from) the amazing RexEgg and Regular-Expressions.info sites. ","date":"2021-11-13","objectID":"/regex/:0:0","tags":["tools","cheatsheet"],"title":"Regex","uri":"/regex/"},{"categories":null,"content":"Basics A regex is a text string that a regex engine uses to find text or positions in a body of text, typically for the purposes of validating, finding, replacing, and splitting. To differentiate between the string that makes up the regex and the string that is being searched, the former is often called regex or pattern and the latter string or subject. A (lexical) token is a string with an assigned and thus identified meaning (more here). For instance, the token \\w in a pattern stands for a word-character, and will be replaced by that when the engine parses the string. The Python regex engine (and all other modern engines) is regex-directed: it attempts all possible permutations of the regex at a character position of the subject before moving on to the next character (which can involve lots of backtracking). In contrast, text-directed engines visit each character in the subject text only once. This makes them faster but also less powerful. A regex engine is eager: it scans alternatives in the regex from left to right and returns the first possible match ‚Äì Jane|Janet would return Jane as a match in Janet is tall. ","date":"2021-11-13","objectID":"/regex/:1:0","tags":["tools","cheatsheet"],"title":"Regex","uri":"/regex/"},{"categories":null,"content":"Characters There are four types of characters: literal characters (e.g. a), metacharacters (^), non-printable characters (\\n), and shorthand character classes (\\w). Literal characters simply match themselves: the single literal character a matches the first a in the string, the sequence of literal characters cat the first occurrence of cat. Metacharacters are the twelve punctuation characters from the ACSII table that make regex work its magic: $, (, ), *, +, ., ?, [, \\, ^, {, |. To match metacharacters as literals, escape them with a backslash, as in 1\\+2=3. Exceptions are { and }, which are only treated as metacharacters when part of a quantifier, and ], which only takes on special meaning when part of a character class). Non-printable characters or formation marks are characters used to tell word processors how text needs to look and do not appear in printed text. Shorthand character classes are tokens for certain common character classes. Non-printable characters Character Legend \\r Carriage return \\n Newline \\r\\n Line-break on Windows \\t Tab \\f Form-feed Shorthand character classes Character Legend \\d Single digit, short for [0-9] \\D Complement of \\d, short for [^\\d] \\w Word character, short for [a-zA-Z\\_] \\W Complement of \\w, short for [^\\w] \\s Whitespace character, short for [\\r\\n\\t\\f\\v ] \\S Complement of \\s, short for [^\\s] \\v Vertical whitespace, short for [\\n\\f\\r] \\V Complement of \\v ","date":"2021-11-13","objectID":"/regex/:2:0","tags":["tools","cheatsheet"],"title":"Regex","uri":"/regex/"},{"categories":null,"content":"Character classes Character classes tell the engine to match one of several characters. Importantly: [^a-z]u does not mean ‚Äúu not preceded by a lowercase letter‚Äù, but ‚Äúu preceded by something that isn‚Äôt a lowercase letter‚Äù. Hence, the pattern doesn‚Äôt match a u at the beginning of a string. (In contrast to the ., the negated character class does match invisible line breaks, though, so the above pattern would match the string \\nu.) Within a character class, metacharacters are literals with the exception of ^, -, \\ and ] if they are used in places where they have special meaning: ^ at the beginning, - as part of a range, ] at the end, and \\ to escape another special character or to form a token (i.e. always), and ] at the end. Hence, this regex matches them all as literals: []\\\\-^] (for details on how to includ metacharacters indide character classes without escaping them, see relevant section here here]. Character classes examples Regex Match [ab] One of a or b [^ab] Any character that isn‚Äôt a or b (incl. newline) [\\w\\s] One word or whitespace character [A-By-z1-2] One of A, B, y, z, 1, 2 [ -~] Any character in the printable section of the ASCII table Exercises: Match gray and grey in London is grey; New York, gray.. Match any character that is neither a digit nor a (hidden) line break character. What does q[^u] match in Iraq and Iraq is a country? How could we match any q not followed by a u? Search for a literal * or +. Match any number greater than 10 made up of all the same digit (e.g. 222, 33, 5555). In b ab cb, match b either at the beginning of the string or when preceded by an a. What‚Äôs the difference between [\\D\\S] and [^\\d\\s]? Solutions: \\bgr[ae]y\\b. Discussion: need global flag on or use findall() in Python to match both words, otherwise I‚Äôll just get the first one. [\\D\\V]. Discussion: the negated character classs matches hidden line break character by default (unlike the .), so need to explicitly exclude them. Nothing and q . Discussion: the regex means ‚Äúq followed by something that is not a u‚Äù, not ‚Äúq not followed by a u‚Äù, so it requires something to follow the q, and that something to not be a u. That something, which happens to be a whitespace in the second string, is part of the match. q(?!u). [*+]. Discussion: these two characters have no special meaning within a character class, so no need to escape them. (\\d)\\1+. Discussion: need a capturing group for this ([\\d]{2,}, for instance, would match any two digit number). (?:^|a)b. Discussion: [a^]b would not work here, since ^ is matched literally inside a character class, so need a non-group with an alternation. [\\D\\S] matches a single character present inside the character class, so a character that is either not a digit or not a space, which is every character. [^\\d\\s] matches a single character that is not present inside the character class, so that is neither a digit nor a space, which is all letters. ","date":"2021-11-13","objectID":"/regex/:3:0","tags":["tools","cheatsheet"],"title":"Regex","uri":"/regex/"},{"categories":null,"content":"Quantifiers A quantifier tells the engine to match the immediately preceding character, token, or subexpression (e.g. (a|b)) a certain number of times. Nomenclature: Greedy quantifiers: the default property of quantifiers to match as many characters as possible and thus return the longest possible match. E.g. \\d+ matches 123 (not 1 or 12) against 123. Docile quantifiers: the property of a greedy quantifier to backtrack and give up characters to try the rest of the pattern to match. This is the property behind the ‚Äúpossible‚Äù in the above definition: a greedy quantifier matches as many characters of the quantified token as it can for the overall pattern to match. E.g. .*c will run all the way to the end of the string abc, fail to match the c in the pattern, backtrack and give up the c character matched by dot-star, try again to match c, succeed, and return. Lazy quantifiers: the property of a quantifier to match as few characters as necessary and thus return the shortest possible match. Quantifiers are made lazy by appending ?. Example: \\d+? matches 1 against 123. Lazy quantifiers are expensive: laziness and helpfulness make the engine advance from the beginning to the end of a string character by character, at each step expanding the match by including the next character, advancing and attempting to match the rest of the pattern, fail, backtrack, and repeat until it finds a match or reaches the end of the string. Helpful quantifiers: the property of a lazy quantifier to backtrack and match additional characters of the quantified token in the attempt to match the rest of the pattern. This is the property behind the ‚Äúnecessary‚Äù in the above definition: a lazy quantifier matches as few characters as it can for the overall pattern to match. E.g. a*?b will first match zero as against aab, advance and try to match the b, fail, backtrack and match the first a, advance and try to match the b, fail again, backtrack and match the second a, advance and try to match the b, succeed and return. Possessive quantifiers: an optional property of a quantifier that prevents it from giving up previously matched characters if the rest of the pattern doesn‚Äôt match (i.e. it makes the quantifier non-docile). We can make a quantifier possessive by appending a +. Example: a++ greedily matches as many as as it can and never gives any of them back. Quantifiers and modifiers: Token Legend ? Match zero or once \\* Match zero or more + Match once or more {n} Match n times {n,m} Match between n and m times, n defaults to 0, m to infinity ? Make quantifier lazy {quantifier}+ Make quantifier possessive Matches in string a5aa5: Pattern Matches in GLOBAL mode (total: list) a 3: a, a, a a+ 2: a, aa a+? 3: a, a, a a* 5: a, ‚Äò‚Äô, aa, ‚Äò‚Äô, '‚Äô a*? 6: ‚Äò‚Äô, ‚Äò‚Äô, ‚Äò‚Äô, ‚Äò‚Äô, ‚Äò‚Äô, ‚Äò‚Äô, '‚Äô a? 6: a, ‚Äò‚Äô, a, a, ‚Äò‚Äô, '‚Äô a?? 6: ‚Äò‚Äô, ‚Äò‚Äô, ‚Äò‚Äô, ‚Äò‚Äô, ‚Äò‚Äô, '‚Äô a{2} 1: aa a{,2} 5: a, ‚Äò‚Äô, aa, ‚Äò‚Äô, '‚Äô a{,2}? 6: ‚Äò‚Äô, ‚Äò‚Äô, ‚Äò‚Äô, ‚Äò‚Äô, ‚Äò‚Äô, '‚Äô a{1,} 2: a, aa The greedy trap In the string {start} Mary {end} had a {start} little lamb {end}, match all tokens that start with {start} and end with {end}. The naive solution is, {start}.*{end}, which will run over the first {end} to match entire string since .* is greedy ‚Äì this is the greedy trap. Solutions: Lazy quantifier: {start}.*?{end}. Computationally inefficient as it proceeds character-by-character left to right with successive backtracking. Negated character class: {start}[^{]*{end}. An example of the contrast principle, but works only if { never appears inside the delimiters. Tempered greedy token: {start}(?:(?!{end}).)*{end}. Ensures that the . never matches the opening bracket of {end}, thus making sure we don‚Äôt run over an end token. This allows the { to occur inside the string. Because it requires a lookahead at east step, it is no more efficient than the lazy quantifier solution in this case, though. Explicit greedy alternation: {start}(?:[^{]|{(?!end}))*{end}. This is an example of the say what you want principle: we either want to match characters that are","date":"2021-11-13","objectID":"/regex/:4:0","tags":["tools","cheatsheet"],"title":"Regex","uri":"/regex/"},{"categories":null,"content":"Anchors and word boundaries Anchors assert that the engine‚Äôs current position in a string matches a certain position like the beginning or the end, while boundaries make assertions about what can and cannot be matched to the left and the right of the current position. ^ matches the position just before the first character of the string, so ^a means ‚Äúposition just before start of string followed by a‚Äù. Similarly, $ matches position just after the last character in the string, and c$ means ‚Äúc followed by the position just after the end of the string‚Äù. Multiline mode makes ^ and $ match positions just before first and just after last character of the line rather than the entire string. $ subtlety: if the very last character in a line is a line break, $ matches both just before it and at the very end just after it (i.e. \\d+$ matches 123 in both 123 and 123\\n. This is true regardless of whether multiline mode is turned on. (If there are multiple line breaks at the end, the above behaviour only applies to the final one, so \\d+$ would not match 123 in 123\\n\\n.) \\z vs \\Z: similar to the above point, in most engines \\z matches only at the very end of a string (i.e. after the linebreak if there is one), while \\Z is the flexible end-of-string anchor that can match before and after the linebreak at the end of a string. In Python, \\Z behaves like \\z, and \\z doesn‚Äôt exist. Character Legend ^ Matches beginning of string or line (in multiline mode) $ Matches end of string or line (in multiline mode) \\A Matches only beginning of string \\Z Matches only very end of string (same as \\z in most other engines) \\b Matches if one side is a word character and the other isn‚Äôt \\B Matches wherever \\b doesn‚Äôt Exercises: Match cat a) on its own or at the end of a word (e.g. tombcat), b) on its own or at the beginning of a word (e.g. catwalk), c) only on its own, d) fully surrounded by word characters, e) fully surrounded or at the beginning or the end but not on its own. Match Jane or Janet. Create a boundary that detects the edge between a letter and a non-letter. In the string 0# 1 #2 #3# 4# #5, match digits where each side is either a hash or the edge of the string (i.e. 0, 3, 5). Within the vernacular of RexEgg, explain the difference between an anchor, a boundary, and a delimiter. Implementing ^ manually: write patterns that match a at the beginning of a) the string, b) each line, c) line three and beyond. Solutions: a) cat\\b b) \\bcat, c) \\bcat\\b, d) \\Bcat\\B, e) \\Bcat|cat\\B. \\bJanet?\\b or \\b(Jane|Janet)\\b. (?i)(?\u003c![a-z])(?=[a-z])|(?\u003c=[a-z])(?![a-z]). Discussion: RexEgg uses the following alternative: (?i)(?\u003c=^|[^a-z])(?=[a-z])|(?\u003c=[a-z])(?=$|[^a-z]). The two versions are the same, but I find the first version easier to read. Using negated character classes requires that we explicitly allow for ^ and $, since otherwise the regex engine tries to match a character that isn‚Äôt a letter and fails. Using negative lookarounds solves this, since these succeed whenever the engine cannot match a lowercase letter in the specified position in the string, which is also true if there is a beginning or end of line character in that position. Using capturing group: (?:^|#)(\\d)(?:$|#). Using lookarounds: (?\u003c=[#^])\\d(?=[#$]). Using double negative delimiter: (?\u003c![^#])\\d(?![^#]). The lookbehind asserts: ‚Äúwhat immediately precedes the current position is not a character that is not a hash‚Äù, which, turning the logic around, is equivalent to ‚Äúeither not a character or a hash‚Äù. The logic of the lookahead is similar. This is thus a clever way to match either a particular (set of) characters or the edge of a string. (This works for single-line strings only, as in multiline strings, \\n characters at the beginning and end are characters that aren‚Äôt a hash.) They all make assertions about the current position in the string: anchors assert that what immediately precedes or follows the furrent position is a particular position in the string such as the beginning of the string or the","date":"2021-11-13","objectID":"/regex/:5:0","tags":["tools","cheatsheet"],"title":"Regex","uri":"/regex/"},{"categories":null,"content":"Alternation Character classes like [ab] tell the regex engine to match a single character out of several possible characters; alternations like (Jane|Bob), to match a single regex out of several possible regexes. Exercises: What do cat|dog, \\bcat|dog\\b, and \\b(cat|dog)\\b match? Find all occurrences of Get, GetValue, Set, SetValue. Solutions: The first matches any occurrences of the strings (e.g. cat in uncategorised or dog in dogmatic), the second matches words that begin with cat and words that end with dog, the third cat and dog on their own. (Get|Set)(Value)? is reasonably concise and easy to read. It works because ? is greedy, which means it attempts to match GetValue before Get, assuring that it never matches Get in GetValue. ","date":"2021-11-13","objectID":"/regex/:6:0","tags":["tools","cheatsheet"],"title":"Regex","uri":"/regex/"},{"categories":null,"content":"Groups Grouping part of a regex together can be useful to apply a quantifier to a group of tokens, to restrict alternation to a part of the pattern, and to use backreference. There are three types of groups: capturing, non-capturing, and atomic. Capturing groups have three main uses: 1) reuse matches using backreferences, 2) use captured text as replacement text in search and replace, 3) use captured parts in applications. Capturing groups get numbered from left to right, and, if they have a quantifier, return the value of the last captured iteration. Non-capturing groups allow for avoiding the capturing overhead when grouping is needed but capturing isn‚Äôt. Atomic groups become solid as a block once the engine leaves the group and thus prevent the engine from backtracking into the group even if the rest of the expression fails to match. This can be useful to avoid unwanted backtracking when groups contain quantifiers or alternation. In the former case, we could also use possessive quantifiers. Character Legend (regex) Capturing group \\1, ..., \\99 Backreferences to capturing groups (?:regex) Non-capturing group (?P\u003cname\u003eregex) Named capturing group in re module (?P=name) Backreference to named capturing group in re (?\u003cname\u003eregex) Named capturing group in regex module \\g\u003cname\u003e Backreference to named capturing group in regex (?\u003eregex) Atomic group Exercises: Find magical dates, dates where the two final year digits are identical to the day and month digits (e.g. 2008-08-08). Capture day, month, year in dd-mm-yyyy dates. Name the groups in the above example (use the regex module). Search for magic dates using a named group. What‚Äôs the difference between the result returned from (\\w+) and (\\w)+ when matching the string Hello? Find all patterns of the form sum of digits = reversed sum of digits (e.g. 22 + 333 = 333 + 22). Find all cases of repeated words in Hello world world this was some great greatness, wasn‚Äôt wasn‚Äôt it?. A typical URL has the form \u003cprotocol\u003e://\u003chost\u003e/\u003cpath\u003e (e.g. https://www.abc.com/index.html). Write a regex that captures the host and, if available, the path but not the protocol, yet validates that the protocol is either http or https (inspired by this SO post). Write a regex similar to the previous one, but now validate that the host is one of http, https, or s3. In strings containing Bob says: word, group the entire regex but only capture the word Bob says (e.g. in Bob says: hello capture hello. In strings containing tokens of the form upNUMBER or downNUMBER, capture the tokens. Does (?\u003eA|.B)C match against ABC? Does (?\u003ea+)[a-z]c match against aac? Solutions: \\d\\d(\\d\\d)-\\1-\\1. (\\d{2})-(\\d{2})-(\\d{4}). (?\u003cday\u003e\\d{2})-(?\u003cmonth\u003e\\d{2})-(?\u003cyear\u003e\\d{4}). \\d\\d(?\u003cyy\u003e\\d\\d)-\\g\u003cyy\u003e-\\g\u003cyy\u003e (\\w+) returns Hello; (\\w)+, o. Discussion: a capturing group with a quantifier contains the last matched iteration. (\\d+) \\+ (\\d+) = \\2 \\+ \\1. (\\b[\\w']++\\b) \\b\\1\\b. Discussion: If we don‚Äôt use word boundaries we‚Äôd also match the ss in was some and great in great greatness, and without allowing for ‚Äô we‚Äôd miss the repetition of wasn‚Äôt. Adding a possessive quantifier avoids unnecessary backtracking, which we never want here, as we always want to capture full words only. https?://([^/\\s]+)(\\S+)?. Discussion: The \\s inside the character class ensures the match doesn‚Äôt include characters that follow the url. We could include / in the second capturing group to be explicit that what immediately follows the host starts with a forward slash, but it‚Äôs redundant because the first group matches up to a space character or a forward slash and the second group only matches if what follows directly thereafter is not a space, character, which implies that the second group can only return a match if its content starts with a forward slash. (?:https?|s3)://([^/\\s]+)(\\S+)?. Discussion: remind yourself why we cannot just add an alternation without a non-capturing group. (?:Bob says: (\\w+)). Discussion: a contrived use of a capturing group within a non-capturi","date":"2021-11-13","objectID":"/regex/:7:0","tags":["tools","cheatsheet"],"title":"Regex","uri":"/regex/"},{"categories":null,"content":"Lookarounds Just like anchors, lookarounds are zero-length assertions that determine whether a match is possible in a given location. The difference to anchors is that lookarounds match characters rather than positions in the string, but then give up the matched strings and simply return whether or not they existed. The last step is what makes them zero-length matches, which means the regex engine stays at the current position in the subject string rather than advancing. Lookaheads can contain any regex, lookbehinds can‚Äôt: they have to be fixed-length expressions (i.e. literalse, character escapes, character classes, or alternations where all alternatives are of equal length, but not quantifiers or backreferences). This is because the regex engine steps back by the length of the lookbehind to evaluate matches, and thus needs to know said length. (Actually, the regex module can handle flexible-width lookbehinds.) As a result of the above, capturing groups can only be used in lookaheads. To use them, simply wrap the regex in parentheses. Lookarounds don‚Äôt look way into the distance: (?=A) doesn‚Äôt mean ‚Äúthere is an A somewhere to the right‚Äù, it asserts that what immediately follows is an A. To look into the distance, you have to include ‚Äúbinoculars‚Äù such as .* or, if possible, more specific tokens. Lookaround Name What it does (?=foo) Lookahead Asserts that what immediately follows the current position in foo (?\u003c=foo) Lookbehind Asserts that what immediately precedes the current position is foo (?!foo) Negative\\nLookahead Asserts that what immediately follows the current position is not foo (?\u003c!foo) Negative\\nLookbehind Asserts that what immediately precedes the current position is not foo Practice: Write \\A using a lookaround. Solution if DOTALL mode is on: (?\u003c!.) Solution if DOTALL mode is off: (?\u003c![\\D\\d]). Discussion: If DOTALL mode is on an . matches every character including linebreaks, the first lookbehind asserts that what precedes the current position is not any character, so the position must be the beginning of the string. [\\D\\d] matches any character that is a digit or not a digit, which is any character, and thus achieves the same thing if DOTALL mode is off. Match e if followed either by aa or bb. Solution: e(?=([ab])\\1). What does q(?=u)i match in ‚Äúquit‚Äù? Solution: nothing. The regex tries to match a u and an i at the same position. Match words that don‚Äôt end in s. Solution: \\b\\w+(?\u003c!s)\\b. Explanation: approach matches words and, at the end postition, looks back to check that the character that immediately precedes the current position, which is the last character, is not an ‚Äús‚Äù. Needs word boundaries to prevent engine from backtracking and match word without final s. Explain why A(?=5)(?=[a-z]) doesn‚Äôt match A5k and write a regex that does. Solution: Because lookarounds stand their ground: they don‚Äôt alter the position in the string, so the second lookahead also starts at A and finds a 5 rather than a letter. A(?=5[a-z]) does the job. Validate that a password meets the following conditions: 1) must have between 6 and 10 word characters, 2) must include at least one lowercase character, 3) must include at least three uppercase characters, 4) must include a digit. Match valid passwords. Solution: 1) \\A(?=\\w{6,10}\\Z), 2) (?=[^a-z]*[a-z]), 3) (?=(?:[^A-Z]*[A-Z]){3}), 4) (?=[\\D]*\\d), to match: .*. Complete solution: \\A(?=\\w{6,10}\\Z)(?=[^a-z]*[a-z])(?=(?:[^A-Z]*[A-Z]){3})(?=\\D*\\d).* Discussion: Why can‚Äôt we just use [a-z] to check for condition 2? This will find a match if the string contains a lowercase letter, but, naturally, the engine will also move to the position of that matching character, whereas we want to stay at the first character to perform subsequent lookaheads, so we need a match that starts at the first character. Show two ways how to remove the redundant lookahead in the above solution. Solution 1: \\A(?=[^a-z]*[a-z])(?=(?:[^A-Z]*[A-Z]){3})(?=\\D*\\d)\\w{6,10}\\Z Solution 2: \\A(?=\\w{6,10}\\Z)(?=[^a-z]*[a-z])(?","date":"2021-11-13","objectID":"/regex/:8:0","tags":["tools","cheatsheet"],"title":"Regex","uri":"/regex/"},{"categories":null,"content":"Flags and inline modifiers To use flags in Python‚Äôs re module, pass the keyword flags=re.FLAGNAME to the method (e.g. re.findall(pattern, string, flags=re.MULTILINE)). Flag (inline modifier) Legend [A]SCII (?a) Make tokens match ASCII rather than Unicode GLOBAL Don‚Äôt return after first match (use re.findall()) [I]GNORECASE (?i) Case insensitive matching [M]ULTILINE (?m) Make ^ and $ match end of line (not end of string) S, DOTALL (?s) Make . match newline (also called single-line) X, VERBOSE (?x) Allow linebreaks for easier-to-read regexes ","date":"2021-11-13","objectID":"/regex/:9:0","tags":["tools","cheatsheet"],"title":"Regex","uri":"/regex/"},{"categories":null,"content":"Subroutines and recursive expressions The backreference \\1 repeats the characters captured by the first capturing group; subroutine (?1), the pattern defined by the first capturing group. This can be very useful to make long expressions shorter. There is lots more to subroutines, but for my current use cases, the basics are enough. Recursive patterns are related to subroutines in that a soubroutine can call itself recursively. In addition, (?R) tries to match the entire pattern recursively (This description of the steps the engine takes is very useful). Exercises: Match instances of Harry meets Sally and Sally meets Harry. Match strings of the form ab, aabb*, aaabbb**. Match the same strings as above, but now as part of a larger string that might contain other characters, including aab, which we‚Äôd not want to match. Match stand-alone strings of the form bbmmee, bme, bbbmmmeee that might possibly occur as part of a larger string. \\b(b(?\u003e(?1)|m)*e)\\b Solutions: (Harry|Sally) meets (?1). a(?R)?b. \\b(a(?1)b)\\b. Discussion: this requires wrapping a recursively called subroutine in word boundaries. Using (?R) instead of (?1) would only match ab, since the recursion would also try to match repeated word boundaries. \\b(b(?\u003e(?1)|m)+e)\\b. Discussion: We need to use a subroutine rather than a recursion of the entire pattern for the same reason as in the previous exercise. The real action happens inside the capturing group: (b(?\u003e(?1)|m)+e) represents the generic pattern pattern to match balanced constructs (I use a + instead of a * quantifier on the atomic group, which ensures there is at least one middle element). How does it work? For the string bbmmee, the first b in the pattern matches, so the engine advances and reaches the alternation inside the atomic group, from which the subroutine matches the second b. The engine again moves on to the alternation, which now matches m greedily one or more times, meaning it eats up all the ms in the centre of the pattern. Finally, the engine tries to match the first e and succeeds, which means it has successfully matched the entire recursive call. The engine now goes back to the initial pattern and tries to match the final e, which also succeeds and results in a successful overall match. We use an atomic group to avoid the engine from unnecessary backtracking (e.g. after matching multiple ms but failing to match an e, the engine would release each m and attempt to match e again, which will never succeed). ","date":"2021-11-13","objectID":"/regex/:10:0","tags":["tools","cheatsheet"],"title":"Regex","uri":"/regex/"},{"categories":null,"content":"Character class set opetations The regex module has supports the set operations intersection, union, and subtraction on character classes. (Inner brackets are optional but can help with readability.) Operation Pattern String Matches Intersection r'[\\W]\u0026\u0026[\\S]]' a.k$_8 [‚Äô.‚Äô, ‚Äò$‚Äô] Union r'[ab||\\d]' a.k$_8 [‚Äòa‚Äô, 8] Subtraction r'[\\w--k]' a.k$_8 [‚Äòa‚Äô, ‚Äò_‚Äô, 8] ","date":"2021-11-13","objectID":"/regex/:11:0","tags":["tools","cheatsheet"],"title":"Regex","uri":"/regex/"},{"categories":null,"content":"Gotchas Based on this page. Exercises: Why doesn‚Äôt [a-z]+ match Cat? How can you fix it? Why doesn‚Äôt My .* cat match the below string? How can you fix it? My dog and my cat How can we avoid the regex cat from matching in the string certificate but find it in patterns like _cat12? The regex [128]|18 is supposed to match 1, 2, 8, and 18. In the string 18 18, (a) what does it match without any flags? (b) what does it match with the global flag on? (c) when would it match 18 and why? (d) how could it be improved to achieve its aim? We use the pattern x* with replacement string y. Running it on x, we get yy, running it on a we get yay. What‚Äôs going on? Solutions: Because regex is case-sensitive by default and thus, as writte, only matches lowercase characters. Could either us an inline modifier (?i)[a-z]+ or explicitly search for uppercase and lowercase characters [A-Za-z]+. Because . does not match line breaks by default. The easiest way to fix this is to use DOTALL mode (also called single-line mode) (?s) My .* cat. If we only ever wanted cat on its own, simple word boundaries \\bcat\\b would do. To match it when surrounded by non-letter word characters, we need real-word boundaries. (a) 1, (b) [1, 8, 1, 8]. This surprised me for a moment: remember that the engine scans alternatives in the regex left to right, eagerly returns the first match, and then moves on to the next character in the string. (c) Never, since it will always match the 1 and move on without attempting to match the right-hand side of the alternation in the pattern. (d) Depending on the context, we could just reverse the alternation to 18|[128], or, more securely, use anchors or boundaries \\b(?:[128]|18)\\b. Zero matches! In the first case, x* first matches x at position 0 of the string and replaces it with y, and then matches the empty space at position 1 after the x and replaces it as well. In the second case, the regex matches the empty string at position zero and replaces it, moves past the a to position 1, and again matches and replaces the empty string, giving us yay. ","date":"2021-11-13","objectID":"/regex/:12:0","tags":["tools","cheatsheet"],"title":"Regex","uri":"/regex/"},{"categories":null,"content":"The elements of regex style Inspired and heavily based on this fantastic section from the RexEgg page. To write good regex, say shat you mean. Say it clearly. A string goes from \\A to \\Z (in re; to \\z, in regex). Summary mnemonic: Greedy atoms anchor again.: greedy vs lazy, the cost of greedy and workarounds (say what you want, contrast); should parts be made atomic?; should I use anchors or boundaries?; should I use repeating subpattern syntax? To match or to capture? The full match is just another capture, in Python and many other engines referred to as group 0 and, by convention called ‚Äúthe match‚Äù. So there is no difference between the two approaches. Best practice advise: use whatever gets the job done, while aiming to reduce overhead by reducing the number of capturing groups (use non-capturing groups if useful). To split or to match all? They are a different way of looking at the same approach, so use whichever is easier to get the job done. Whenever possible, anchor. It ensures that the engine finds the match in the right place, and often saves unnecessary backtracking. Say what you want and don‚Äôt want, and avoid ‚Äúdot-star soup‚Äù. It saves unnecessary backgtracking and is clearer to read. Create contrast with consequtive tokens that are mutually exclusive (\\D and \\d, or [^a-z] and [a-z]). It can simplify patterns and save unnecessary backtracking. Example: to find strings with exactly three digits that are located at the end, I might start with ^.+\\d{3}$. This doesn‚Äôt work because . also matches digits, so I‚Äôd match abc12345. I could use negative lookbehind like so: ^.+(?\\\u003c!\\d), but this would still match ab3c123. The real solution is to use mutually exclusive tokens to start with: ^\\D+\\d{3}$. Beware of lazyness. Avoid lazy quantifiers in favour and use contrast to say what you want to save unnecessary backtracking. Example: {.*?} matches everything inside curly brackets, but backtracking is costly as they backtrack at every step. {[^}]*} is more direct and faster. Use greediness and laziness deliberately. A greedy quantifier may shoot all the way to the end of the string, a lazy one tuck along backtracking at every step. Either can be useful when employed for a suitable purpose. Use atomic quantifiers. They can save a lot of backtracking. Design to fail: Compose regexes to minimise the number of unnecessary unsuccessful attempts. Example: with GLOBAL and MULTILINE modes on, (?=.*flea).* matches lines that contain ‚Äúflea‚Äù. But for lines that don‚Äôt contain flea, it unnecessarily tries the lookahead at every single character. Anchoring the lookahead at the start of the line, ^(?=.*flea).*, remedies that by only looking ahead from the first position of each line. Trust the dot-star to get you to the end of the line. It allows you to simplify patterns. Example: in a string such as @abc @bcd you want to match the last token if and only if the string contains more than one token. @[a-z]+$ won‚Äôt do because it also matches the last token if there is only one. @[a-z].*\\K@[a-z]+ does the trick, as the dot-start will shoot all the way to the end and then backtrack as as needed to match the rest of the pattern. To validate n conditions and capture strings that meet them, use n-1 lookaheads. ","date":"2021-11-13","objectID":"/regex/:13:0","tags":["tools","cheatsheet"],"title":"Regex","uri":"/regex/"},{"categories":null,"content":"Miscellaneous From tokens of the form abc_12, return only the digits. Solution: a simple way is to use the keep out token, which discards anything matched before it: \\w+_\\K\\d+. Check whether string length is a multiple of 2, then a multiple of n. Solution: ^(?:..)+$ checks for multiples of two, ^(?:.{n})+$ for multiples of n. ","date":"2021-11-13","objectID":"/regex/:14:0","tags":["tools","cheatsheet"],"title":"Regex","uri":"/regex/"},{"categories":null,"content":"Frequently used patterns Work in progress: Match all strings inside quotation marks in the below block. These are ‚Äòstring one‚Äô and ‚Äòstring two‚Äô and ‚Äôthat‚Äôs string three‚Äô. Thus far (with global flag/findall()): ‚Äò.*‚Äô Doesn‚Äôt work because * is greedy and matches ‚Äòstring one‚Äô and ‚Äô and similar match on second line. ‚Äò.*?‚Äô Doesn‚Äôt match string two since . doesn‚Äôt match line-breaks and get‚Äôs mixed up on second line. [^‚Äô\\v]+ Behaves as the above [^‚Äô]+ Now matches second string but including the linebreak, which we don‚Äôt want as part of the match, and still gets tripped up by that‚Äôs. Wanted: match string two but then exclude \\v from match, ignore ‚Äô that are part of a word. ","date":"2021-11-13","objectID":"/regex/:15:0","tags":["tools","cheatsheet"],"title":"Regex","uri":"/regex/"},{"categories":null,"content":"Resources Regular expressions cookbook RexEgg, awesome online regex resource Regular-Expressions.info, another excellent online resource Regular Expressions 101, very good regex tester ","date":"2021-11-13","objectID":"/regex/:16:0","tags":["tools","cheatsheet"],"title":"Regex","uri":"/regex/"},{"categories":null,"content":"Queues Using built-in list Implement a single-ended queue with basic operations enqueue(), dequeue(), and is_empty(), as well as optional argument maxlen. class Queue: \"\"\" Dequeue takes O(n) time since all elements need to shift. \"\"\" def __init__(self, maxlen=None): self.items = [] self.maxlen = maxlen def enqueue(self, item): if maxlen and len(self.items) == maxlen: self.items = self.items[1:] self.items.append(item) def dequeue(self): return self.items.pop(0) def is_empty(self): return len(self.items) == 0 Using standard library implementation Import the standard library queue module. What does its name stand for? # I tend to forget whether it is deque or heapq that is part of the # collections module. Use 'deck from collections' as an mnemonic. # Deque stands for 'double-ended-queue'. from collections import deque Instantiate the que with a max length of 3. q = deque(maxlen=3) Add 1, and then, in one go, 2, 3, and 4, and, finally, the list [5, 6, 7] to the queue. q.append(1) q.extend([2, 3, 4]) q.append([5, 6, 7]) Dequeue the first item. What will it be? q.popleft() 3 ","date":"2021-11-12","objectID":"/advanced-data-structures/:1:0","tags":["python, cs"],"title":"Advanced data structures","uri":"/advanced-data-structures/"},{"categories":null,"content":"Stacks Using a list stack = [1, 2, 3, 4, 5] stack.pop() stack [1, 2, 3, 4] Using deque from collections import deque stack = deque([1, 2, 3, 4]) reversed_stack = [] while stack: reversed_stack.append(stack.pop()) reversed_stack [4, 3, 2, 1] stack.append(1) stack.append([9, 4, 2]) print(stack.popleft(), stack.pop()) 1 [9, 4, 2] ","date":"2021-11-12","objectID":"/advanced-data-structures/:2:0","tags":["python, cs"],"title":"Advanced data structures","uri":"/advanced-data-structures/"},{"categories":null,"content":"Sources Fluent Python Python Cookbook Learning Python The Hitchhiker‚Äôs Guide to Python Effective Python ","date":"2021-11-12","objectID":"/advanced-data-structures/:3:0","tags":["python, cs"],"title":"Advanced data structures","uri":"/advanced-data-structures/"},{"categories":null,"content":"todo: Integrate notes from codebase.py ","date":"2021-11-12","objectID":"/basic-data-structures/:0:0","tags":["python, cs"],"title":"Basic data structures","uri":"/basic-data-structures/"},{"categories":null,"content":"Tuple A tuple is a finite, ordered, immutable sequence of elements. type((1)) # =\u003e int type(()) # =\u003e tuple type((1,)) # =\u003e tuple ","date":"2021-11-12","objectID":"/basic-data-structures/:1:0","tags":["python, cs"],"title":"Basic data structures","uri":"/basic-data-structures/"},{"categories":null,"content":"Strings Strings are a special type of array ‚Äì one composed of characters. \"abcd\".strip(\"cd\") 'ab' \"Hello world\".find(\"wo\") 6 ","date":"2021-11-12","objectID":"/basic-data-structures/:2:0","tags":["python, cs"],"title":"Basic data structures","uri":"/basic-data-structures/"},{"categories":null,"content":"Unicode conversion Convert character string into unicode code point ord(\"1\"), ord(\"2\"), ord(\"A\"), ord(\"B\") (49, 50, 65, 66) Convert unicode code point to character chr(49), chr(50), chr(65), chr(66) ('1', '2', 'A', 'B') Trick to convert integer to string representation chr(ord(\"0\") + 2) '2' ","date":"2021-11-12","objectID":"/basic-data-structures/:2:1","tags":["python, cs"],"title":"Basic data structures","uri":"/basic-data-structures/"},{"categories":null,"content":"Sequences ","date":"2021-11-12","objectID":"/basic-data-structures/:3:0","tags":["python, cs"],"title":"Basic data structures","uri":"/basic-data-structures/"},{"categories":null,"content":"Useful stuff Ignore minus sign in string number (using fact that False is 0 and True is 1. a, b = \"-123\", \"123\" a[a[0] == \"-\" :], b[b[0] == \"-\" :] ('123', '123') ","date":"2021-11-12","objectID":"/basic-data-structures/:3:1","tags":["python, cs"],"title":"Basic data structures","uri":"/basic-data-structures/"},{"categories":null,"content":"Slice from (and/or) to particular characters a = \"abc[def]\" a[a.find(\"[\") :] '[def]' this works because a.find(\"[\") 3 ","date":"2021-11-12","objectID":"/basic-data-structures/:3:2","tags":["python, cs"],"title":"Basic data structures","uri":"/basic-data-structures/"},{"categories":null,"content":"Sets # A set is an unordered, finite collection of distinct, hashable elements. a = set(\"hello\") b = set(\"world\") Lookup Lookup has worst case time complexity O(n) and avearge time complexity O(1). Why? \"e\" in a True Difference Basic set difference a-b has time complexity O(len(a)) (for every element in a move to new set, if not in b) and space complexity O(len(a) + len(b)), since a new set is being created. a - b {'e', 'h'} A variant is difference update, which has time complexity O(len(b)) (for every element in b remove from a if it exists) and space complexity O(1), as we don‚Äôt create a new set but update set a in place. a.difference_update(b) a {'e', 'h'} Basic operations print(a) # unique elements in a print(\"k\" in a) # membership testing a.add(\"z\") a.remove(\"l\") # remove element from set, KeyError if not a member a.discard(\"m\") # remove element from set, do nothing if not a member print(a) print({\"e\", \"h\"} \u003c a) # {e, h} is a strict subset of a print(a \u0026 b) # intersection print(a | b) # union print(a - b) # difference print(a ^ b) # symmetric difference ","date":"2021-11-12","objectID":"/basic-data-structures/:4:0","tags":["python, cs"],"title":"Basic data structures","uri":"/basic-data-structures/"},{"categories":null,"content":"Named tuples Basic use from collections import namedtuple # create Stock class Stock = namedtuple(\"Stock\", [\"name\", \"shares\", \"price\", \"date\", \"time\"]) # instantiate class s = Stock(\"aapl\", \"100\", \"55\", None, None) # attribute access s.name 'aapl' Replace values s._replace(shares=200) Stock(name='aapl', shares=200, price='55', date=None, time=None) Use replace to populate named tuples with optional or missing fields stock_prototype = Stock(\"\", 0, 0.0, None, None) def dict_to_stock(s): return stock_prototype._replace(**s) portfolio = [ {\"name\": \"IBM\", \"shares\": 100, \"price\": 91.1}, {\"name\": \"AAPL\", \"shares\": 50, \"price\": 543.22}, {\"name\": \"FB\", \"shares\": 200, \"price\": 21.09}, ] stocks = [dict_to_stock(s) for s in portfolio] stocks [Stock(name='IBM', shares=100, price=91.1, date=None, time=None), Stock(name='AAPL', shares=50, price=543.22, date=None, time=None), Stock(name='FB', shares=200, price=21.09, date=None, time=None)] ","date":"2021-11-12","objectID":"/basic-data-structures/:5:0","tags":["python, cs"],"title":"Basic data structures","uri":"/basic-data-structures/"},{"categories":null,"content":"Sources Python time complexities ","date":"2021-11-12","objectID":"/basic-data-structures/:6:0","tags":["python, cs"],"title":"Basic data structures","uri":"/basic-data-structures/"},{"categories":null,"content":"Basics Values and order: All values of a categorical valiable are either in categories or are np.nan. Order is defined by the order of categories, not the lexical order of the values. Memory structure: Internally, the data structure consists of a categories array and an integer arrays of codes, which point to the values in the categories array. The memory usage of a categorical variable is proportional to the number of categories plus the length of the data, while that for an object dtype is a constant times the length of the data. As the number of categories approaches the length of the data, memory usage approaches that of object type. Use cases: To save memory (if number of categories is small relative to the number of rows) If logical order differs from lexical order (e.g.¬†‚Äòsmall‚Äô, ‚Äòmedium‚Äô, ‚Äôlarge‚Äô) To signal to libraries that column should be treated as a category (e.g.¬†for plotting) ","date":"2021-11-11","objectID":"/pandas-categories/:1:0","tags":["python"],"title":"Pandas categories","uri":"/pandas-categories/"},{"categories":null,"content":"General best practices Operating on categories: Operate on category values directly rather than column elements (e.g.¬†to rename categories use df.catvar.cat.rename_rategories(*args, **kwargs)). If there is no cat method available, consider operating on categories directly with df.catvar.cat.categories. Merging: Pandas treats categorical variables with different categories as different data types Category merge keys will only be categories in the merged dataframe if they are of the same data types (i.e.¬†have the same categories), otherwise they will be converted back to objects Grouping: By default, we group on all categories, not just those present in the data. More often than not, you‚Äôll want to use df.groupby(catvar, observed=True) to only use categories observed in the data. ","date":"2021-11-11","objectID":"/pandas-categories/:2:0","tags":["python"],"title":"Pandas categories","uri":"/pandas-categories/"},{"categories":null,"content":"Operations I frequently use import numpy as np import pandas as pd import seaborn as sns df = sns.load_dataset(\"taxis\") df[\"pickup\"] = pd.to_datetime(df.pickup) df[\"dropoff\"] = pd.to_datetime(df.dropoff) df.head(2) pickup dropoff passengers distance fare tip tolls total color payment pickup_zone dropoff_zone pickup_borough dropoff_borough 0 2019-03-23 20:21:09 2019-03-23 20:27:24 1 1.60 7.0 2.15 0.0 12.95 yellow credit card Lenox Hill West UN/Turtle Bay South Manhattan Manhattan 1 2019-03-04 16:11:55 2019-03-04 16:19:00 1 0.79 5.0 0.00 0.0 9.30 yellow cash Upper West Side South Upper West Side South Manhattan Manhattan ","date":"2021-11-11","objectID":"/pandas-categories/:3:0","tags":["python"],"title":"Pandas categories","uri":"/pandas-categories/"},{"categories":null,"content":"Convert all string variables to categories str_cols = df.select_dtypes(\"object\") df[str_cols.columns] = str_cols.astype(\"category\") ","date":"2021-11-11","objectID":"/pandas-categories/:3:1","tags":["python"],"title":"Pandas categories","uri":"/pandas-categories/"},{"categories":null,"content":"Convert labels of all categorical variables to lowercase cat_cols = df.select_dtypes(\"category\") df[cat_cols.columns] = cat_cols.apply(lambda col: col.cat.rename_categories(str.lower)) ","date":"2021-11-11","objectID":"/pandas-categories/:3:2","tags":["python"],"title":"Pandas categories","uri":"/pandas-categories/"},{"categories":null,"content":"String and datetime accessors When using the str and dt accessors on a variable of type category, pandas applies the operation on the categories rather than the entire array (which is nice) and then creates and returns a new string or date array (which is often not helpful for me). df.payment.str.upper().head(3) 0 CREDIT CARD 1 CASH 2 CREDIT CARD Name: payment, dtype: object For operations that cat provides methods for (e.g.¬†renaming as used above), the solution is to use those methods. For others (e.g.¬†regex searches) the solution is to operate on the categories directly myself. ","date":"2021-11-11","objectID":"/pandas-categories/:3:3","tags":["python"],"title":"Pandas categories","uri":"/pandas-categories/"},{"categories":null,"content":"Object creation Convert sex and class to the same categorical type, with categories being the union of all unique values of both columns. cols = [\"sex\", \"who\"] unique_values = np.unique(titanic[cols].to_numpy().ravel()) categories = pd.CategoricalDtype(categories=unique_values) titanic[cols] = titanic[cols].astype(categories) print(titanic.sex.cat.categories) print(titanic.who.cat.categories) Index(['child', 'female', 'male', 'man', 'woman'], dtype='object') Index(['child', 'female', 'male', 'man', 'woman'], dtype='object') # restore sex and who to object types titanic[cols] = titanic[cols].astype(\"object\") ","date":"2021-11-11","objectID":"/pandas-categories/:4:0","tags":["python"],"title":"Pandas categories","uri":"/pandas-categories/"},{"categories":null,"content":"Custom order df = pd.DataFrame({\"quality\": [\"good\", \"excellent\", \"very good\"]}) df.sort_values(\"quality\") quality 1 excellent 0 good 2 very good ordered_quality = pd.CategoricalDtype([\"good\", \"very good\", \"excellent\"], ordered=True) df.quality = df.quality.astype(ordered_quality) df.sort_values(\"quality\") quality 0 good 2 very good 1 excellent ","date":"2021-11-11","objectID":"/pandas-categories/:5:0","tags":["python"],"title":"Pandas categories","uri":"/pandas-categories/"},{"categories":null,"content":"Unique values Series.unique returns values in order of appearance, and only returns values that are present in the data. dfs = df.head(5) assert not len(dfs.pickup_zone.unique()) == len(dfs.pickup_zone.cat.categories) ","date":"2021-11-11","objectID":"/pandas-categories/:6:0","tags":["python"],"title":"Pandas categories","uri":"/pandas-categories/"},{"categories":null,"content":"References Docs Useful Medium article ","date":"2021-11-11","objectID":"/pandas-categories/:7:0","tags":["python"],"title":"Pandas categories","uri":"/pandas-categories/"},{"categories":null,"content":" import matplotlib.pyplot as plt import numpy as np import pandas as pd import seaborn as sns sns.set_style('whitegrid') df = sns.load_dataset('diamonds') df.head(2) carat cut color clarity depth table price x y z 0 0.23 Ideal E SI2 61.5 55.0 326 3.95 3.98 2.43 1 0.21 Premium E SI1 59.8 61.0 326 3.89 3.84 2.31 df.clarity.value_counts() SI1 13065 VS2 12258 SI2 9194 VS1 8171 VVS2 5066 VVS1 3655 IF 1790 I1 741 Name: clarity, dtype: int64 ","date":"2021-10-15","objectID":"/data-visualisation-notes/:0:0","tags":["dataviz"],"title":"Data visualisation notes","uri":"/data-visualisation-notes/"},{"categories":null,"content":"Use countplot() (not a histogram!) for categorical frequencies To visualise the frequency of a categorical variable, I‚Äôd often reflexively plot a histogram, even though I kind of knew that that‚Äôs not what it‚Äôs for (it‚Äôs for plotting the distribution of continuous variables). For categorical frequencies, it‚Äôs much better to use countplot(), one several plot types seaborn provides for categorical variables. sns.countplot(x='clarity', data=df, color='tab:blue'); ","date":"2021-10-15","objectID":"/data-visualisation-notes/:1:0","tags":["dataviz"],"title":"Data visualisation notes","uri":"/data-visualisation-notes/"},{"categories":null,"content":"Letter-value plots fp = f'~/tmp/entropy_X77.parquet' df = pd.read_parquet(fp) def make_data(df): return (df.loc[df.account_type.ne('other')] .set_index('date') .groupby(['account_type', 'account_id'], observed=True) .resample('M').id.count() .rename('num_txns') .reset_index()) data = make_data(df) data.head(2) account_type account_id date num_txns 0 current 15837 2014-07-31 5 1 current 15837 2014-08-31 27 I want to compare the distributions of monthly transactions per account types. An obvious choice is a boxplot. sns.boxplot(data=data, y='account_type', x='num_txns'); The plot shows quartiles, whiskers indicating variability outside the quartiles, and outliers for each distribution. By default, Seaborn defines as outliers all observations that are outside of 1.5 x IQR on either side of the box. The width (height, really) of the boxes is arbitrary (in particular, they are not related to sample size, as is the case in some variations). The plot has obvious limitations, though. For small datasets (n \u003c 200) estimates of tail behaviour is unreliable and boxplots provide appropriately vague information beyond the quartiles. Also, for small datasets, the number of outliers is usually small and tractable. For larger datasets (10,000 - 100,000), boxplots have two shortcomings: 1) we could and would like to provide more information about tail behaviour, and 2) they classify lots of points as outliers (see above), many of which are extreme but not unexpected (i.e.¬†they are extreme values drawn from the same distribution as the rest of the data, not genuine outliers drawn from a different distribution). This is where letter-value plots (or boxenplots, as Seaborn calls them), are useful. fix, ax = plt.subplots(figsize=(10, 5)) sns.boxenplot(data=data, y='account_type', x='num_txns') ax.set_xticks(range(0, 350, 25)); Letter-value plots address the two shortcomints of boxplots for large datasets: they provide more information about the distribution in the tails, and classify fewer points as outliers. Interpreting letter-value plots: Each box on either side captures half the remaining data on that side. For instance, we can see that the median number of monthly transactions for current accounts is about 48. The right half of the innermost box then tells us that half of the remaining observations to the right of the median, i.e.¬†25% of all data, have fewer than 80 transactions. This is equivalent to saying that three quarters of all month-account observations have fewer than 80 transactions, and corresponds to the box shown in the boxplot above. The interpretation of the next box is the same: it tells us that half of all the remaining data past the third quartile (12.5% of all the data) have fewer than about 105 transactions, and similarly for all further boxes. Boxes with identical heights correspond the the same level. The width of each box indicates how spread out the data are in that region of the distribution (e.g.¬†the large width of the right-most boxes shows that in the extremes, the data is very spread out). ","date":"2021-10-15","objectID":"/data-visualisation-notes/:2:0","tags":["dataviz"],"title":"Data visualisation notes","uri":"/data-visualisation-notes/"},{"categories":null,"content":"Sources: Letter-value plots paper Boxplots on Wikipedia Dennis Meisner on Medium ","date":"2021-10-15","objectID":"/data-visualisation-notes/:2:1","tags":["dataviz"],"title":"Data visualisation notes","uri":"/data-visualisation-notes/"},{"categories":null,"content":"Data model - everything is an object # Everything is an object isinstance(4, object) # =\u003e True isinstance([1, 2], object) # =\u003e True # Objects have identity id(4) # =\u003e 4501558496 (e.g.) # Objects have type type(4) # =\u003e int isinstance(type(4), object) # =\u003e Types are also objects # Objects have value (41).__sizeof__() # =\u003e 28 (bytes) # Variables are references to objects x = 4 y = x x == y # =\u003e True (both refer to object 4) # Duck typing (if it walks, swims, and quacks like a duck...) def compute(a, b, c): return (a + b) * c compute(1, 2, 3) compute([1], [2, 3], 4) compute(\"lo\", \"la\", 3) ","date":"2021-10-10","objectID":"/python-fundamentals/:1:0","tags":["python"],"title":"Python fundamentals","uri":"/python-fundamentals/"},{"categories":null,"content":"Lexical analysis What is a Python program read by? A parser, which reads a sequence of tokens produced by a lexical analyser. What is a legixal analyser? A program that performs lexical analysis. What is lexical analysis, and what‚Äôs the etymology of the term? Lexical analysis, also called lexing or tokenizing, is the process of converting a sequence of characters into a sequence of tokens (src). The root of the term is the Greek lexis, meaning word. What is a token? A string with an identified meaning, structured as a name-value tuple. Common token names (aking to parts of speech in linguistics) are identifier, keyword, separator, operator, literal, comment. For the expression a = b + 2, lexical analysis would yield the sequence of tokens [(identifier, a), (operator, =), (identifier, b), (operator, +), (literal, 2)] (src). What‚Äôs the difference between lexical and syntactic definitions? The former operates on the individual characters of the input source (during tokenizing), the latter on the stream of tokens created by lexial analysis (during parsing). By default, Python reads text as Unicode code points. Hence, encoding declarations are only needed if some other encoding is required. References Python Language Reference ","date":"2021-10-10","objectID":"/python-fundamentals/:2:0","tags":["python"],"title":"Python fundamentals","uri":"/python-fundamentals/"},{"categories":null,"content":"difflib Docs here import difflib Most simple use case m = difflib.SequenceMatcher(None, 'NEW YORK METS', 'NEW YORK MEATS') m.ratio() 0.9629629629629629 Create helper function so we don‚Äôt need to specify None each time. from functools import partial matcher = partial(difflib.SequenceMatcher, None) matcher('NEW YORK METS', 'NEW YORK MEATS').ratio() 0.9629629629629629 Compare one sequence to multiple other sequences (SequenceMatcher caches second sequence) m = difflib.SequenceMatcher() m.set_seq2('abc') for s in ['abc', 'ab', 'abcd', 'cde', 'def']: m.set_seq1(s) length = len(m.a) + len(m.b) print('{}, {:{}} -\u003e {:.3f}'.format(m.a, m.b, 10-length, m.ratio())) abc, abc -\u003e 1.000 ab, abc -\u003e 0.800 abcd, abc -\u003e 0.857 cde, abc -\u003e 0.333 def, abc -\u003e 0.000 ","date":"2021-09-27","objectID":"/python-fuzzy-matching/:1:0","tags":["python, datascience"],"title":"Fuzzy matching in Python","uri":"/python-fuzzy-matching/"},{"categories":null,"content":"fuzzywuzzy Based on this tutorial. ","date":"2021-09-27","objectID":"/python-fuzzy-matching/:2:0","tags":["python, datascience"],"title":"Fuzzy matching in Python","uri":"/python-fuzzy-matching/"},{"categories":null,"content":"Finding perfect or imperfect substrings One limitation of SequenceMatcher is that two sequences that clearly refer to the same thing might get a lower score than two sequences that refer to something different. print(matcher(\"YANKEES\", \"NEW YORK YANKEES\").ratio()) matcher(\"NEW YORK METS\", \"NEW YORK YANKEES\").ratio() 0.6086956521739131 0.7586206896551724 fuzzywuzzy has a useful function for this based on what they call the ‚Äúbest-partial‚Äù heuristic, which returns the similarity score for the best substring of length min(len(seq1)), len(seq2)). from fuzzywuzzy import fuzz print(fuzz.partial_ratio(\"YANKEES\", \"NEW YORK YANKEES\")) print(fuzz.partial_ratio(\"NEW YORK METS\", \"NEW YORK YANKEES\")) 100 69 For one of my projects, I want to filter out financial transactions for which the description is a perfect or near-perfect substring of another transaction. So this is exactly what I need. a = '' ","date":"2021-09-27","objectID":"/python-fuzzy-matching/:2:1","tags":["python, datascience"],"title":"Fuzzy matching in Python","uri":"/python-fuzzy-matching/"},{"categories":null,"content":"Formatted string literals (f-strings) A basic f-string consists of a combination of literal characters and replacement characters, the latter of which are placed inside braces (full grammar here, useful explanation of how they are parsed here). The general form for the replacement field is \"{\" expression [\"=\"] [\"!\" conversion] [\":\" format_spec] \"}\". name = \"world\" f\"Hello {name}\" 'Hello world' Add = to also print the name of the replacement expression (useful for debugging). name = \"world\" f\"Hello {name=}\" \"Hello name='world'\" Add a ! for conversion: !s calls str(), !r calls repr(), and !a calls ascii(). name = \"world\" f\"Hello {name!r}\" \"Hello 'world'\" Add : to add a format specifier, using the format mini language. import datetime today = datetime.datetime.today() f\"It's {today:%H.%M} on {today:%d %B, %Y.}\" \"It's 06.45 on 01 December, 2021.\" Expressions can be nested (it‚Äôs a contrived example, we could just used %p to get AM and PM) f\"It's {today:%H.%M{'am' if today.hour \u003c 12 else 'pm'}} on {today:%d %B, %Y.}\" \"It's 06.45am on 01 December, 2021.\" value = 5.123 width = 5 precision = 2 f\"{value:{width}.{precision}}\" ' 5.1' Backslashes are not allowed in expressions. If I need backslash escapes, use a variable. newline = ord(\"\\n\") f\"newline: {newline}\" 'newline: 10' ","date":"2021-09-11","objectID":"/python-string-formatting/:1:0","tags":["python"],"title":"Python string formatting","uri":"/python-string-formatting/"},{"categories":null,"content":"Format specifier form The general form of the format specifier is (docs): [[fill]align][sign][#][0][width][grouping_option][.precision][type] where: fill: any character align: \u003c, \u003e, ^ (right, left, centered alignment), = (sign-aware zero padding, see below) sign: + (show positive and negative sign), - (show negative sign only, default), space (show space for positive numbers and minus sign for negative ones) #: user ‚Äúalternate form‚Äù for conversion. Effect depends on type. Prevents removal of trailing zeroes in g and G conversion (see below) grouping_option: ,, _, n (comma, underscore, locale aware thousands separator) 0: Turns on sign-aware zero padding (equivalent to 0 fill character with = alignment, see below) .precision: number of digits after decimal points for f or F formatted floats, before and after decimal point for g or G formatted floats, and number of characters used for non-numeric types. type: see below. ","date":"2021-09-11","objectID":"/python-string-formatting/:2:0","tags":["python"],"title":"Python string formatting","uri":"/python-string-formatting/"},{"categories":null,"content":"Types The type determines how the data should be presented. ","date":"2021-09-11","objectID":"/python-string-formatting/:3:0","tags":["python"],"title":"Python string formatting","uri":"/python-string-formatting/"},{"categories":null,"content":"String types The only available type is s for string format, which is the default and can be omitted. s = \"Hello World.\" f\"{s}\", f\"{s:s}\" ('Hello World.', 'Hello World.') ","date":"2021-09-11","objectID":"/python-string-formatting/:3:1","tags":["python"],"title":"Python string formatting","uri":"/python-string-formatting/"},{"categories":null,"content":"Integer types The default is d for decimal integer, which represents the integer in base 10. This is pretty much all I ever need. But the below also shows examples of how to prepresent an integer in bases two (b for binary), eight (o for octal), and sixteen (x for hexadecimal). There are also a few more options available. n = 10 f\"{n:d}\", f\"{n:b}\", f\"{n:o}\", f\"{n:x}\" ('10', '1010', '12', 'a') ","date":"2021-09-11","objectID":"/python-string-formatting/:3:2","tags":["python"],"title":"Python string formatting","uri":"/python-string-formatting/"},{"categories":null,"content":"Float (and integer) types (Much of this also applies to decimals, which provide a solution to minor rounding issues that happen with floats. But I‚Äôve never needed them and ignore them for now.) e produces scientific notation with one digit before and precision digits after the decimal point for a total of 1 + precision significant digits. precision defaults to 6. E is the same but uses an upper case E as a separator. n = 01234.56789 precision = 0.3 f\"{n:e}\", f\"{n:{precision}e}\", f\"{n:{precision}E}\" ('1.234568e+03', '1.235e+03', '1.235E+03') f produces fixed-point notation with precision number of digits after the decimal point. precision defaults to 6. F is the same but converts nan to NAN and inf to INF. f\"{n:f}\", f\"{n:{precision}f}\" ('1234.567890', '1234.568') g produces general format with precision number of significant digits and the number formatted either using fixed-point or scientific notation depending on its magnitude. precision defaults to 6. G has the same behaviour as F and converts large numbers to scientific notation. (In the last example, remember that the high precision doesn‚Äôt add zero padding because it determines the number of significant digits.) f\"{n:g}\", f\"{n:.1g}\", f\"{n:.3g}\", f\"{n:.12g}\" ('1234.57', '1e+03', '1.23e+03', '1234.56789') The below might be unexpected (it was for me, anyways). It is a result of the fact that decimals can‚Äôt be represented exactly in binary floating point. The right-hand side expression represents another example of the same limitation. If such high precision is needed, the decimal module should help. f\"{n:.25g}\", 1.1 + 2.2 ('1234.567890000000033978722', 3.3000000000000003) % produces a percentage by multiplying the number by 100, adding a percent sign, and formatting the number using fixed-point format (e.g.¬†the default is 6 ‚Äúprecision digits‚Äù after the decimal point). f\"{n:%}\", f\"{n:.2%}\" ('123456.789000%', '123456.79%') ","date":"2021-09-11","objectID":"/python-string-formatting/:3:3","tags":["python"],"title":"Python string formatting","uri":"/python-string-formatting/"},{"categories":null,"content":"Strings .precision determines the number of characters used. text = \"Hello World!\" f\"{text:.^30.5}\" '............Hello.............' ","date":"2021-09-11","objectID":"/python-string-formatting/:3:4","tags":["python"],"title":"Python string formatting","uri":"/python-string-formatting/"},{"categories":null,"content":"Digits By convention, an empty format field produces the same result as calling str() on the value. n = 01234.56789 f\"{n:}\", str(n) ('1234.56789', '1234.56789') By default, the width equals the length of the data, so fill and align have no effect. f\"{n:@\u003c}\" '1234.56789' We can use = alignment to add padding between the sign and the digit, and use a 0 before width as a shortcut to get sign-aware zero padding (i.e.¬†the equivalent of a 0 fill with = alignment). f\"{n:=+15}\", f\"{n:0=+15}\", f\"{n:+015}\" ('+ 1234.56789', '+00001234.56789', '+00001234.56789') Use # to keep trailing zeroes in g and G conversions. f\"{123.400:g}\", f\"{123.400:#g}\" ('123.4', '123.400') Thousands separators. n = 1_000_000 f\"{n:,}\", f\"{n:_}\", f\"{n:n}\" ('1,000,000', '1_000_000', '1000000') ","date":"2021-09-11","objectID":"/python-string-formatting/:3:5","tags":["python"],"title":"Python string formatting","uri":"/python-string-formatting/"},{"categories":null,"content":"Character sets The string module (docs here) provides a set of useful character sets as module constants. import string string.ascii_lowercase 'abcdefghijklmnopqrstuvwxyz' string.ascii_letters 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ' string.digits '0123456789' string.punctuation '!\"#$%\u0026\\'()*+,-./:;\u003c=\u003e?@[\\\\]^_`{|}~' ","date":"2021-09-11","objectID":"/python-string-formatting/:4:0","tags":["python"],"title":"Python string formatting","uri":"/python-string-formatting/"},{"categories":null,"content":"Dates Quick reference for strftime() and strptime() codes I use often and keep forgetting. Full list here. (As a reminder: strftime() is an instance method that converts datetime objects to a string in a given format, while strptime() is a class method that parses a string and converts it to datetime.) import datetime today = datetime.datetime.today() print(today) 2021-12-01 06:36:20.652838 fmt = \"%d %b %Y\" today.strftime(fmt), datetime.datetime.strptime(\"1 Dec 2021\", fmt) ('01 Dec 2021', datetime.datetime(2021, 12, 1, 0, 0)) today.strftime(\"%y %Y\") '21 2021' today.strftime(\"%a %A\") 'Wed Wednesday' today.strftime(\"%b %B\") 'Dec December' print(today.strftime(\"%H:%M:%S\")) # 24-hour clock print(today.strftime(\"%I:%M%p\")) # 12-hour clock 06:36:20 06:36AM today.strftime(\"%c || %x || %X\") # Locale's appropriate formatting and literals 'Wed Dec 1 06:36:20 2021 || 12/01/21 || 06:36:20' ","date":"2021-09-11","objectID":"/python-string-formatting/:5:0","tags":["python"],"title":"Python string formatting","uri":"/python-string-formatting/"},{"categories":null,"content":"Applications ","date":"2021-09-11","objectID":"/python-string-formatting/:6:0","tags":["python"],"title":"Python string formatting","uri":"/python-string-formatting/"},{"categories":null,"content":"string doc examples Accessing argument‚Äôs items point = (2, 5) \"x = {0[0]}, y = {0[1]}\".format(point) 'x = 2, y = 5' Using format mini-language n = 10000 \"{:.\u003e20,.2f}\".format(n) '...........10,000.00' Formatting dates import datetime today = datetime.datetime.today() print(\"Day: {date:%d}\\nMonth: {date:%b}\\nYear: {date:%Y}\".format(date=today)) Day: 07 Month: Dec Year: 2021 ","date":"2021-09-11","objectID":"/python-string-formatting/:6:1","tags":["python"],"title":"Python string formatting","uri":"/python-string-formatting/"},{"categories":null,"content":"Create table from list of tuples Based on example from page 29 in Fluent Python. Code is available here. metro_areas = [ (\"Tokyo\", \"JP\", 36.933, (35.689722, 139.691667)), (\"Delhi NCR\", \"IN\", 21.935, (28.613889, 77.208889)), (\"Mexico City\", \"MX\", 20.142, (19.433333, -99.133333)), (\"New York-Newark\", \"US\", 20.104, (40.808611, -74.020386)), (\"Sao Paulo\", \"BR\", 19.649, (-23.547778, -46.635833)), ] hline, hhline = \"-\" * 39, \"=\" * 39 print(hhline) print(\"{:15} | {:^9} | {:^9}\".format(\" \", \"lat.\", \"long.\")) print(hline) fmt = \"{:15} | {:\u003e9.4f} | {:\u003e9.4f}\" for name, cc, pop, (lat, long) in metro_areas: if long \u003c= 0: print(fmt.format(name, lat, long)) print(hhline) ======================================= | lat. | long. --------------------------------------- Mexico City | 19.4333 | -99.1333 New York-Newark | 40.8086 | -74.0204 Sao Paulo | -23.5478 | -46.6358 ======================================= ","date":"2021-09-11","objectID":"/python-string-formatting/:6:2","tags":["python"],"title":"Python string formatting","uri":"/python-string-formatting/"},{"categories":null,"content":"Main sources string docs Formatted string literals docs Fluent Python Python Cookbook ","date":"2021-09-11","objectID":"/python-string-formatting/:7:0","tags":["python"],"title":"Python string formatting","uri":"/python-string-formatting/"},{"categories":["craft"],"content":"Preliminaries I use neovim. My configuration is here. There, I map \u003cesc\u003e to jk, a mapping I also use throughout this file. I‚Äôve remaped Caps Look to \u003cctrl\u003e on my mac. ","date":"2021-09-11","objectID":"/vim/:1:0","tags":["tools","cheatsheet"],"title":"Vim cheatsheet","uri":"/vim/"},{"categories":["craft"],"content":"Quarto setup \u003cshift-k\u003e to open help on object under cursor, \u003cshift-kk\u003e to ender help window, q to quit help. Terminal: open new terminal with \u003cleader\u003ec, then activate terminal by going into insert mode and going back to normal mode with usual keybinding. ","date":"2021-09-11","objectID":"/vim/:2:0","tags":["tools","cheatsheet"],"title":"Vim cheatsheet","uri":"/vim/"},{"categories":["craft"],"content":"Reminders git-gutter: ]h and [h to jump to next and previous hunk, \u003cleader\u003ehs and \u003cleader\u003ehu to stage and unstage hunk. Vim takes an additional command on startup (e.g. nvim +CommandT). Use one keystroke to move and one to execute (e.g. the dot-formula). Exit and re-enter insert mode strategically to chunk your undos (all changes in a single insert session count as one change). If you hit cursor keys more than 2 or 3 times, press backspace more than a couple times, perform the same change on several lines, there is a better way. I want to open a file and get an E325: ATTENTION Found a swap file warning. What happened? For me, it‚Äôs most likely that I accidentally closed a terminal window while still editing the file. What to do? First, check that I‚Äôm not already editing the file elsewhere. Second, recover the file, save it under a new name (:w filename2), force quit the session, compare the original and the new file (diff filename filename2), use the file with the content I need and delete the other one and the swap file. (Based on this great SE answer.) Don‚Äôt solve a problem unless you come across it frequently (and if you do, check whether one of Tim Pope‚Äôs plugins solves it). Useful stuff I tend to forget: Command Effect \u003cC-f\u003e/\u003cC-b\u003e Scroll down/up screen-wise (‚Äúforwards-backwards‚Äù) c-x c-e In command line: edit current line in vim, run after quit :x Like :wq but only write if file was changed set: {option}? Show present setting for {option} set: {option}\u0026 Set option back to default value | Command separator (equivalent to ; in shell) \u003cc-k\u003e-N Enter en dash in insert mode using digraphs \u003cc-o-o\u003e After opening vim, opens last file with cursor at last edit :scriptnames List of scripts loaded on startup :map, :nmap, ‚Ä¶ Show existing mappings \u003cc-o\u003e / \u003cc-i\u003e To move back and forth in the jumplist ","date":"2021-09-11","objectID":"/vim/:3:0","tags":["tools","cheatsheet"],"title":"Vim cheatsheet","uri":"/vim/"},{"categories":["craft"],"content":"Help Command Effect gO Show table of contents for current help file :helpc[lose] Close help windows if any are open :vert h {keyword} Open help in a vertical split ","date":"2021-09-11","objectID":"/vim/:4:0","tags":["tools","cheatsheet"],"title":"Vim cheatsheet","uri":"/vim/"},{"categories":["craft"],"content":"Modes ","date":"2021-09-11","objectID":"/vim/:5:0","tags":["tools","cheatsheet"],"title":"Vim cheatsheet","uri":"/vim/"},{"categories":["craft"],"content":"Normal mode Operators work as follows: operator + [count] + motion = action. E.g. d2l deletes two character to the right, diw the word under the cursor (without the surrounding whitespace), dap the current paragraph (including the surrounding whitespace). Similarly, gUap converts the current paragraph to uppercase. Common operators: Trigger Effect c Change d Delete into register d Delete into register y Yank into register p Paste after cursor P Paste before cursor ~ Swap case of character under cursor and move right gu Make lowercase gU Make uppercase g~ Swap case \u003e Shift right \u003c Shift left = Autoindent ! Filter {motion} lines through an external program Moving back and forth: Forwards Backwards Effect / ? Seach for pattern * # Search for word under cursor n N Jump to next search match $ ^ Jump to end of line f{char} F{char} Position cursor on character t{char} T{char} Position cursor before character ; , Repeat the last r, F, t, or T w b Move to the start of the next word W B Move to the start of the next WORD } { Move down one (blank-line-separated) paragraph gg Jump to the first line of the document G Jump to the last line of the document Act, repeat, reverse: Intent Act Repeat Reverse Make a change {edit} . u Scan line for next character f{char}/t{char} ; , Scan line for previous character F{char}/T{char} ; , Scan document for next match /pattern\u003cCR\u003e n N Scan document for previous match ?pattern\u003cCR\u003e n N Perform substitution :s/old/new \u0026 u Execute a sequence of changes qx{change}q @x u Compound commands: Compound command Equivalent in longhand C c$ (delete from cursor until end of line and start insert) D d$ (delete from cursor until end of line) Y y$ (similar to above, but has to be mapped, see h: Y) s cl (delete single character and start insert) S ^c (delete entire line and start inster, synonym for cc) x dl (delete one character to the right) X dh (delete one character to the left) I ^i (jump to beginning of line and start insert) A $a (jumpt to end of line and start insert) o A\u003ccr\u003e O ko Miscellaneous: Command Effect \u003cC-a\u003e/ \u003cC-x\u003e Add / subtract from the next number \u003cC-o\u003e/ \u003cC-i\u003e Move backwards to last / forward to previous location u/\u003cC-r\u003e Undo / redo change ga Reveal numeric representation of character under cursor gx Open url under cursor \u003cC-z\u003e/fg Put vim in background / return to vim ","date":"2021-09-11","objectID":"/vim/:5:1","tags":["tools","cheatsheet"],"title":"Vim cheatsheet","uri":"/vim/"},{"categories":["craft"],"content":"Insert mode Entering insert mode: Trigger Effect i Insert before cursor a Insert after cursor I Insert at beginning of current line A Insert at end of current line o Insert in a new line below the current one O Insert in a new line above the current one gi Insert at the end of last insert Useful commands: Keystroke action \u003cc-h\u003e delete back one character (backspace) \u003cc-w\u003e delete back one word \u003cc-u\u003e delete back one line \u003cc-o\u003e Enter insert normal mode to execute a single normal cmd \u003cC-r\u003e{register} Paste content from address (use 0 for last yanked text) \u003cC-r\u003e= Perform calculation in place r, R Enter replace mode for single replacement or until exit \u003cC-v\u003e{123} Insert character by decimal code \u003cC-v\u003eu{1234} Insert character by hexadecimal code \u003cC-v\u003e{char1}{char2} Insert character by digraph ","date":"2021-09-11","objectID":"/vim/:5:2","tags":["tools","cheatsheet"],"title":"Vim cheatsheet","uri":"/vim/"},{"categories":["craft"],"content":"Visual mode Once in visual mode, you can use any normal mode movement command to specify the area to be selected. Reminder to myself: use search more often for this. Command Effect v Enter character-wise visual mode / back to normal mode V Enter line-wise visual mode / back to normal mode \u003cC-v\u003e Enter block-wise visual mode / back to normal mode gv Reselect last visual selection o Toggle the free end of a selection Exercises: Turn the list of objects into dictionary elements as shown. foo bar baz buzz d[0] = ‚Äúfoo‚Äù d[1] = ‚Äúbar‚Äù d[2] = ‚Äúbaz‚Äù d[3] = ‚Äúbuzz‚Äù Solutions: yip to select paragraph, :s/\\(.*\\)/d[0] = \"\\1\" to replace each line with dict entry, then move to 0 in last paragraph, then \u003cc-v\u003eg\u003cc-a\u003e to activate block visual mode, and increment the lines. ","date":"2021-09-11","objectID":"/vim/:5:3","tags":["tools","cheatsheet"],"title":"Vim cheatsheet","uri":"/vim/"},{"categories":["craft"],"content":"Command-line mode Ex-commands allow you to make changes (in multiple places) anywhere in the file without moving the cursor ‚Äì ‚Äúthey strike far and wide‚Äù. The general syntax is :[range]{command}, where [range] is either a single address or a range of addresses of the form {start},{stop}. There are three types of addresses: line numbers, visual selections, and patterns. To execute a command on all selected lines, use visual mode to make the selection and press :. This will start the command prompt with '\u003c, '\u003e:, to which you can then add the command. You can also specify offsets. For example, :/\u003ctag\u003e/+1\u003c\\/tag\u003e/-1{cmd} would operate on the lines inside the html tag but not the lines containing the tag marks. Command mode commands: Command Effect :, /, ? Opens command line / search /reverse search mode \u003cC-r\u003e\u003cC-w\u003e Insert word under cursor in command prompt \u003cleft\u003e/\u003cright\u003e Move one character left or right \u003cS-left\u003e Move one word left (similar for right) \u003cC-b\u003e/\u003cC-e\u003e Move to the beginning/end of the command \u003cC-w\u003e Delete last word \u003cC-u\u003e Delete from cursor to beginning of line Types of addresses: Command Effect :4{cmd} execute command on line 4 :4,8{cmd} execute command on lines 4 to 8 (inclusive) :/#/{cmd} execute command on next line with an # :/\u003ctag\u003e/\u003c\\/tag\u003e/{cmd} Execute command on next occurring html tag :'\u003c,'\u003e{cmd} Execute command on selected lines Useful address/range characters: Symobol Address 1 First line of the file $ Last line of the file 0 Virtual line above first line (e.g. to paste to top of file) . Line of cursor 'm Line containing mark m '\u003c Start of visual selection '\u003e End of visual selection % The entire file (short for :1,$) Common Ex-commands: Command Effect p[rint] Print d[elete] Delete j[oin] Join lines s[ubstitute] Substitute (e.g. s/old/new) n[ormal] Execute normal mode command m[ove] Move to {address}, (e.g. :1,5m$ moves lines to end of file) co[py] (or t) Copy to {address}, (e.g. :6t. copies line 6 to current line) Exercises: Wrap all elements in the first column of a table in quotes. Replace the word under the cursor throughout the file. Open help for word under the cursor. Solutions: With cursor on word in first row: :{start},{stop}normal ysaW'. *,cw{change}jk, :%s//\u003cC-r\u003e\u003cC-w\u003e/\u003coptions\u003e. :h \u003cC-r\u003e\u003cC-w\u003e\u003cCR\u003e. ","date":"2021-09-11","objectID":"/vim/:5:4","tags":["tools","cheatsheet"],"title":"Vim cheatsheet","uri":"/vim/"},{"categories":["craft"],"content":"Quickfix List The quickfix list is a special mode to speed up the edit-compile-edit cycle. But it can be used more generally to find a list of positions in files (e.g. list could hold search matches from running :vimgrep). The Location list is a local version of the quickfix list that is bound to the currently active window. There can be as many local lists as there are windows, while there is only a single globally available quickfix list. Command Effect :make [target] Compile target (and jump to first error if there are some) :make! [target] Compile target without jumping to first error :copen Open quickfix window :cclose Close quickfix window ]q / [q Jump to next/previous match (uses vim-unimpaired plugin) ","date":"2021-09-11","objectID":"/vim/:6:0","tags":["tools","cheatsheet"],"title":"Vim cheatsheet","uri":"/vim/"},{"categories":["craft"],"content":"Files Setting the working directory: Command Effect :pwd Show current directory window :cd Set directory for all windows :cd - Revert back to previous directory :lcd Set directory for current window :tcd Set directory for current tab ","date":"2021-09-11","objectID":"/vim/:7:0","tags":["tools","cheatsheet"],"title":"Vim cheatsheet","uri":"/vim/"},{"categories":["craft"],"content":"Buffers A buffer is an in-memory representation of a file. A hidden buffer is one that contains changes you haven‚Äôt written to disk yet but switched away from. For a session with hidden buffers, quitting will raise error messages, and vim will automatically display the first hidden buffer. You now have the following options: :w[rite] to write the buffer‚Äôs content to disk, :e[dit]! to reread the file from disk and thus revert all changes made, :qa[ll]! to discard all changes, and :wa[ll] to write all modified buffers to disk. :bufdo executes an Ex command in all open buffers, :argo in all grouped ones (e.g. :argdo %s/hello/world/g substitutes world for hello in all buffers in :args, :argdo edit! reverts all changes, and :argdo update writes changed buffers to disk. :[range]bd deletes buffers in range, with [range] working as for other Ex-commands (see above). Command Effect :x[it] / exi[t] Like :wq but only write if file was changed :xa Write all changed buffers and exit (like :wqa) vim-eunuch commands: Command Effect Move[!] {file} Like :saveas, but deletes old file Rename[!] {file} Rename current buffer and file Chmod {mode} Change permissions of current file Mkdir {dir} Create dir with mkdir() Mkdir! {dir} Create dir with mkdir() with ‚Äúp‚Äù argument (mkdir -p) Toggle buffer settings from vim-unimpaired: Command Effect yoh Toggle search highlighting yob Toggle light background yoc Toggle cursor line highlighting yon Toggle line numbers yor Toggle relative line numbers yos Toggle the spell checker ","date":"2021-09-11","objectID":"/vim/:7:1","tags":["tools","cheatsheet"],"title":"Vim cheatsheet","uri":"/vim/"},{"categories":["craft"],"content":"Windows A window is a viewport onto a buffer. We can open different windows that all provide a (different) view onto the same buffer, or load multiple buffers into one window. Command Effect \u003cC-w\u003ew Go to next window \u003cC-w\u003es Split window horizontally \u003cC-w\u003ev Split window vertically :sp[lit] {file} Horizontally split window and load {file} into new buffer :vsp[lit] {file} Vertically split window and load {file} into new buffer :new Split horizontally with new file :vne[w] Split vertically with new file on[ly] Close all but current window \u003cC-w\u003e= Equalize width and height of all windows \u003cC-w\u003er Rorate windows \u003cC-w\u003ex Exchange position of current window with its neighbour q[uit] Close current window :sb[uffer] Open buffer number N in horizontal split :vert sb N Open buffer number N in vertical split (\u003cleader\u003evb) ","date":"2021-09-11","objectID":"/vim/:7:2","tags":["tools","cheatsheet"],"title":"Vim cheatsheet","uri":"/vim/"},{"categories":["craft"],"content":"Tabs A tab is a container of windows. Command Effect :tabe[dit]{file} Open new tab with {file} if specified or empty otherwise :[count]tabnew Open a new tab in an empty window. \u003cC-w\u003eT Move current window into new tab :tabc[lose] Close current tab with all its windows :tabo[nly] Close all tabs but the current one {N}gt Go to tab {N} if specified, or to next otherwise gT Go to previous tab Handy [count] options for tabnew: Count Opens new tab ‚Ä¶ [.] ‚Ä¶ after current one - ‚Ä¶ before current one 0 ‚Ä¶ before first one $ ‚Ä¶ after last one ","date":"2021-09-11","objectID":"/vim/:7:3","tags":["tools","cheatsheet"],"title":"Vim cheatsheet","uri":"/vim/"},{"categories":["craft"],"content":"Opening files I use command-t to fuzzy-find files, which is what I use most of the time when working inside a project. To navigate file trees, I use netrw and vinegar. Deleting folders: netrw uses delete() with the d flat to delete directories. As explained in :h delete(), this only removes empty directories. I leave this default for now. To easily open a new file from the same directory as the current buffer in a new window/split/vertical split/tab I use the mappings \u003cleader\u003eew/es/ev/et, following this Vimcast. command-t commands: Command Effect \u003cc-o\u003e Open or close command-t \u003cc-i\u003e Open command-t for open buffers \u003cc-f\u003e Flush path cash and rescan directory \u003cc-v\u003e Open file in vertical split netrw commands: Command Effect e[dit]. Open file explorer for current working directory E[xplore] Open file explorer for the directory of active buffer % Open new file in current directory d Create new directory in current one R Rename file or directory under cursor D Delete file or directory under cursor gh Toggle hiding dot-files :Ve Open explorer in vertical split :Rex Exit/return to explorer \u003cc-l\u003e Refresh listing ","date":"2021-09-11","objectID":"/vim/:7:4","tags":["tools","cheatsheet"],"title":"Vim cheatsheet","uri":"/vim/"},{"categories":["craft"],"content":"Navigation Motions move within a file, jumps between files. Each motion can be prepended by a count (5l moves five characters to the right). ","date":"2021-09-11","objectID":"/vim/:8:0","tags":["tools","cheatsheet"],"title":"Vim cheatsheet","uri":"/vim/"},{"categories":["craft"],"content":"Within files General: Command Effect \u003cC-g\u003e Shows current filepath and line number zz Redraw current line in middle of window zt Redraw current line at top of window Left-right and up-down: You can use search after an operator to great effect. For instance: typing d/to \u003cCR\u003e when the cursor is at the beginning of ‚Äúafter‚Äù in the previous sentence turns it into ‚ÄúYou can use search to greate effect‚Äù. This works because d is an exclusive operator (h: exclusive) and doesn‚Äôt apply the operation on the endpoint of the selection. I use vim-smoothie for smoother screening behaviour of half-screen and screen-wise scrolling. Command Effect h/l Move left/right (use [count] as needed) j/k Down/up one line (think of j as a down arrow, use [count] as needed) gj/gk Down/up by display rather than real lines 0/^/$ To first non-blank/first/last character of line G Goto line [count], default is last line gg Goto line [count], default is first line f{char}/F{char} To next occurrence of {char} to the right/left t{char}/T{char} Till (before) next occurrence of {char} to the right/left H/M/L Jump to the top/middle/bottom of the screen \u003cC-e\u003e/\u003cC-y\u003e Scroll down/up linewise \u003cC-d\u003e/\u003cC-u\u003e Scroll down/up half-screen-wise (‚Äúdown-up‚Äù) \u003cC-f\u003e/\u003cC-b\u003e Scroll down/up screen-wise (‚Äúforwards-backwards‚Äù) Words: Command Effect w/e Forward to start/end of current or next word b/ge Backward to start/end of current or previous word W, E, B Move WORD rather than word wise Text objects: Text objects come in two types: those within a pair of delimiters (e.g. text inside parentheses) and chucks of text (Vim calls them ‚Äúblock‚Äù and ‚Äúnon-block‚Äù objects). They can be moved over or selected. Text object selection start with i (‚Äúinner sentence‚Äù) or a (‚Äúa sentence‚Äù). For example: vi) highlights text inside parentheses but not the parentheses themselves, while va) highlights the parentheses as well. Useful tip: when I‚Äôm not inside a block object, nvim selects applies the command to the next one. Command Effect )/( Move [count] sentences forward/backward }/{ Move [count] paragraphs forward/backward Command Select inside or around‚Ä¶ w/W word/WORD s sentence p paragraph ] a [] block ) or b a () block } or B a {} block \u003c a \u003c\u003e block t a tag block Marks: Command Effect m{a-zA-Z} Set lowercase (local) or uppercase (global) mark `{mark} Jump to mark double-backquote Go to position before last jump `. Go to position of last change % Go to matching bracket ","date":"2021-09-11","objectID":"/vim/:8:1","tags":["tools","cheatsheet"],"title":"Vim cheatsheet","uri":"/vim/"},{"categories":["craft"],"content":"Between files A jump is a long-range motion (which, roughly, means moving faster than WORD-wise). Traversing the jumps and changes lists Command Effect :jumps Show the jump list \u003cC-o\u003e/\u003cC-i\u003e Traverse jump history backwards/forwards :changes Show the change list g;/g, Traverse change list backwards/forwards gf Jump to file under cursor \u003cC-]\u003e Jump to definition of keyword under cursor ","date":"2021-09-11","objectID":"/vim/:8:2","tags":["tools","cheatsheet"],"title":"Vim cheatsheet","uri":"/vim/"},{"categories":["craft"],"content":"Back and forth Vim-unimpaired provides a set of normal mode commands to move between next (]) and previous ([), toggle options, and special pasting. Some commands I use often are listed below. The mnemonic is that ] is next in general and ‚Äúnext line‚Äù here, and lowercase navigates one by one while lowercase jumpts to first or last (e.g. [b moves to previous buffer, [B jumps to first one). Command Effect ]\u003cspace\u003e/[\u003cspace\u003e Add [count] blank lines below/above the cursor ]e/[e Exchanges the current line with the one below/above ","date":"2021-09-11","objectID":"/vim/:8:3","tags":["tools","cheatsheet"],"title":"Vim cheatsheet","uri":"/vim/"},{"categories":["craft"],"content":"Registers ","date":"2021-09-11","objectID":"/vim/:9:0","tags":["tools","cheatsheet"],"title":"Vim cheatsheet","uri":"/vim/"},{"categories":["craft"],"content":"Copy and paste A register is a container that holds text. By default, Vim deletes, yanks and puts to and from the unnamed register \". We can set the register with which a command interacts by prepending the command with \"{register}{cmd} (e.g. to explicitly state that we want to delete the current line to the unnamed register, we‚Äôd use \"\"dd; to put the just copied text, \"\"p. But these are equivalent to dd and p, so we‚Äôd probably not do that.) However, the default register will always contains the content from the last command, even if an additional register was specified. Transposing characters and lines: to correct ‚ÄúThi sis‚Äù, starting from the last letter, use F\u003cspace\u003exp; to swap the current with the subsequent line, use ddp. As an alternative to ddp, which is useful to move lines up and down more flexibly, use ]e from vim-unimpaired (see below). Expression register: when we fetch the content of the expression register, Vim drops into command-line mode with a = prompt. When we enter Vim script and press \u003cCR\u003e, Vim will coerce the result into a string if possible and use it. Command Effect \"{reg}{cmd} Make {cmd} interact with register {reg} \"\" The unnamed register (redundant, as it‚Äôs the default) \"0 The yank register \"_ The black hole register (nothing returns from it) \"{a-z} Named registers (replace with {a-z}, append with {A-Z}) \"+ The system clipboard \"% Name of current file (e.g. \"%p) \"# Name of alternate file \". Last inserted text \": Last Ex command \"/ Last search pattern :reg[ister] [reg] List content of registers reg, all by default \u003cC-r\u003e{reg} Paste content of {reg} in insert mode Useful patterns: Replace firstword with secondword. Solution 1: cursor at beginning of secondword; ye; bb; ve; p. Solution 2: cursor at beginning of secondword; ye; bb; cw; \u003cC-r\u003e0. Has advantage that . now replaces current word with firstword. Swap firstword and secondword. Solution: cursor at beginning of firstword; de; mm; ww; ve; p; `m; P. Explanation: this exploits a quirk in the behaviour or p in visual mode. When we paste in visual mode, we put the content of the default register in place of the highlighted text, and the highlighted text into the default register. Complete the statement 27 * 45 = x. Solution: cursor at x and in insert mode; \u003cC-r\u003e=27*45\u003cCR\u003e. ","date":"2021-09-11","objectID":"/vim/:9:1","tags":["tools","cheatsheet"],"title":"Vim cheatsheet","uri":"/vim/"},{"categories":["craft"],"content":"Macros Macros can be executed in sequence (e.g. 22@a) or in parallel ([range]:normal @a). The former can be useful as it aborts on failure, which could be what we want (e.g. replace all search results and stop once none are left). But if it‚Äôs not, then the latter approach is more useful (e.g. if you want to perform a change on all list items but not on other lines). Command Effect q{a-z} Start recording macro to register q End recording [count]@{a-z} Invoke macro in register [count] times @@ Replay last invoked macro q{A-Z} Append to macro (e.g. if I forgot something) Exercises: In the below block of code, prepend *var * and append ; to each line. foo = 1 bar = 2 baz = 3 Edit macro q by prepending it with a ^ using a) yanking and b) visual editing (based on this useful post). Macro q hits its target with n; invoke it quickly for all 12 search results in the document. In the list below, change . to ) and words to title case. a. ho b. hi c. he Make the reverse changes in the list below a) Ho b) Hi // a comment c) He Turn the below lines into a numbered list. - first - second - third Turn the below list into a numbered list. - This is the first bullet - stretching over multiple lines. Well, actually, it didn't, but now it does. - The second bullet is long, too. Again, it wasn't really, but now it is, so we can actually simulate what would happen. - The third one is short. - The fourth and final one is long. Solutions: With the cursor anywhere on the first line, start recording and perform the change on the first line, then either repeat it (a) sequentially or (b) in parallel. qq, Ivar\u003cesc\u003eA;\u003cesc\u003ej, (a) 2@q, (b) Vj:normal@q. a) Paste the macro content into the buffer and edit it: \"qpI^\u003cesc\u003e, yank it back into the q register: \"qyy, clean macro from the buffer dd. b) Redefine the macro content directly using let command by opening the register :let @q=', pasting the current contents \u003cc-r\u003e\u003cc-r\u003eq, adding ^ at the beginning, and adding ' and press enter to close the quote and finish editing the macro. 22@q. Explanation: Because q uses n to hit its targets, it will automatically abort once there are no more results. We can thus avoid counting matches and simply use a number that‚Äôs comfortably above the number of matches. 22 is convenient because, on my keyboard, it‚Äôs the same key as @. Start with cursor anywhere on first line of list, record macro: qq0f.r)w~jq, replay macro: 22@q. Start with cursor anywhere on first line of list, record macro: qq0f.r)w~q, replay macro: V}:normal @q. Discussion: Executing the macro in series as in the previous exercise would abort at the line of the comment, so we need to execute it in parallel. As a result, there is no need to move to next line after performing the changes. Start anywhere on the first line, then instantiate the counter: :let i=1, record the changes: qq0s\u003cC-r\u003e=i\u003cCR\u003e)\u003cEsc\u003e, advance the counter: let i+=1, stop recording: q and replay: jVj:normal @q. My best solution thus far: We need a few preparation steps before we can execute the macro; first, select and then deselect the area within which you want to replace list item markers (e.g. vap, jk). Second, search for all list item markers inside that area using /\\%V\\_^-, where the \\%V atom restricts the search to the previous selection, the \\_^ atom matches start-of-line, and - matches the hyphens used as list item markers. Finally, initialise the counter using :let i=1. To record the marco, move the curser to before the first hypen in the list, then record the motions to the q register qq, move to the first hypehn and replace it while entering insert mode ncw, replace it with the counter and add a dot and exit insert mode \u003cc-r\u003e=i\u003ccr\u003e.jk, and increment the counter and stop the recording :let i=i+1q. Now, you can simply replay the recording in sequence 22@q and, if needed, reformat the area gvgq. (I use 22 to replay the macro for convenience as on my keyboard, 2 is the same key as the @ symbol, but any number at least as large as the number of","date":"2021-09-11","objectID":"/vim/:9:2","tags":["tools","cheatsheet"],"title":"Vim cheatsheet","uri":"/vim/"},{"categories":["craft"],"content":"Patterns ","date":"2021-09-11","objectID":"/vim/:10:0","tags":["tools","cheatsheet"],"title":"Vim cheatsheet","uri":"/vim/"},{"categories":["craft"],"content":"Matching patterns and literals In my .vimrc, I use set: ignorecase and set: smartcase to set the default search to be case insensitive except when I use an uppercase character in my pattern. You can use pattern swtiches anywhere in the search pattern. Use \\v for regex search, \\V for verbatim search. Command Effect \\c Force case insensitive search \\C Force case sensitive search \\v (very magic) All characters assume special meaning (regex-like search) \\V (very nomagic) No character but ‚Äú\" has special meaning (verbatim search) () Capture matched pattern inside and store in numbered silo % When before (), don‚Äôt capture submatch \u003c/\u003e Word boundaries when used with \\v switch \\zs/\\ze Start and end of match Useful patterns: Find the but not words it is a part of (e.g. these). Solution: /\\v\u003cthe\u003e\u003cCR\u003e. In a CSS file, find all hex colour codes. Solution: /\\v#(\\x{6}|\\x{3}). Explanation: use \\v for regex search and \\x to capture hexadecimal characters (equivalent to [0-9a-fA-F]). Find ‚Äúa.k.a.‚Äù in a file. Solution: /\\Va.k.a.. Explanation: we need \\V or else . matches any character and we‚Äôd also find words like ‚Äúbackwards‚Äù. Check for words that occurr twice in a row. Solution: /\\v\u003c(\\w+)\\_s+\\1\u003e. Explanation: (\\w+) captures any word, \\_s matches a linebreak or a space (see h: /\\_), \\1 is the reference to the previously captured word, and \u003c,\u003e ensure that only two occurrences of the same word get matched and not also patterns like ‚Äún‚Äù in ‚Äúin nord‚Äù. Reverse the order of all occurrences of Fab Gunzinger and Fabian Gunzinger. Solution: /\\v(Fa%(b|bian)) (Gunzinger); :%s//\\2, \\1/g. Explanation: the first bit captures the short and full version of my first name, and my last name, without capturing the b or bian fragments. First and last name can now be references using \\1 and \\2, respectively. The substitution command finds the last search pattern (since we leave pattern blank) and replaces it with my first and last names reversed. Find all occurences of ‚ÄúVim‚Äù that are part of ‚ÄúPractical Vim‚Äù. Solution: /Practical \\zsVim\u003cCR\u003e. Find all quoted text. Solution: /\\v\"\\zs[^\"]+\\ze\". Explanation: \"[^\"]+\" matches quotes followed by one or more occurances of anything but quotes followed by quotes (this is a useful regex idiom). \\zs and \\ze exclude the quotes from the match. Note: this only recognises quoted text on the same line. Note: doesn‚Äôt work over multiple lines. Find http://someurl.com/search?=\\//. Solution: Yank pattern into a register, u for url, say; /\\V\u003cC-r\u003e=escape(@u, getcmdtype().'\\')\u003cCR\u003e. Explanation: see tip 79 in PV. Exercises: In music amuse fuse refuse replace us with az in amuse and fuse using the substitute command (the point is to practice substitution in a limited area within a line). Solutions: Use to the a at the beginning of amuse, then use vee to select the two words needed, jk (my mapping for \u003cesc\u003e) to leave visual mode, and :s/\\%Vus/az/g to make the substitution in the last selected area: the \\%V atom is what restricts the substitution to the last selected area. ","date":"2021-09-11","objectID":"/vim/:10:1","tags":["tools","cheatsheet"],"title":"Vim cheatsheet","uri":"/vim/"},{"categories":["craft"],"content":"Search Within a file Command Effect /\u003cCR\u003e Search previous pattern forward ?\u003cCR\u003e Search previous pattern backwards /\u003cUP\u003e Access search history (similar for backward search) \u003cC-r\u003e\u003cC-w\u003e Autocomplete search field based on preview match /{pattern}/e\u003cCR\u003e Use search offset to jump to end of match (`h: search-offset` for more offset options) gn Operate on a complete search match Exercises: In the paragraph below, replace all occurrences of ‚Äúlang‚Äù or ‚Äúlangs‚Äù with ‚Äúlanguage‚Äù or ‚Äúlanguages‚Äù. learn a lang each year which lang did you learn? which lang will you learn? how many langs do you know? Now repeat the above, but start out by searching without using the search offset to jump to the end of the word and then make use of it midway through my search. Search for the line below each occurrence of ‚Äúlang‚Äù. Replace all occurrences of ‚ÄúPyCode‚Äù and ‚ÄúPythonCode‚Äù with ‚ÄúPYCode‚Äù or ‚ÄúPYTHONCode‚Äù. Solutions: /lang/\u003cCR\u003e; ea; uage; n.. Explanation: the second / denotes teh end of the pattern, so from then on we‚Äôre back in command line mode. Use //e\u003cCR\u003e to repeat the previous search pattern but with search offset used. /lang/+1. Explanation: +# in search offset positions the cursor # lines after the match. /\\vPy(thon)?\\C\u003cCR\u003e; gUgn; .. Explanation: gn applies the pending operator (gU in this case) to the current match or, if the cursor isn‚Äôt on a match, on the next one. After executing gUgn for the first time, the cursor changes the first match and remains there. Once we press ., the word under the cursor no longer is a match, so Vim jumps to the next match and applies the pending gU operator. Drew Neil calls this the ‚ÄúImproved dot-formula‚Äù, since we can use . to achieve n.. Across files There are many options for this. I currently use vim-ripgrep. The basic syntax is :Rg \u003cstring|pattern\u003e, with \u003cstring|pattern\u003e defaulting to the word under the cursor. Vim will ist the results in the quickfix window and jump to the first entry in the window. Exercises: Find all files that contain the line import s3fs in (a) the current directory and (b) in the subdirectory /data. Solutions: (a) :Rg 'import s3fs, (b) :Rg 'import s3fs' data/. ","date":"2021-09-11","objectID":"/vim/:10:2","tags":["tools","cheatsheet"],"title":"Vim cheatsheet","uri":"/vim/"},{"categories":["craft"],"content":"Substitution Full syntax is :[range]s[ubstitute]/{pattern}/{string}/[flags] For substitutions across files I use quickfix-reflector, which allows for editing the quickfix window, and performs the changes made there in all files. Flags: Command Effect g Substitute all matches on line (global) c Confirm substitution n Count number of matches instead of substitution \u0026 Reuse flags from previous substitution Replacement strings: Command Effect \\1 Insert first submatch (similar for {1-9}) \\0/\u0026 Insert entire matched pattern ~ Use string from previous substitution \\={vim scrip} Evaluate vim-script expression Useful commands Command Effect :\u0026 Rerun last substitution (flags aren‚Äôt remembered) :\u0026\u0026 Rerun last substitution and reuse flags :g\u0026 Rerun last search globally Exercise: Replace import helpers.aws as ha with from helpers import aws in all files in the current directory. Decouple pattern matching and substitution (useful for complex patterns that require trial and error). Use last search pattern in substitute command. Substitute the highlighted text fragment. Rerun the last line-wise substitution globally. In a file with columns ‚Äúname‚Äù, ‚Äúage‚Äù, ‚Äúheight‚Äù, change order to ‚Äúheight‚Äù, ‚Äúname‚Äù, ‚Äúage‚Äù. Replace ‚ÄúHello‚Äù in (and only in) ‚ÄúHello World‚Äù with ‚ÄúHi‚Äù in all files in my project. Solutions: Find all files and open the quickfix window :Rg 'import helpers.aws as ha', perform the change in each file inside the quickfix window and save. /{pattern} until you get it right (maybe use q/), then :s//{string}. Explanation: leaving {pattern} blank uses last search pattern. :s/\u003cC-r\u003e//{string} *:s//{string}. Explanation: with vim visual star plugin installed, * searches for pattern highlighted in visual mode. g\u0026. /\\v^([^,]), ([^,]), ([^,])$; :%s//\\3, \\1, \\2. First, test pattern in current buffer: /Hello\\ze World\u003cCR\u003e, then search all files in project and populate quickfix list with files that have a match: :vimgrep // **/*.txt, finally: iterate through the files in the quickfix list to execute the substitute and update commands: :cfdo %s//Hi/gc | update ","date":"2021-09-11","objectID":"/vim/:10:3","tags":["tools","cheatsheet"],"title":"Vim cheatsheet","uri":"/vim/"},{"categories":["craft"],"content":"Global commands` Full syntax: :[range] global[!] /{pattern}/ {cmd}. Range defaults to the entire file, leaving the pattern empty uses last search pattern, and command defaults to print. A generalised version of the command, useful to operate inside text or code blocks, is :g/{start} .,{finish} [cmd], which applies the command to each range of lines that begins with {start} and ends with {finish}. See CSS sorting example below. Command Effect g[lobal] Global command v[global] Invert global g[lobal]! Invert global Exercises: Delete all lines that contain ‚ÄúHi‚Äù. Keep only lines that contain ‚ÄúHi‚Äù. Print all lines that contain ‚ÄúHi‚Äù. Yank all lines that contain ‚ÄúTODO‚Äù into register a. Glance at markdown file structure (create a table of contents). Glance at top-level markdown titles. Glance at markdown top and secondary level titles. Alphabetically sort properties inside each rule of a CSS file. Solutions: :g/Hi/d. :v/Hi/d. :g/Hi. qaq (to empty register); :g/TODO/yank A. Explanation: need capital A to append to rather than overwrite register. g/^#. :g/^# :g/\\v^#(#)? . Explanation: Need \\v so that parentheses have magic characteristics (otherwise I‚Äôd have to escape them, which is cumbersome), need ? ‚Ä¶ :g/{/ .+1,/}/-1 sort. Explanation: /{/ is the pattern of the global command and searches for all lines that contain an {. .+1,/}/-1 is the range of the Ex command, specified as from the current line until the next line that contains a closing curly bracket. The offsets narrow the range to exclude the lines with curly brackets. The current line address here stands for each line in turn that matches the /{/ pattern. ","date":"2021-09-11","objectID":"/vim/:10:4","tags":["tools","cheatsheet"],"title":"Vim cheatsheet","uri":"/vim/"},{"categories":["craft"],"content":"Folds Command Effect zR Open all folds zM Close all folds \u003cleader\u003e\u003cspace\u003e Toggle fold under cursor (mapping of za) ","date":"2021-09-11","objectID":"/vim/:11:0","tags":["tools","cheatsheet"],"title":"Vim cheatsheet","uri":"/vim/"},{"categories":["craft"],"content":"Mappings General syntax: {cmd} {attr} {lhs} {rhs}. Mapping process: define the sequence of keys to be mapped, decide the editing mode in which the mapping will work, find a suitable and free key sequence. Understanding noremap mappings: by default, vim mappings are recursive (i.e. if a is mapped to b and b to c, then a is really mapped to c because b will be expanded on the rhs. The second mapping could be part of a plugin so that I‚Äôm not even aware of it). This behaviour is set with the remap option. To define non-recursive mappings, we can use the noremap mappings. Command Effect :nmap {char} List all normal mode mappings starting with {char} :verbose nmap {char} As above, but shows location where maps are defined ","date":"2021-09-11","objectID":"/vim/:12:0","tags":["tools","cheatsheet"],"title":"Vim cheatsheet","uri":"/vim/"},{"categories":["craft"],"content":"Editing ","date":"2021-09-11","objectID":"/vim/:13:0","tags":["tools","cheatsheet"],"title":"Vim cheatsheet","uri":"/vim/"},{"categories":["craft"],"content":"Autocompletion Command Effect \u003cC-n\u003e/\u003cC-p\u003e Trigger autocompletion and navigate \u003cC-e\u003e Dismiss autocomplete window \u003cC-n\u003e\u003cC-p\u003e Filter suggestions as we type \u003cC-x\u003e\u003cC-k\u003e Dictionary lookup (requires spellchecker on - yos) \u003cC-x\u003e\u003cC-l\u003e Autocomplete entire line \u003cC-x\u003e\u003cC-f\u003e Autocomplete filename (relative to pwd) I‚Äôve experimented with youcompleteme, which I deleted again because its too clunky for my taste. In case I want to install again in the future, this might be helpful: Often doesn‚Äôt work with Anaconda Python, and I seem to be one of those cases. Followed the suggestion in the link. I first tried compiling with /usr/bin/python3, but this didn‚Äôt work. I then tried /usr/local/bin/python3.9, following this, which seems to have worked. ###¬†Spell checking Command Effect yos Toggle spell checker (uses vim-unimpaired) ]s/[s Jump to next/previous misspelled word z= Suggest corrections [n]z= Correct word with nth suggestion zg Add current word to spell file (mnem: ‚Äúgood‚Äù) zw Remove current word from spell file (mnem: ‚Äúwrong‚Äù) zug Revert zg or zw command for current word ###¬†Formatting Command Effect gq{motion} Formats text, defaults to wrapping long lines. ###¬†Case coercion Uses vim-abolish, which deals with word variants and provides powerful searching, grepping, substitution and case coercion. Command Effect crs Coerce to snake_case crc Coerce to camelCase crm Coerce to MixedCase cru Coerce to UPPER_CASE cr- Coerce to dash-case cr. Coerce to dot.case cr\u003cspace\u003e Coerce to space case crt Coerce to Title Case Exercises: Replace all occurrences of child[ren] with adult[s]. Replace all occurrences of man with dog and vice versa. Solutions: :%S/child{,ren}/adult{,s}/g :%S/{man,dog}/{dog,man}/g. Discussion: Don‚Äôt use whitespace after comma, as Vim would treat it as part of the search/replacement pattern. ","date":"2021-09-11","objectID":"/vim/:13:1","tags":["tools","cheatsheet"],"title":"Vim cheatsheet","uri":"/vim/"},{"categories":["craft"],"content":"Comments Uses vim-commentary Main commands: gc{motion} gcc {Visual}gc :[range]Commentary ","date":"2021-09-11","objectID":"/vim/:13:2","tags":["tools","cheatsheet"],"title":"Vim cheatsheet","uri":"/vim/"},{"categories":["craft"],"content":"Language and program specific settings ","date":"2021-09-11","objectID":"/vim/:14:0","tags":["tools","cheatsheet"],"title":"Vim cheatsheet","uri":"/vim/"},{"categories":["craft"],"content":"Git I use basic commands from vim-fugitive ","date":"2021-09-11","objectID":"/vim/:14:1","tags":["tools","cheatsheet"],"title":"Vim cheatsheet","uri":"/vim/"},{"categories":["craft"],"content":"Python Execute makefiles using :make (Set up quicklist such that I can jump to errors directly, this currently doesn‚Äôt work. Probably requires some additional setup to recognise Python errors.) I use Ctags to navigate my codebase. I‚Äôve followed Tim Pope‚Äôs approach to set this up. For newly initialised or cloned directories, this setup automatically creates hooks and indexes the code with Ctags. For existing directories, you need to run git init to copy the hook templates into the local .git/hooks, and then git ctags to index the code. Ctag commands: Command Effect \u003cC-]\u003e Jump to definition of keyword under cursor g\u003cC-]\u003e As above, but select definition if there are multiple :tag {keyword} Jump to definition of keyword :tjump {keyword} As above, but select definition if there are multiple Look into using matchit or something similar for faster code navigation. ","date":"2021-09-11","objectID":"/vim/:14:2","tags":["tools","cheatsheet"],"title":"Vim cheatsheet","uri":"/vim/"},{"categories":["craft"],"content":"Latex You can use \u003cC-N\u003e completion for words that already appear in one of the open buffers. This is especially useful for bibliography completion: just open the .bib file in another buffer and \u003cC-N\u003e will provide a list of available keys. I use vimtex, with Skim as my viewer. In vimtex, most shortcuts use localleader, which, by default, is set to \\. Vimtex commands: Command Effect \\ll Toggle continuous compilation using latexmk \\lk Kill compilation process \\lc Clear auxiliary files \\lt Show table of contents \\ds{c/e/$/d} Delete surrounding command/environment/math env/delimiter \\cs{c/e/$/d} Change surrounding command/environment/math env/delimiter :VimtexDocPackage Show docs for argument under cursor or supplied package :VimtexCountWords Count words in document \u003cC-x\u003e\u003cC-o\u003e/ Citation completion (inside \\cite{) ]] To next section ]m To next environment ]n To next math zone ]r To next frame Vimtex text objects: Command Effect c Command d Delimiters (e.g. [, {) e Environment $ Inline math environment P Sections m Items ","date":"2021-09-11","objectID":"/vim/:14:3","tags":["tools","cheatsheet"],"title":"Vim cheatsheet","uri":"/vim/"},{"categories":["craft"],"content":"Obscurities \u003cplug\u003e Allows authors of plugins to pre-map commands to so users can easily map them to their preferred keys. E.g. \u003cplug\u003e(test) might stand for a very long sequence of commands. To map that sequence to \u003cleader\u003et, I can simply use nmap \u003cleader\u003et \u003cplug\u003e(test). This SE answer explains it very clearly. ","date":"2021-09-11","objectID":"/vim/:15:0","tags":["tools","cheatsheet"],"title":"Vim cheatsheet","uri":"/vim/"},{"categories":["craft"],"content":"Troubleshooting ","date":"2021-09-11","objectID":"/vim/:16:0","tags":["tools","cheatsheet"],"title":"Vim cheatsheet","uri":"/vim/"},{"categories":["craft"],"content":"Command-t ruby version differs from expected Error mesassage: ‚Äúcommand-t cound not load the C extension ‚Ä¶ VIM RUBY version (version number) Expected version (different version number).‚Äù Solution: recompile Command-T with the new system Ruby. Steps are detailed under ‚ÄúCompiling Command-T‚Äù in the Command-T helpfiles. They are as follows: cd~/.config/nvin/plugged/command-t/ruby/command-t/ext/command-t ruby extconf.rb make. ","date":"2021-09-11","objectID":"/vim/:16:1","tags":["tools","cheatsheet"],"title":"Vim cheatsheet","uri":"/vim/"},{"categories":["craft"],"content":"CSS indent not working Vim recognised css files, but used my global indent of 4 spaces rather than a file-type specific indent of 2 spaces. h: filetype notes that filetype detection files are located in the runtime path. Going there, I found the ftplugins folder that contains the default file settings. Looking at css.vim makes clear that it doesn‚Äôt set any tabstop settings, which explains why the defaults were used. Googling something along the lines of ‚Äúcustom filetype indent vim‚Äù let me to this SO answer, which helpfully links to h: filetype-plugins. Once there, it was easy to find the relevant section, ftplugin-overrule that documents how to add custom filetype settings. This is what I did, and it worked like a charm. ","date":"2021-09-11","objectID":"/vim/:16:2","tags":["tools","cheatsheet"],"title":"Vim cheatsheet","uri":"/vim/"},{"categories":["craft"],"content":"Sources This cheat sheet started out as a summary of Drew Neil‚Äôs phenomenal Practical Vim, which I can‚Äôt recommend enough as a start to learning Vim seriously. Other resources I found useful: Vim Fandom mappings tutorial Dough Black‚Äôs good vimrc Using help: https://vim.fandom.com/wiki/Learn_to_use_help Idiomatic VIM Awesome vimrc: https://github.com/amix/vimrc Vim as Python IDE: https://realpython.com/vim-and-python-a-match-made-in-heaven/ ","date":"2021-09-11","objectID":"/vim/:17:0","tags":["tools","cheatsheet"],"title":"Vim cheatsheet","uri":"/vim/"},{"categories":null,"content":" A makefile is a data base that tells the make utilit how to recompile a system. In the default use case, $: make \u003cfilename\u003e checks whether filename is out of date and, if so, recompiles it. In the way I use makefiles, $: make \u003crule\u003e executes a predefined rule to accomplish a certain task like cleaning a particular dataset. Rules consist of a target (the name of a file to be modified), prerequisites (other files on which the target depends on), and commands to be run in order to update the traget based on changes in the prerequisites. A rule tells make when a target is out of date and how to update it. A target is out of date if it doesn‚Äôt exist or is older then one of its prerequisite files. $: make executes the first specified rule, $: make \u003crule\u003e executes a particular rule. A normal prerequisite makes both an order statement and a dependency statement: the order statement ensures that all commands needed to produce the prerequisete are fully executed before any commands to produce the target, while the dependency statement ensures that the target is updated every time a prerequisite changes. Occasionally, we want a prerequisite to invoke the order without the dependency statement (i.e. target is not udpated when the prerequisite changes, but when target is being updated, then the prerequisite commandas are run first). We can do this by writing the rule as target: normal-prerequisites | order-only-prerequisites. make does its work in two phases: during the read-in phase, it reads the makefile and internalises variables and rules to construct a dependency graph of all targets and their prerequisies; during the target-update phase, it determines what rules to update in what order and executes the commandas to do so. As a result, variable and function expansion can happen either immediately (during the read-in phase) or deferred (after the read-in phase), and gives rise to two flavours of variables: recursively expanded variables, defined by varname = value are expanded at the time the variable is substituted during the target-update phase. Before that point, varname contains the content of value verbatim (e.g. if value is $(othervar), then that last string is the value of varname). In contrast, simply expanded variables, defined by varname := value is expanded immediately when the variable is defined during the read-in phase (and varname would be bound to the value of othervar in the above example). To define a variable containing all csv files in a directory, do csvs := $(wildcard *.csv). The wildcard function is needed here so that the wildcard gets expanded during function creation (as opposed to creating the variable with value *.csv). I could also create a list containing the same files but with a parquet extensions like so: `parqs := $(patsubst %.csv,%.parquet,$(wildcard *.csv)). Automatic variables: $^ is a list of all prerequisites, $@ is the target, $\u003c the first prerequisite. If a target is an action to be performed rather than a file to be updated, then it‚Äôs called a phony target. In this case, telling make that we‚Äôre using a phone target explicitly by prepending the rule with a line like .PHONY : nameofrule is useful for two reasons: make doesn‚Äôt think of a file called nameofrule as the target (which, if it did, would mean that our rule never gets run because it has no prerequisites so that make would think of nameofrule as always up to date) and it doesn‚Äôt check for implicit commands to update the target, which improves performance. Commands begin with a tab and, unless specified otherwise, are executed by bin/sh. You can set a different shell by changing the value of the SHELL variable (I usually use SHELL := /bin/bash. Each line that begines with a tab and appears within a rule context (anything between the start of one rule and another) is interpreted as a command and sent to the shell. The only thing make does with commands is to check for \\ before newline, and for variables to expand (if you want $ to appear i","date":"2021-08-23","objectID":"/makefiles/:0:0","tags":["tools","cheatsheet"],"title":"Makefiles","uri":"/makefiles/"},{"categories":null,"content":"Best practices Define a phony target: .PHONY: clean clean: rm *.csv Make $: make run all rules: .PHONY: all all : rule1 rule2 .PHONY: rule1 rule1: mkdir hello .PHONY: rule2 rule2: rm -rf hello ","date":"2021-08-23","objectID":"/makefiles/:1:0","tags":["tools","cheatsheet"],"title":"Makefiles","uri":"/makefiles/"},{"categories":null,"content":"Notes on testing and data validation. What I currently do: I use assert to test small ad-hoc pieces of code, pytest to test crucial pieces of code, and a series of validating functions that check certain assumptions about the data What I need: A process to ensure that my data preprocessing pipeline produces the intended data. I still don‚Äôt fully understand how good data scientists test their code. Unit testing seems incomplete because while it ensures that a piece of code works as expected, the main challenge when working with large datasets is often that there are special cases in the data that I don‚Äôt know in advance and can‚Äôt think of. To catch these, I need to perform tests on the full data. In addition to that, manually creating a dataframe for testing is a huge pain when I need to test functions that create more complex data patterns (e.g.¬†checking whether, in a financial transactions dataset, certain individuals in the data have a at least a certain number of monthly transactions for a certain type of bank account). I currently mainly use validating functions that operate on the entire dataset at the end of the pipeline (e.g.¬†transaction data production in entropy project), which already has proven invaluable in catching errors. Using a decorator to add each validator function to a list of validators, and then running the data through each validator in that list works well and is fairly convenient. pandera seems potentially useful in that the defined schema can be used both to validate data and ‚Äì excitingly ‚Äì can also be used to generate sample datasets for hypothesis and pytest, which could go a long way towards solving the above problem. But specifying a data schema for a non-trivial dataset is not easy, and I can‚Äôt see how to write one for a dataset like MDB, where I need constraints such as a certain number of financial accounts of a certain type per user. So, for now, I just use my own validation functions. The testing branch in the entropy project has a schema.py file that expriments with the library. This article has been very useful, suggesting the following approach to testing: assert statements for ad-hoc pieces of code in Jupyter Lab, pytest for pieces of code others user, hypothesis for code that operates on the data, and pandera or other validator libraries for overall data validation. I basically do the first and last of these, and am still looking for ways to do ","date":"2021-08-12","objectID":"/02021-11-27-testing-and-validating/:0:0","tags":["python, datascience"],"title":"Testing and validating","uri":"/02021-11-27-testing-and-validating/"},{"categories":null,"content":"pytest notes ","date":"2021-08-12","objectID":"/02021-11-27-testing-and-validating/:1:0","tags":["python, datascience"],"title":"Testing and validating","uri":"/02021-11-27-testing-and-validating/"},{"categories":null,"content":"Basic test for return value def convert_to_int(s): return int(s.replace(\",\", \"\")) Very basic test def test_convert_to_int(): assert convert_to_int(\"1,200\") == 1200 More transparent test with message, which will show when AssertionError is raised def test_convert_to_int(): actual = convert_to_int(\"1,200\") expected = 1200 message = f\"convert_to_int('1,200') returned {actual} instead of {expected}.\" assert actual == expected, message Careful with floats: because of this: 0.1 + 0.1 + 0.1 == 0.3 False Use this: 0.1 + 0.1 + 0.1 == pytest.approx(0.3) True ","date":"2021-08-12","objectID":"/02021-11-27-testing-and-validating/:1:1","tags":["python, datascience"],"title":"Testing and validating","uri":"/02021-11-27-testing-and-validating/"},{"categories":null,"content":"Testing for exceptions Use context manager that silences expected error if raised within context and raises an assertion error if expected error isn‚Äôt raised. with pytest.raises(ValueError): raise ValueError with pytest.raises(ValueError): pass Failed: DID NOT RAISE \u003cclass 'ValueError'\u003e Basic example: def incrementer(x): if isinstance(x, int): return x + 1 elif isinstance(x, str): raise TypeError(\"Please enter a number\") def test_valueerror_on_string(): example_argument = \"hello\" with pytest.raises(TypeError): incrementer(example_argument) test_valueerror_on_string() Test for correct error message def test_valueerror_on_string(): example_argument = \"hello\" with pytest.raises(TypeError) as exception_info: incrementer(example_argument) assert exception_info.match(\"Please enter a number\") test_valueerror_on_string() ","date":"2021-08-12","objectID":"/02021-11-27-testing-and-validating/:1:2","tags":["python, datascience"],"title":"Testing and validating","uri":"/02021-11-27-testing-and-validating/"},{"categories":null,"content":"What‚Äôs a ‚Äúwell-tested‚Äù function? Argument types: Bad arguments Examples: incomplete args, wrong dimensions, wrong type, etc. Return value: exception Special arguments Examples: values triggering special logic, boundary value (value between bad and good arguments and before or after values that raise special logic) Return value: expected value Normal arguments Examples: All other values, test 2 or 3 Return value: expected value ","date":"2021-08-12","objectID":"/02021-11-27-testing-and-validating/:1:3","tags":["python, datascience"],"title":"Testing and validating","uri":"/02021-11-27-testing-and-validating/"},{"categories":null,"content":"Keeping tests organised Principles to follow: Mirror structure of src directory in tests directory Name test modules as test_\u003cname of src module\u003e Within test module, collect all tests for a single function in a class named TestNameOfFunction (from DataCamp: ‚ÄòTest classes are containers inside test modules. They help separate tests for different functions within the test module, and serve as a structuring tool in the pytest framework.‚Äô) # test class layout class TestNameOfFunction(object): def test_first_thing(self): pass def test_second_thing(self): pass ","date":"2021-08-12","objectID":"/02021-11-27-testing-and-validating/:1:4","tags":["python, datascience"],"title":"Testing and validating","uri":"/02021-11-27-testing-and-validating/"},{"categories":null,"content":"Marking tests as expected to fail Sometimes we might want to differentiate between failing code and tests that we know won‚Äôt run yet or under certain conditions (e.g.¬†we might follow TDD and haven‚Äôt written a test yet, or we know a function only runs in Python 3). In this case, we can apply decorators to either functions or classes Expect to fail always (e.g.¬†because not implemented yet) class TestNameOfFunction(object): @pytest.mark.xfail def test_first_thing(self): pass @pytest.mark.xfail(reason=\"Not implemented yet.\") def test_first_thing(self): \"\"\"With optional reason arg.\"\"\" pass # or @pytest.mark.xfail(reason=\"Not implemented yet.\") class TestNameOfFunction(object): def test_first_thing(self): pass def test_first_thing(self): \"\"\"With optional reason arg.\"\"\" pass Expect to fail under certain conditions (e.g.¬†certain Python versions, operating systems, etc.). import sys class TestNameOfFunction(object): @pytest.mark.skipif(sys.version_info \u003c (3, 0), reason=\"Requires Python 3\") def test_first_thing(self): \"\"\"Only runs in Python 3.\"\"\" pass # or @pytest.mark.skipif(sys.version_info \u003c (3, 0), reason=\"Requires Python 3\") class TestNameOfFunction(object): def test_first_thing(self): \"\"\"Only runs in Python 3.\"\"\" pass ","date":"2021-08-12","objectID":"/02021-11-27-testing-and-validating/:1:5","tags":["python, datascience"],"title":"Testing and validating","uri":"/02021-11-27-testing-and-validating/"},{"categories":null,"content":"Running pytests pytest runs all tests pytest -x stops after first failure pytest \u003cpath to test module\u003e runs all tests in test module pytest \u003cpath to test module\u003e::\u003ctest class name\u003e runs all tests in test module with specified node id pytest \u003cpath to test module\u003e::\u003ctest class name\u003e::\u003ctest name\u003e runs test with specified node id pytest -k \u003cpatter\u003e runds tests that fit pattern pytest -k \u003cTestNameOfFunction\u003e runs all tests in specified class pytest -k \u003cNameOf and not second thing\u003e runs all tests in specified class except for test_second_thing pytest -r show reasons pytest -rs show reasons for skipped tests pytest -rx show reasons for xfailed tests pytest -rsx show reasons for skipped and xfailed ","date":"2021-08-12","objectID":"/02021-11-27-testing-and-validating/:1:6","tags":["python, datascience"],"title":"Testing and validating","uri":"/02021-11-27-testing-and-validating/"},{"categories":null,"content":"Fixtures # create raw and clean data files in fixture @pytest.fixture def raw_and_clean_data(): raw_path = \"raw.csv\" clean_path = \"clean.csv\" with open(raw_path, \"w\") as f: f.write(\"1000, 40\\n\" \"2000, 50\\n\") yield raw_path, clean_path # teardown code so we start with clean env in next test os.remove(raw_path) os.remove(clean_path) # use fixture in test def test_on_raw_data(raw_and_clean_data): raw_path, clean_path = raw_and_clean_data preprocess(raw_path, clean_path) Useful alternative using tempdir() and fixture chaining: @pytest.fixture def raw_and_clean_data(tempdir): raw_path = tempdir.join(\"raw.csv\") clean_path = tempdir.join(\"clean.csv\") with open(raw_path, \"w\") as f: f.write(\"1000, 40\\n\" \"2000, 50\\n\") yield raw_path, clean_path # no teardown needed NameError: name 'pytest' is not defined ","date":"2021-08-12","objectID":"/02021-11-27-testing-and-validating/:1:7","tags":["python, datascience"],"title":"Testing and validating","uri":"/02021-11-27-testing-and-validating/"},{"categories":null,"content":"Mocking Testing functions independently of dependencies ","date":"2021-08-12","objectID":"/02021-11-27-testing-and-validating/:1:8","tags":["python, datascience"],"title":"Testing and validating","uri":"/02021-11-27-testing-and-validating/"},{"categories":null,"content":"Testing models To test models, use toy datasets for which I know the correct results and perform sanity-checks using assertions I can know. Tests for training function from models import train_model def test_on_linear_data(): \"\"\"Can easily predict results precisely.\"\"\" test_arg = np.array([[1, 3], [2, 5], [3, 7]]) expected_slope = 2 expected_intercept = 1 slope, intercept = train_model(test_arg) assert slope == pytest.approx(expected_slope) assert intercept == pytest.approx(expected_intercept) def test_on_positively_correlated_data(): \"\"\"Cannot easily predict result precisely, but can still assert that slope is positive as a sanity-check. \"\"\" test_arg = np.array([[1, 4], [2, 3], [4, 8], [3, 7]]) slope, intercept = train_model(test_arg) assert slope \u003e 0 Tests for final model def model_test(test_set, slope, intercept): \"\"\"Assert that R^2 is between 0 and 1.\"\"\" rsq = ... assert 0 \u003c= rsq \u003c= 1 ","date":"2021-08-12","objectID":"/02021-11-27-testing-and-validating/:1:9","tags":["python, datascience"],"title":"Testing and validating","uri":"/02021-11-27-testing-and-validating/"},{"categories":null,"content":"Testing plots Overall approach: Create baseline plot using plotting function and store as PNG image Test plotting function and compare to baseline Install pytest-mpl def plot_best_fit_line(slope, intercept, x_array, y_array, title): \"\"\"Plotting function to be tested.\"\"\" pass @pytest.mark.mpl_image_compare def test_plot_for_linear_data(): \"\"\"Testing function. Under the hood, creates baseline and comparisons. \"\"\" slope = 2 intercept = 1 x_array = np.array([1, 2, 3]) y_array = np.array([3, 5, 7]) title = \"Test plot for linear data\" return plot_best_fit_line(slope, intercept, x_array, y_array, title) baseline image needs to be stored in baseline subfolder of the plot module testing directory. To create baseline image, do following: \u003epytest -k 'test_plot_for_linear_data' --mpl-generate-path \u003cpath-to-baseline-folder\u003e To compare future tests with baseline image run: \u003e pytest -k 'test_plot_for_linear_data' --mpl ","date":"2021-08-12","objectID":"/02021-11-27-testing-and-validating/:1:10","tags":["python, datascience"],"title":"Testing and validating","uri":"/02021-11-27-testing-and-validating/"},{"categories":null,"content":"CI Travis CI Python stuff Using Conda Travis CI build info ","date":"2021-08-12","objectID":"/02021-11-27-testing-and-validating/:2:0","tags":["python, datascience"],"title":"Testing and validating","uri":"/02021-11-27-testing-and-validating/"},{"categories":null,"content":"Resources Peter Baumgartner, Ways I use testing as a data scientist Code from DataCamp course ","date":"2021-08-12","objectID":"/02021-11-27-testing-and-validating/:3:0","tags":["python, datascience"],"title":"Testing and validating","uri":"/02021-11-27-testing-and-validating/"},{"categories":null,"content":" When people look under the hood, we want them to be impressed by the neatness, consistency, and attention to detail [‚Ä¶] If instead they see a scrambled mass of code that looks like it was written by a bevy of drunken sailors, then they are likely to conclude that the same inattention to detail pervades every other aspect of the project. Robert Martin, Clean Code ","date":"2021-07-22","objectID":"/clean-code/:0:0","tags":["cs"],"title":"Clean code","uri":"/clean-code/"},{"categories":null,"content":"Definitions A design pattern is a general repeatable solution to a frequently occuring problem. An idiom is the translation of a design pattern into code using the language clearly and correctly. ","date":"2021-07-22","objectID":"/clean-code/:1:0","tags":["cs"],"title":"Clean code","uri":"/clean-code/"},{"categories":null,"content":"Principles Don‚Äôt repeat yourself. Collect often used pieces of code in a function of class for reuse. Don‚Äôt copy and paste more than once. Single Responsibility Principle (SRP): a class or module should only have a single reason to change ‚Äì it should be responsible to a single actor that can demand change. Example: an employee class that produces outputs for the finance and HR departments violates the principle, as both the CFO and the CHO might demand changes that then unintenionally affects the output seen by the other. Solution: Separate code that different actors depend on. Corollary: don‚Äôt reuse a function for two different outputs just because it does they require the same task, only reuse the function for two outputs that require the same task and have a common owner. Example, don‚Äôt if both HR and finance need to calculate regular hours, don‚Äôt use the same function, as the CFO might want to change the definition of regular hours but HR doesn‚Äôt. Open-Closed Principle (OCP): classes should be open for extension and closed for modification. (We should easily be able to add new functionality without having to change existing functionality.) Use names to make the context explicit (e.g. ‚Äúfor user in users‚Äù is explicit, ‚Äúfor i in list‚Äù isn‚Äôt). Get to proof of concept asap. ‚ÄúYou ain‚Äôt gonna need it‚Äù (YAGNI). Don‚Äôt add functionality before it‚Äôs really necessary. ","date":"2021-07-22","objectID":"/clean-code/:2:0","tags":["cs"],"title":"Clean code","uri":"/clean-code/"},{"categories":null,"content":"Names Choose names that make clear what a thing is, what it does, and how it is used. Use plain and unabbreviated words. Omit needless words like ‚Äúget‚Äù or ‚Äúcalculate‚Äù, but remember that ‚Äúterseness and obscurity are the limits where brevity should stop‚Äù. Use verbs or verb phrases for functions, nouns for classes. Choose names of variables in proportion to their scope. Whenever appropriate, use names from the solution domain (e.g. computer or data science) or the problem domain (e.g. personal finance) otherwise. ","date":"2021-07-22","objectID":"/clean-code/:3:0","tags":["cs"],"title":"Clean code","uri":"/clean-code/"},{"categories":null,"content":"Functions Functions should do one thing and one thing only and should do it well. (It‚Äôs not always obvious what ‚Äúone thing‚Äù is, use your judgement.) Make functions as short as possible to make it obvious how they work and what they are for. Most often, blocks inside flow control statements should be one line long - calls to transparently named functions. A good function interface allows the user to do what they need without having to worry about unnecessary details. Hence: ask for the minimally required number of intuitive arguments and return the expected output. Write pure functions. A function is pure when it is idempotent (returns the same output for a given input) and has no side-effects. ","date":"2021-07-22","objectID":"/clean-code/:4:0","tags":["cs"],"title":"Clean code","uri":"/clean-code/"},{"categories":null,"content":"Comments and docstrings Don‚Äôt comment bad code ‚Äì rewrite it. Add docstrings to functions unless ‚Äì following Google ‚Äì they are helpers, short and obvious. ","date":"2021-07-22","objectID":"/clean-code/:5:0","tags":["cs"],"title":"Clean code","uri":"/clean-code/"},{"categories":null,"content":"Modules Use the module.function idiom (i.e. use import module rather than from module import function) in all but the simplest projects. ","date":"2021-07-22","objectID":"/clean-code/:6:0","tags":["cs"],"title":"Clean code","uri":"/clean-code/"},{"categories":null,"content":"Systems Kent Beck‚Äôs four rules for a simply designed system (in order or importance): It runs all tests Contains no duplication Expresses the intent of the programmer (choose expressive names, keep functions and classes small, use standard nomenclature, good unit tests) Minimises the number of classes and methods ","date":"2021-07-22","objectID":"/clean-code/:7:0","tags":["cs"],"title":"Clean code","uri":"/clean-code/"},{"categories":null,"content":"Resources Clean Code The Hitchhiker‚Äôs Guide to Python, code style IPython cookbook, writing high-quality Python code Google style guide Jeff Knupp post Think Python ","date":"2021-07-22","objectID":"/clean-code/:8:0","tags":["cs"],"title":"Clean code","uri":"/clean-code/"},{"categories":null,"content":"Switching 0 to 1 and 1 to 0 Using not. flip = lambda x: int(not x) a, b = 1, 0 flip(a), flip(b) (0, 1) Using xor. flip = lambda x: x ^ 1 a, b = 1, 0 flip(a), flip(b) (0, 1) ","date":"2021-07-03","objectID":"/python-tricks/:1:0","tags":["python"],"title":"Python tricks","uri":"/python-tricks/"},{"categories":null,"content":"Coercing input to type of something else type(\"\")(5) '5' ","date":"2021-07-03","objectID":"/python-tricks/:2:0","tags":["python"],"title":"Python tricks","uri":"/python-tricks/"},{"categories":null,"content":"If-else logic in append statement small = [1, 2] large = [11, 12] for x in [3, 4, 13, 14]: (small if x \u003c 10 else large).append(x) small, large ([1, 2, 3, 4], [11, 12, 13, 14]) ","date":"2021-07-03","objectID":"/python-tricks/:3:0","tags":["python"],"title":"Python tricks","uri":"/python-tricks/"},{"categories":null,"content":"Indexing with the unary invert operator def is_palindromic(string): return all(string[i] == string[~i] for i in range(len(string) // 2)) is_palindromic(\"kayak\"), is_palindromic(\"world\") (True, False) What‚Äôs happening here? ~ is the bitwise unary invert operator, which, for an integer x, returns -(x+1) (docs, to understand what‚Äôs going on, start here). ~1, ~2, ~12, ~-12 (-2, -3, -13, 11) This allows us to step through an array from the outside in. a = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] for i in range(5): print(a[i], a[~i]) 1 10 2 9 3 8 4 7 5 6 ","date":"2021-07-03","objectID":"/python-tricks/:4:0","tags":["python"],"title":"Python tricks","uri":"/python-tricks/"},{"categories":null,"content":"Using an iterator to eliminate leading zeroes in arrays (From Elements of Programming Interviews in Python problem 5.3) a = [0, 0, 0, 1, 0, 2, 3] a[next(i for i, x in enumerate(a) if x != 0) :] [1, 0, 2, 3] What‚Äôs happening here? Just like for a list extension, Python creates an object containing all elements that meet the condition and then iterates through them. [i for i in a if i \u003e 0] [1, 2, 3] iterator = (i for i in a if i \u003e 0) for item in iterator: print(item) 1 2 3 Using next once thus returns the first item that meets the condition. In the original example we thus get the index of the first non-zero item, which is 3. next(i for i, x in enumerate(a) if x != 0) 3 The rest of the syntax produces a common list slice of the form a[3:], which gets us what we want. Really rather clever. ","date":"2021-07-03","objectID":"/python-tricks/:5:0","tags":["python"],"title":"Python tricks","uri":"/python-tricks/"},{"categories":null,"content":"Sources Fluent Python Python Cookbook Elements of Programming Interviews in Python ","date":"2021-07-03","objectID":"/python-tricks/:6:0","tags":["python"],"title":"Python tricks","uri":"/python-tricks/"},{"categories":null,"content":"Modules A module is a file that contains definitions intended for reuse in a script or an interactive session. Calling import module for the first time does three things: 1) create a new namespace that acts as the global namespace for all objects defined in module, 2) execute the entire module, 3) create a name ‚Äì identical to the module name ‚Äì within the caller namespace that references to the module. This can be used to access module objects in the caller namespace as module.object. Calling from module import symbol imports symbol into the current namespace. However, the global namespace for symbol (if it‚Äôs a function) always remains the namespace in which it was defined, not the caller‚Äôs namespace. Regardless of what variant of the import statement is being used to import contents from a module, all of the module‚Äôs statements will be initialised the first time (and only the first time) the module name is encountered in an import statement (more details here). One reason from module import * is generally discouraged is that it directly imports all the module‚Äôs objects into the caller‚Äôs namespace, which is often said to cluter it up. Especially when importing large modules this makes sense, as it‚Äôs much cleaner to keep objects defined in imported modules in eponymous namespaces and accessing them via module.object, which immediately makes clear where object comes from and can help greatly with debugging. One implication of all the above is that as a developer, you don‚Äôt have to worry about clashing variable names between modules, as they are each stored in their own namespace, and accessed via moduleA.foo and moduleB.foo in the caller namespace. When we import a module foo, the interpreter first searches for a built-in module and, if none is found, searches a list of directories given the variable sys.path. sys.path contains the directory of the input script, the variable PYTHONPATH, and installation-dependent defaults. I can manipulate sys.path using standard list operations; to add a directory, use sys.path.append('dirname'). A common usecase of the above for me is to make a package available to Jupyter Notebooks. By default, a notebook‚Äôs sys.path contains the folder the noteook is located in and a bunch of conda managed directories linked to my running Conda environment. To make available a package that lives in the project root directory, just do sys.path.append('/Users/fgu/dev/projectname/packagename'). I can then reference modules from the package using from packagename import module. Use dir(modulename) to list all names defined in modulname, or dir() to list all names that are currently defined. ","date":"2021-05-21","objectID":"/modules-and-packages/:1:0","tags":["python"],"title":"Python modules and packages","uri":"/modules-and-packages/"},{"categories":null,"content":"Running a module as a script For relative imports to work as described, for instance, here and in Chapter 8 in Python Essential References and in recipees 10.1 and 10.3 in the Python Cookbook, the file into which you import has itself to be a module rather than a top-level script. If it‚Äôs the latter, it‚Äôs name will be main and it won‚Äôt be considered part of a package, regardless of where on the file system it is saved. Generally, for a file to be considered part of a package, it needs to nave a dot (.) in its name, as in package.submodule.modulename. To import modules into a main script, one (somewhat unideal) solution is to add the absolute path to the package to sys.path. ","date":"2021-05-21","objectID":"/modules-and-packages/:2:0","tags":["python"],"title":"Python modules and packages","uri":"/modules-and-packages/"},{"categories":null,"content":"Packages Packages are collections of modules. They help structure Python‚Äôs module namespace by using dotted module names. E.g. a.b refers to submodule b in package a. Thus, just as the use of modules alleviates worries about clashing global variable names between modules, using a package alleviates worries about clashing module between multi-module packages. ","date":"2021-05-21","objectID":"/modules-and-packages/:3:0","tags":["python"],"title":"Python modules and packages","uri":"/modules-and-packages/"},{"categories":null,"content":"Example: creating utility package Utils repo: If you want to publish to PyPI, choose name that doesn‚Äôt exist yet. Create virtual environment pyenv virtualenv 3.9 futils and activate venv pyenv activate futils. Create project folder with Poetry for nice default setup poetry new projectname but rename projectname subdirectory to src because of this blog post. Install required dependencies poetry add pandas numpy seaborn and development dependencies poetry add --dev ipykernel. Can reinstall dependencies using poetry install. Publish project to private server: poetry publish -r reponame. ‚Äì currently not working, getting 403 forbidden error Project repo: Install ","date":"2021-05-21","objectID":"/modules-and-packages/:3:1","tags":["python"],"title":"Python modules and packages","uri":"/modules-and-packages/"},{"categories":null,"content":"todo Automatic git credentials reading when publishing package Remove pyenv prompt warning message ","date":"2021-05-21","objectID":"/modules-and-packages/:4:0","tags":["python"],"title":"Python modules and packages","uri":"/modules-and-packages/"},{"categories":null,"content":"Sources Python docs - Modules SO answer on relative imports for scripts ","date":"2021-05-21","objectID":"/modules-and-packages/:5:0","tags":["python"],"title":"Python modules and packages","uri":"/modules-and-packages/"},{"categories":null,"content":"My notes on decorator functions (I don‚Äôt use classes enough to worry about class decorators). ","date":"2021-03-23","objectID":"/python-decorators/:0:0","tags":["python"],"title":"Python Decorators","uri":"/python-decorators/"},{"categories":null,"content":"Intro Decorators are functions designed to wrap other functions to enhance their capability at runtime. They do this by replacing the wrapped function with the return value of the decorator. They work as syntactic sugar for decorated = decorator(decorated). Decorators are run when the decorated function is defined, not when it is run (i.e.¬†they run at import time, not runtime). ","date":"2021-03-23","objectID":"/python-decorators/:1:0","tags":["python"],"title":"Python Decorators","uri":"/python-decorators/"},{"categories":null,"content":"Basic mechanics def decorator(func): print(\"Running decorator\") return func @decorator def greeter(): return \"Hello\" greeter() Running decorator 'Hello' The above is equivalent to: greeter = decorator(greeter) greeter() Running decorator 'Hello' A decorator simply replaces the value of the function it wraps with the decorator‚Äôs return value, which can, in principle, be anything. def decorator(func): return \"Decorator return value\" @decorator def f(): return \"Function return value\" f 'Decorator return value' ","date":"2021-03-23","objectID":"/python-decorators/:2:0","tags":["python"],"title":"Python Decorators","uri":"/python-decorators/"},{"categories":null,"content":"Registration decorators The simplest kind of decorator performs some kind of action and returns the function itself. registry = [] def register(func): registry.append(func.__name__) return func @register def greeter(): print(\"Hello\") registry ['greeter'] Notes: greeter = register(greeter) assigns greeter to itself, as that‚Äôs what‚Äôs returned by register. ","date":"2021-03-23","objectID":"/python-decorators/:3:0","tags":["python"],"title":"Python Decorators","uri":"/python-decorators/"},{"categories":null,"content":"Decorators that return a different function import time def timer(func): def wrapper(*args): start = time.time() result = func(*args) elapsed = time.time() - start name = func.__name__ arg_str = \", \".join(repr(arg) for arg in args) print(f\"[{elapsed:.6f}s] {name}({arg_str}) -\u003e {result}\") return result return wrapper @timer def factorial(n): return 1 if n \u003c 2 else n * factorial(n - 1) factorial(3) [0.000000s] factorial(1) -\u003e 1 [0.000353s] factorial(2) -\u003e 2 [0.000719s] factorial(3) -\u003e 6 6 ","date":"2021-03-23","objectID":"/python-decorators/:4:0","tags":["python"],"title":"Python Decorators","uri":"/python-decorators/"},{"categories":null,"content":"Q\u0026A: What‚Äôs functools.wraps() about? It ensures that function metainformation is correctly handeled. For instance, that factorial.__name__ returns ‚Äòfactorial‚Äô. Without wraps, it would return ‚Äòwrapper‚Äô. How does this work? By running factorial = timer(factorial), the decorator assigns factorial to wrapper. Thus, when we call factorial we really call wrapper, which returns the same result factorial would have, but also performs the extra functionality. We can check the name attribute of factorial to confirm this; the decorated factorial function points to wrapper, no longer to factorial. (In practice, we should decorate the wrapper function with functools.wraps(func) to ensure that function meta information is passed through, so that factorial.__name__ would return ‚Äòfactorial‚Äô.) factorial.__name__ 'wrapper' How does wrapper have access to func without taking it as an argument? func is a variable of the local scope of the timer function, which makes wrapper a closure: a function with access to variables that are neither global nor defined in its function body (my notes on closures). The below confirms this. factorial.__closure__[0].cell_contents \u003cfunction __main__.factorial(n)\u003e Where does wrapper get the arguments from factorial from? The short answer is: the arguments are passed directly to it when we call the decorated factorial function. This follows directly from the answer to the first question above: once factorial is decorated, calling it actually calls wrapper. Why don‚Äôt we pass the function arguments as arguments to timer (i.e.¬†why isn‚Äôt it timer(func, *args)? Because all timer does is replace factorial with wrapper, which then gets called as wrapper(*args). So, timer has no use for arguments. ","date":"2021-03-23","objectID":"/python-decorators/:4:1","tags":["python"],"title":"Python Decorators","uri":"/python-decorators/"},{"categories":null,"content":"Decorators with state def logger(func): calls = 0 def wrapper(*args, **kwargs): nonlocal calls calls += 1 print(f\"Call #{calls} of {func.__name__}\") return func(*args, **kwargs) return wrapper @logger def greeter(): print(\"Hello\") @logger def singer(): print(\"lalala\") @logger def congratulator(): print(\"Congratulations!\") greeter() greeter() singer() congratulator() Call #1 of greeter Hello Call #2 of greeter Hello Call #1 of singer lalala Call #1 of congratulator Congratulations! ","date":"2021-03-23","objectID":"/python-decorators/:5:0","tags":["python"],"title":"Python Decorators","uri":"/python-decorators/"},{"categories":null,"content":"Decorator with arguments Now I want the ability to deactivate the logger for certain functions. So I wrap the decorator in a decorator factory, like so: def param_logger(active=True): def decorator(func): calls = 0 def wrapper(*args, **kwargs): nonlocal calls if active: calls += 1 print(f\"Call #{calls} of {func.__name__}\") return func(*args, **kwargs) return wrapper return decorator @param_logger() def greeter(): print(\"Hello\") @param_logger(active=True) def singer(): print(\"lalala\") @param_logger(active=False) def congratulator(): print(\"Congratulations!\") greeter() greeter() singer() congratulator() Call #1 of greeter Hello Call #2 of greeter Hello Call #1 of singer lalala Congratulations! ===== work in progress ===== How does this work? I‚Äôm not completely confident, actually, but this is how I explain it to myself. How I think this works (not sure about this): temp = param_logger(), returns decorator with access to nonlocal active argument. Because we add () to decorator, decorator is immediately called and returns wrapper, which is also assigned to temp, i.e.¬†temp = decorator(func) = wrapper(*args, **kwargs). In our initial logger function above, both the argument to the outer function (func) and the variable defined inside the outer function (calls) are free variables of the closure function wrapper, meaning that wrapper has access to them even though they are not bound inside wrapper. ===== work in progress ===== If we remember that @logger def greeter(): print(\"Hello\") is equivalent to greeter = logger(greeter) and if we know that we can use __code__.co_freevars to get the free variables of a function, then it follows that we can get a view of the free variables of the decorated greeter function like so: logger(greeter).__code__.co_freevars ('calls', 'func') This is as expected. Now, what are the free variables of param_logger? param_logger().__code__.co_freevars ('active',) This makes sense: active is the function argument and we do not define any additional variables inside the scope of param_logger, so given our result above, this is what we would expect. But param_logger is a decorator factory and not a decorator, which means it produces a decorator at the time of decoration. So, what are the free variables of the decorator it produces? Similar to above, remembering that @param_logger def greeter(): print(\"Hello\") is equivalent to greeter = param_logger()(greeter) we can inspect the decorated greeter function‚Äôs free variables like so: param_logger()(greeter).__code__.co_freevars ('active', 'calls', 'func') We can see that active is now an additional free variable that our wrapper function has access to, which provides us with the answer to our question: decorator factories work by producing decorators at decoration time and passing on the specified keyword to the decorated function. ","date":"2021-03-23","objectID":"/python-decorators/:6:0","tags":["python"],"title":"Python Decorators","uri":"/python-decorators/"},{"categories":null,"content":"Decorator factory beautifying A final point for those into aesthetics or coding consistency: we can tweak our decorator factory so that we can ommit the () if we pass no keyword arguments. def logger(func=None, active=True): def decorator(func): calls = 0 def wrapper(*args, **kwargs): nonlocal calls if active: calls += 1 print(f\"Call #{calls} of {func.__name__}\") return func(*args, **kwargs) return wrapper return decorator(func) if func else decorator @logger def greeter(): print(\"Hello\") @logger() def babler(): print(\"bablebalbe\") @logger(active=True) def singer(): print(\"lalala\") @logger(active=False) def congratulator(): print(\"Congratulations!\") greeter() greeter() babler() singer() congratulator() Call #1 of greeter Hello Call #2 of greeter Hello Call #1 of babler bablebalbe Call #1 of singer lalala Congratulations! To understand what happens here, remember that decorating func with a decorator is equivalent to func = decorator(func) While decorating it with a decorator factory is equivalent to func = decorator()(func) The control flow in the final return statement of the above decorator factory simply switches between these two cases: if logger gets a function argument, then that‚Äôs akin to the first scenario, where the func argument is passed into decorator directly, and so the decorator factory returns decorator(func) to mimic this behaviour. If func is not passed, then we‚Äôre in the standard decorator factory scenario above, and we simply return the decorator uncalled, just as any plain decorator factory would. Recipe 9.6 in the Python Cookbook discusses a neat solution to the above for a registration decorator using functools.partial(), which I haven‚Äôt managed to adapt to a scenario with a decorator factory. Might give it another go later. ","date":"2021-03-23","objectID":"/python-decorators/:7:0","tags":["python"],"title":"Python Decorators","uri":"/python-decorators/"},{"categories":null,"content":"Mistakes I often make I often do the below: from functools import wraps def decorator(func): @wraps def wrapper(*args, **kwargs): print(\"Func is called:\", func.__name__) return func(*args, **kwargs) return wrapper @decorator def greeter(name): return f\"Hello {name}\" greeter(\"World\") AttributeError: 'str' object has no attribute '__module__' What‚Äôs wrong, there? @wraps should be @wraps(func). from functools import wraps def decorator(func): @wraps(func) def wrapper(*args, **kwargs): print(\"Func is called:\", func.__name__) return func(*args, **kwargs) return wrapper @decorator def greeter(name): return f\"Hello {name}\" greeter(\"World\") Func is called: greeter 'Hello World' ","date":"2021-03-23","objectID":"/python-decorators/:8:0","tags":["python"],"title":"Python Decorators","uri":"/python-decorators/"},{"categories":null,"content":"Applications Reverse function arguments from functools import wraps def reversed_arguments(f): @wraps(f) def wrapper(*args): return f(*args[::-1]) return wrapper def power(a, b): return a ** b new_power = reversed_arguments(power) new_power(2, 3) Pass kwargs to decorator and make factory return function result funcs = [] def factory(**kwargs): def adder(func): funcs.append(func(**kwargs)) return func return adder @factory(text=\"This is very cool!\") def shout(text=\"Hello\"): print(text.upper()) for f in funcs: f THIS IS VERY COOL! Create tuple and supply kwargs upon function call in make_data.py from collections import namedtuple FunctionWithKwargs = namedtuple(\"FunctionWithKwargs\", [\"func\", \"kwargs\"]) funcs = [] def factory(func=None, **kwargs): def adder(func): funcs.append(FunctionWithKwargs(func, kwargs)) return func return adder(func) if func else adder @factory(text=\"Ha\", mark=\"@\") def shout(text=\"Hello\", mark=\"!\"): print(text.upper() + mark) for f in funcs: f.func(**f.kwargs) HA@ Can I just alter the parametrisation of func inside the factory based on the kwargs and then return the newly parametrised function without having to call it? ","date":"2021-03-23","objectID":"/python-decorators/:9:0","tags":["python"],"title":"Python Decorators","uri":"/python-decorators/"},{"categories":null,"content":"Main sources Fluent Python Python Cookbook Python Essential Reference Learning Python ","date":"2021-03-23","objectID":"/python-decorators/:10:0","tags":["python"],"title":"Python Decorators","uri":"/python-decorators/"},{"categories":null,"content":"Raw strings Raw string notation keeps regular expressions sane. re tutorial ","date":"2021-03-23","objectID":"/python-regex/:1:0","tags":["python"],"title":"Regex in Python","uri":"/python-regex/"},{"categories":null,"content":"Raw strings in Python Just like the regex engine, Python uses \\ to escape characters in strings that otherwise have special meaning (e.g.¬†' and \\ itself) and to create tokens with special meaning (e.g.¬†\\n). print(\"Hello\\nWorld\") Hello World Without escaping a single quotation mark, it takes on its special meaning as a delimiter of a string. 'It's raining' SyntaxError: invalid syntax (3769801028.py, line 1) To give it its literal meaning as an apostrophe, we need to escape it. \"It's raining\" \"It's raining\" ","date":"2021-03-23","objectID":"/python-regex/:1:1","tags":["python"],"title":"Regex in Python","uri":"/python-regex/"},{"categories":null,"content":"Python and regex interaction A string is processed by the Python interpreter before being passed on to the regex engine. Once consequence of this is that if in our regex pattern we want to treat as a literal a character that has special meaning in both Python and regex, we have to escape it twice. For example: to search for a literal backslash in our regex pattern, we need to write \\\\\\\\. The Python interpreter reads this as \\\\ and passes it to the regex engine, which then reads it as \\ as desired. import re s = \"a \\ b\" m = re.search(\"a \\\\\\\\ b\", s) print(m[0]) m[0] a \\ b 'a \\\\ b' This is obviously cumbersome. A useful alternative is to use raw strings r'', which make the Python interpreter read special characters as literals, obviating the first set of escapes. Hence, it‚Äôs a good idea to use raw strings in Python regex expressions. m = re.search(r\"a \\\\ b\", s) print(m.group()) a \\ b ","date":"2021-03-23","objectID":"/python-regex/:1:2","tags":["python"],"title":"Regex in Python","uri":"/python-regex/"},{"categories":null,"content":"Escape sequences rabbit hole First things first: an escape sequence is a a sequence of characters that does not represent itself when used within a string literal but is translated into another character or sequence of characters that might be difficult or impossible to represent (from Wikipedia). When I tried a version of this string = \"foo 1a bar 2baz\" pattern = \"\\b\\d[a-z]\\b\" re.findall(pattern, string) [] it took me 10 minutes to figure out why 1a didn‚Äôt match. The short answer is: thou shalt use raw strings! raw_pattern = r\"\\b\\d[a-z]\\b\" re.findall(raw_pattern, string) ['1a'] But why? Because Python interpretes escape sequences in strings according to the rules of Standard C, where \\b happens to stand for the backspace. Hence, the pattern without the r prefix means ‚Äúa backspace immediately followed by a digit immediately followed by a lowercase letter immediately followed by another backspace‚Äù, which is not present in the string. To convince ourselves of this, we can add backspaces to the string and try again ‚Äì now the pattern matches. string = \"foo \\N{backspace}1a\\N{backspace} baz 2bar\" re.findall(pattern, string) ['\\x081a\\x08'] One point that was not immediately obvious to me was why pattern works without the backspace character ‚Äì why do the backspaces in \\d and \\w not need escaping? pattern = \"\\d\\w\" re.findall(pattern, string) ['1a', '2b'] The explanation is that \\ is interpreted literally if it is not part of an escape sequence, as in print(\"a\\k\") a\\k and \\d and \\w aren‚Äôt escape sequences in Python (or C). Hence, these two tokens are passed on unaltered to the regex engine, where they are interpreted according to regex syntax rules. ","date":"2021-03-23","objectID":"/python-regex/:1:3","tags":["python"],"title":"Regex in Python","uri":"/python-regex/"},{"categories":null,"content":"Remove punctuation rabbit hole I wanted to remove punctuation in a string like the below. s = \"Some .' test \u0026 with * punctuation \\ characters.\" Thinking I was clever, I thought of the useful constants provided by the string module, which provide easy access to character sequences like the set of punctuation characters. import string string.punctuation '!\"#$%\u0026\\'()*+,-./:;\u003c=\u003e?@[\\\\]^_`{|}~' I did the below and was about to celebrate victory. p = string.punctuation try: re.sub(p, \" \", s) except Exception as e: print(e) multiple repeat at position 10 Oops! It‚Äôs a clear case where I jupmpted to a conclusion a little bit too soon, and where spending a few more minutes thinking things through before starting to code would probably have helped me see the two flaws in my approach: I need to escape special characters, and, given that I want to search for characters individually, I need to wrap them in a character rather than passing them as a single stringü§¶‚Äç‚ôÇÔ∏è p = f\"[{re.escape(string.punctuation)}]\" r = re.sub(p, \"\", s) r 'Some test with punctuation characters' To remove extra whitespace, I could use: re.sub(\" +\", \" \", r) 'Some test with punctuation characters' Alternatively, I could use a regex-native approach. p = r\"[\\W_]\" re.sub(p, \" \", s) 'Some test with punctuation characters ' ","date":"2021-03-23","objectID":"/python-regex/:1:4","tags":["python"],"title":"Regex in Python","uri":"/python-regex/"},{"categories":null,"content":"re module import re Overview of search methods pattern = \"a\" string = \"Jack is a boy\" methods = [ (\"re.match (start of string)\", re.match(pattern, string)), (\"re.search (anywhere in string)\", re.search(pattern, string)), (\"re.findall (all matches)\", re.findall(pattern, string)), (\"re.finditer (all matches as iterator)\", re.finditer(pattern, string)), ] for desc, result in methods: print(\"{:40} -\u003e {}\".format(desc, result)) re.match (start of string) -\u003e None re.search (anywhere in string) -\u003e \u003cre.Match object; span=(1, 2), match='a'\u003e re.findall (all matches) -\u003e ['a', 'a'] re.finditer (all matches as iterator) -\u003e \u003ccallable_iterator object at 0x11236d2e0\u003e ","date":"2021-03-23","objectID":"/python-regex/:2:0","tags":["python"],"title":"Regex in Python","uri":"/python-regex/"},{"categories":null,"content":"re.findall() Returns list of all matches if no capturing groups specified, and a list of capturing groups otherwise. Example: find stand-alone numbers data = \"\"\" 012 foo34 56 78bar 9 a10b \"\"\" Without capturing groups entire match is returned proper_digits = \"\\s+\\d+\\s+\" re.findall(proper_digits, data, flags=re.MULTILINE) ['\\n 012\\n', ' \\n 56\\n', '\\n9\\n '] One capturing groups returns list of capturing groups proper_digits = \"(?m)\\s+(\\d+)\\s+\" re.findall(proper_digits, data, flags=re.MULTILINE) ['012', '56', '9'] Multiple capturing groups return list of multi-tuple capturing groups proper_digits = \"\\s+(\\d)(\\d+)?\\s+\" re.findall(proper_digits, data, flags=re.MULTILINE) [('0', '12'), ('5', '6'), ('9', '')] To return the full match if the pattern uses capturing groups, simply capture the entire match, too. s = \"Hot is hot. Cold is cold.\" p = r\"((?i)(\\w+) is \\2)\" [groups[0] for groups in regex.findall(p, s)] ['Hot is hot', 'Cold is cold'] Finding overlapping matches pattern = r\"(?=(\\w+))\" re.findall(pattern, \"abc\") ['abc', 'bc', 'c'] ","date":"2021-03-23","objectID":"/python-regex/:2:1","tags":["python"],"title":"Regex in Python","uri":"/python-regex/"},{"categories":null,"content":"re.match() Find pattern at the beginning of a string line = '\"688293\"|\"777\"|\"2011-07-20\"|\"1969\"|\"20K to 30K\"' pattern = r'\"\\d+\"\\|\"(?P\u003cuser_id\u003e\\d+)\"' match = re.match(pattern, line) print(match) print(match.group(\"user_id\")) print(match[\"user_id\"]) # alternative, simpler, syntax \u003cre.Match object; span=(0, 14), match='\"688293\"|\"777\"'\u003e 777 777 from itertools import compress addresses = [ \"5412 N CLARK\", \"5148 N CLARK\", \"5800 E 58TH\", \"2122 N CLARK\" \"5645 N RAVENSWOOD\", \"1060 W ADDISON\", \"4801 N BROADWAY\", \"1039 W GRANVILLE\", ] def large_house_number(address, threshold=2000): house_number = int(re.match(\"\\d+\", address)[0]) return house_number \u003e threshold has_large_number = [large_house_number(x) for x in addresses] list(compress(addresses, has_large_number)) ['5412 N CLARK', '5148 N CLARK', '5800 E 58TH', '2122 N CLARK5645 N RAVENSWOOD', '4801 N BROADWAY'] ","date":"2021-03-23","objectID":"/python-regex/:2:2","tags":["python"],"title":"Regex in Python","uri":"/python-regex/"},{"categories":null,"content":"re.escape() I want to match ‚Äú(other)‚Äù. To match the parentheses literally, I‚Äôd have to escape them. If I don‚Äôt, the regex engine interpres them as a capturing group. m = re.search(\"(other)\", \"some (other) word\") print(m) m[0] \u003cre.Match object; span=(6, 11), match='other'\u003e 'other' I can escape manually. re.search(\"\\(other\\)\", \"some (other) word\") \u003cre.Match object; span=(5, 12), match='(other)'\u003e But if I have many fields with metacharacters (e.g.¬†variable values that contain parentheses) this is a massive pain. The solution is to just use re.escape(), which does all the work for me. re.search(re.escape(\"(other)\"), \"some (other) word\") \u003cre.Match object; span=(5, 12), match='(other)'\u003e ","date":"2021-03-23","objectID":"/python-regex/:2:3","tags":["python"],"title":"Regex in Python","uri":"/python-regex/"},{"categories":null,"content":"re.split() pattern = r\"(?\u003c=\\w)(?=[A-Z])\" s = \"ItIsAWonderfulWorld\" re.split(pattern, s) ['It', 'Is', 'A', 'Wonderful', 'World'] ","date":"2021-03-23","objectID":"/python-regex/:2:4","tags":["python"],"title":"Regex in Python","uri":"/python-regex/"},{"categories":null,"content":"re.sub() Stip a string of whitespace and punctuation. s = \"String. With! Punctu@tion# and _whitespace\" re.sub(r\"[\\W_]\", \"\", s) 'StringWithPunctutionandwhitespace' Using zero-width match to turn CamelCase into snake_case s = \"ThisIsABeautifulDay\" pattern = r\"(?\u003c=[a-zA-Z])(?=[A-Z])\" re.sub(pattern, \"_\", s).lower() 'this_is_a_beautiful_day' Use same approach with MULTILINE mode to comment out all lines. s = \"\"\"first second third\"\"\" pattern = \"(?m)^\" print(re.sub(pattern, \"#\", s)) #first #second #third ","date":"2021-03-23","objectID":"/python-regex/:2:5","tags":["python"],"title":"Regex in Python","uri":"/python-regex/"},{"categories":null,"content":"Matching end of line and end of string \\Z matches strict end of string but not cases where last character is a line-break a = \"\"\"no newline at end\"\"\" b = \"\"\"newline at end \"\"\" print(re.search(r\"d\\Z\", a)) print(re.search(r\"d\\Z\", b)) \u003cre.Match object; span=(17, 18), match='d'\u003e None \\$ matches end of string flexibly (i.e.¬†before or after final linebreak) a = \"\"\"no newline at end\"\"\" b = \"\"\"newline at end \"\"\" print(re.findall(r\"[ed]$\", a)) print(re.findall(r\"[ed]$\", b)) ['d'] ['d'] \\$ with MULTILINE mode matches end of line a = \"\"\"no newline at end\"\"\" b = \"\"\"newline at end \"\"\" print(re.findall(r\"(?m)[ed]$\", a)) print(re.findall(r\"(?m)[ed]$\", b)) ['e', 'd'] ['e', 'd'] ","date":"2021-03-23","objectID":"/python-regex/:2:6","tags":["python"],"title":"Regex in Python","uri":"/python-regex/"},{"categories":null,"content":"regex module docs Todo: Comparison between re and regex. # would usually import as `import regex as re`, but because I # want to compare to built-in re here, I'll import as regex. # default version is VERSION0, which emulates re to use additional # functionality, use VERSION1 import regex regex.DEFAULT_VERSION = regex.VERSION1 ","date":"2021-03-23","objectID":"/python-regex/:3:0","tags":["python"],"title":"Regex in Python","uri":"/python-regex/"},{"categories":null,"content":"Keep out token The keep out token \\K drops everything matched thus far from the overall match to be returned. pattern = r\"\\w+_\\K\\d+\" string = \"abc_12\" regex.match(pattern, string)[0] '12' ","date":"2021-03-23","objectID":"/python-regex/:3:1","tags":["python"],"title":"Regex in Python","uri":"/python-regex/"},{"categories":null,"content":"Inline flags Flags placed inside the regex pattern take effect from that point onwards. As an example, this helps us find uppercase words that later appear in lowercase. To start, let‚Äôs match all words that reappear later in the string. string = \"HELLO world hello world\" pattern = r\"(?i)(\\b\\w+\\b)(?=.*\\1)\" re.findall(pattern, string) ['HELLO', 'world'] To only match uppercase words that later reappear in lowercase, we can do this (explanation): pattern = r\"(\\b[A-Z]+\\b)(?=.*(?=\\b[a-z]+\\b)(?i)\\1)\" regex.findall(pattern, string) ['HELLO'] ","date":"2021-03-23","objectID":"/python-regex/:3:2","tags":["python"],"title":"Regex in Python","uri":"/python-regex/"},{"categories":null,"content":"Subroutines Subroutines obviate the repetition of long capturing groups s = \"Tarzan loves Jane\" p = r\"(Tarzan|Jane) loves (?1)\" m = regex.search(p, s) m[0], m[1] ('Tarzan loves Jane', 'Tarzan') Recursive patterns Subroutines can call themselves to create a recursive pattern, which can be useful to match tokens where one letter is perfectly balanced by another. s = \"ab and aabb and aab and aaabbb and abb\" p = r\"\\b(a(?1)?b)\\b\" regex.findall(p, s) ['ab', 'aabb', 'aaabbb'] Experimental only standalone expressions s = \"aaaabbbb aabb aab ab\" p = r\"a(?R)?b\" regex.findall(p, s) ['aaaabbbb', 'aabb', 'ab', 'ab'] s = \"a a a a b b b b aabb aab ab\" p = r\"\\b ?a(?R)? b\\b\" regex.findall(p, s) ['a a a a b b b b'] Pre-defined subroutines We can predefine subroutines to produce nicely modular patterns that can easily be reused through our regex. (The \\ in the pattern is needed because in free-spacing mode, whitespace that we want to match rather than ignore needs to be escaped.) defs = \"\"\" (?(DEFINE) (?\u003cquant\u003e\\d+) (?\u003citem\u003e\\w+) ) \"\"\" pattern = rf\"{defs} (?\u0026quant)\\ (?\u0026item)\" string = \"There were 5 elephants walking towards the water hole.\" regex.search(pattern, string, flags=regex.VERBOSE) \u003cregex.Match object; span=(11, 22), match='5 elephants'\u003e A useful application of this is to create real-word boundaries (rwb) that match between letters and other characters (rather than between word and non-word characters). defs = \"\"\" (?(DEFINE) (?\u003crwb\u003e (?i) # case insensitive (?\u003c![a-z])(?=[a-z]) # beginning of word |(?\u003c=[a-z])(?![a-z]) # end of word ) ) \"\"\" pattern = rf\"{defs} (?\u0026rwb)\\w+(?\u0026rwb)\" string = \"\"\" cats23, +dogs55, %bat*\"\"\" regex.findall(pattern, string, flags=regex.VERBOSE) ['cats', 'dogs', 'bat'] Using default word boundaries in the above string would also return digits and underscores, since they are word characters. regex.findall(r\"\\b\\w+\\b\", string) ['cats23', 'dogs55', 'bat'] ","date":"2021-03-23","objectID":"/python-regex/:3:3","tags":["python"],"title":"Regex in Python","uri":"/python-regex/"},{"categories":null,"content":"Named groups Supports named groups with a cleaner syntax: (?\u003cname\u003e...) instead of the somewhat verbose (?P\u003cname\u003e...) to define named groups s = \"Zw√§tschgi was born on 23 Dec 1986\" p = r\"\\b(?\u003cday\u003e\\d{2}) (?\u003cmonth\u003e\\w{3}) (?\u003cyear\u003e\\d{4})\\b\" regex.search(p, s).groupdict() {'day': '23', 'month': 'Dec', 'year': '1986'} and \\g\u003cname\u003e instead of (?P=name) for backreference. s = \"2012-12-12\" p = \"\\d\\d(?\u003cyy\u003e\\d\\d)-\\g\u003cyy\u003e-\\g\u003cyy\u003e\" regex.match(p, s) \u003cregex.Match object; span=(0, 10), match='2012-12-12'\u003e ","date":"2021-03-23","objectID":"/python-regex/:3:4","tags":["python"],"title":"Regex in Python","uri":"/python-regex/"},{"categories":null,"content":"Unicode categories regex provides support for unicode categories, which can be super handy. ## search for any punctuation character s = \". and _\" pattern = r\"\\p{P}\" regex.findall(pattern, s) ['.', '_'] ","date":"2021-03-23","objectID":"/python-regex/:3:5","tags":["python"],"title":"Regex in Python","uri":"/python-regex/"},{"categories":null,"content":"Variable-width lookbehinds One useful feature of regex is that it allows for variable-width lookbehinds. Like most regex engines, the re doesn‚Äôt and tells you so if you try. For example, if we want to match uppercase words preceeded by a prefix compused of digits and an underscore, such as BANANA in 123_BANANA, the below doesn‚Äôt work: string = \"123456_ORANGE abc12_APPLE\" pattern = r\"(?\u003c=\\b\\d+_)[A-Z]+\\b\" try: re.findall(pattern, string) except Exception as e: print(e) look-behind requires fixed-width pattern In contrast, regex succeeds. regex.findall(pattern, string) ['ORANGE'] Another application is if we wanted (for whatever reason) to match all words beginning with a at the beginning of a line from lines three onwards. string = \"\"\"abba abacus alibaba ada beta adagio aladin abracadabra \"\"\" pattern = \"(?\u003c=\\n.*\\n)a\\w+\" regex.findall(pattern, string) ['alibaba', 'aladin'] ","date":"2021-03-23","objectID":"/python-regex/:3:6","tags":["python"],"title":"Regex in Python","uri":"/python-regex/"},{"categories":null,"content":"Character class set operations Intersection # inside [] are optional but can make pattern easier to read pattern = r\"[[\\W]\u0026\u0026[\\S]]\" subject = \"a.b*5_c 8!\" regex.findall(pattern, subject) ['.', '*', '!'] Union pattern = r\"[ab||\\d]\" subject = \"a.b*5_c 8!\" regex.findall(pattern, subject) ['a', 'b', '5', '8'] Subtraction pattern = r\"[[a-z]--[b]]\" subject = \"a.b*5_c 8!\" regex.findall(pattern, subject) ['a', 'c'] pattern = \"[\\w--[_\\d]]\" subject = \"a b 3 k _ f 4\" regex.findall(pattern, subject) ['a', 'b', 'k', 'f'] ","date":"2021-03-23","objectID":"/python-regex/:3:7","tags":["python"],"title":"Regex in Python","uri":"/python-regex/"},{"categories":null,"content":"Pandas import pandas as pd ","date":"2021-03-23","objectID":"/python-regex/:4:0","tags":["python"],"title":"Regex in Python","uri":"/python-regex/"},{"categories":null,"content":"Insert text in position Insert an underscore between words df = pd.DataFrame({\"a\": [\"HelloWorld\", \"HappyDay\", \"SunnyHill\"]}) pattern = r\"(?\u003c=[a-z])(?=[A-Z])\" df[\"a\"] = df.a.str.replace(pattern, \"_\", regex=True) df a 0 Hello_World 1 Happy_Day 2 Sunny_Hill def colname_cleaner(df): \"\"\"Convert column names to stripped lowercase with underscores.\"\"\" df.columns = df.columns.str.lower().str.strip() return df def str_cleaner(df): \"\"\"Convert string values to stripped lowercase.\"\"\" str_cols = df.select_dtypes(\"object\") for col in str_cols: df[col] = df[col].str.lower().str.strip() return df movies = data.movies().pipe(colname_cleaner).pipe(str_cleaner) movies.head(2) title us gross worldwide gross us dvd sales production budget release date mpaa rating running time min distributor source major genre creative type director rotten tomatoes rating imdb rating imdb votes 0 the land girls 146083.0 146083.0 NaN 8000000.0 jun 12 1998 r NaN gramercy None None None None NaN 6.1 1071.0 1 first love, last rites 10876.0 10876.0 NaN 300000.0 aug 07 1998 r NaN strand None drama None None NaN 6.9 207.0 ","date":"2021-03-23","objectID":"/python-regex/:4:1","tags":["python"],"title":"Regex in Python","uri":"/python-regex/"},{"categories":null,"content":"Finding a single pattern in text pattern = \"hello\" text = \"hello world it is a beautiful day.\" match = re.search(pattern, text) match.start(), match.end(), match.group() (0, 5, 'hello') In Pandas movies.title.str.extract(\"(love)\") 0 0 NaN 1 love 2 NaN 3 NaN 4 NaN ... ... 3196 NaN 3197 NaN 3198 NaN 3199 NaN 3200 NaN 3201 rows √ó 1 columns contains(): Test if pattern or regex is contained within a string of a Series or Index. match(): Determine if each string starts with a match of a regular expression. fullmatch(): extract(): Extract capture groups in the regex pat as columns in a DataFrame. extractall(): Returns all matches (not just the first match). find(): findall(): replace(): movies.title.replace(\"girls\", \"hello\") 0 the land girls 1 first love, last rites 2 i married a strange person 3 let's talk about sex 4 slam ... 3196 zack and miri make a porno 3197 zodiac 3198 zoom 3199 the legend of zorro 3200 the mask of zorro Name: title, Length: 3201, dtype: object Let‚Äôs drop all movies by distributors with ‚ÄúPictures‚Äù and ‚ÄúUniversal‚Äù in their title. # inverted masking names = [\"Universal\", \"Pictures\"] pattern = \"|\".join(names) mask = movies.distributor.str.contains(pattern, na=True) result = movies[~mask] result.head(2) title us_gross worldwide_gross us_dvd_sales production_budget release_date mpaa_rating running_time_min distributor source major_genre creative_type director rotten_tomatoes_rating imdb_rating imdb_votes 0 The Land Girls 146083.0 146083.0 NaN 8000000.0 Jun 12 1998 R NaN Gramercy None None None None NaN 6.1 1071.0 1 First Love, Last Rites 10876.0 10876.0 NaN 300000.0 Aug 07 1998 R NaN Strand None Drama None None NaN 6.9 207.0 # negated regex names = [\"Universal\", \"Pictures\"] pattern = \"\\|\".join(names) neg_pattern = f\"[^{pattern}]\" neg_pattern mask = movies.distributor.str.contains(neg_pattern, na=False) result2 = movies[mask] neg_pattern '[^Universal\\\\|Pictures]' def drop_card_repayments(df): \"\"\"Drop card repayment transactions from current accounts.\"\"\" tags = [\"credit card repayment\", \"credit card payment\", \"credit card\"] pattern = \"|\".join(tags) mask = df.auto_tag.str.contains(pattern) \u0026 df.account_type.eq(\"current\") return df[~mask] ","date":"2021-03-23","objectID":"/python-regex/:4:2","tags":["python"],"title":"Regex in Python","uri":"/python-regex/"},{"categories":null,"content":"Sources Python string documentation Pyformat Fluent Python Python Cookbook Learning Python Python for Data Analysis Python Data Science Handbook ","date":"2021-03-23","objectID":"/python-regex/:5:0","tags":["python"],"title":"Regex in Python","uri":"/python-regex/"},{"categories":null,"content":" This post is part of my series of posts on pandas. Fluent Pandas contains notes on how to effectively use pandas core features. Fast pandas contains notes on how to effectively work with large datasets. Pandas cookbook is a list of recipes for effectively solving common and not so common problems. import numpy as np import pandas as pd import seaborn as sns ","date":"2021-03-12","objectID":"/fluent-pandas/:0:0","tags":["python, datascience"],"title":"Fluent Pandas","uri":"/fluent-pandas/"},{"categories":null,"content":"Sort and filter df = sns.load_dataset(\"diamonds\") print(df.shape) df.head(2) (53940, 10) carat cut color clarity depth table price x y z 0 0.23 Ideal E SI2 61.5 55.0 326 3.95 3.98 2.43 1 0.21 Premium E SI1 59.8 61.0 326 3.89 3.84 2.31 ","date":"2021-03-12","objectID":"/fluent-pandas/:1:0","tags":["python, datascience"],"title":"Fluent Pandas","uri":"/fluent-pandas/"},{"categories":null,"content":"Filter data cutoff = 30_000 a = df.loc[df.amount \u003e cutoff] b = df.query(\"amount \u003e @cutoff\") c = df[df.amount \u003e cutoff] all(a == b) == all(b == c) True ","date":"2021-03-12","objectID":"/fluent-pandas/:1:1","tags":["python, datascience"],"title":"Fluent Pandas","uri":"/fluent-pandas/"},{"categories":null,"content":"Filter columns df.filter(like=\"sepal\", axis=1).head(2) sepal_length sepal_width 0 5.1 3.5 1 4.9 3.0 df.filter(regex=\".+_length\").head(2) sepal_length petal_length 0 5.1 1.4 1 4.9 1.4 ","date":"2021-03-12","objectID":"/fluent-pandas/:1:2","tags":["python, datascience"],"title":"Fluent Pandas","uri":"/fluent-pandas/"},{"categories":null,"content":"groupb() vs resample() groupby() implements the splict-apply-combine paradigm, while resample() is a convenience method for frequency conversion and resampling of time series. When both are used on time series, the main difference is that resample() fills in missing dates while groupby() doesn‚Äôt. index = pd.date_range(\"2020\", freq=\"2d\", periods=3) data = pd.DataFrame({\"col\": range(len(index))}, index=index) data col 2020-01-01 0 2020-01-03 1 2020-01-05 2 data.resample(\"d\").col.sum() 2020-01-01 0 2020-01-02 0 2020-01-03 1 2020-01-04 0 2020-01-05 2 Freq: D, Name: col, dtype: int64 data.groupby(level=0).col.sum() 2020-01-01 0 2020-01-03 1 2020-01-05 2 Freq: 2D, Name: col, dtype: int64 ","date":"2021-03-12","objectID":"/fluent-pandas/:2:0","tags":["python, datascience"],"title":"Fluent Pandas","uri":"/fluent-pandas/"},{"categories":null,"content":"Aggregate ","date":"2021-03-12","objectID":"/fluent-pandas/:3:0","tags":["python, datascience"],"title":"Fluent Pandas","uri":"/fluent-pandas/"},{"categories":null,"content":"count() vs size() count() is a DataFrame, Series, and Grouper method that return the count of non-missing rows. size() is a Grouper method that returns the count of rows per group (including rows with missing elements) size is also a DataFrame property that returns the number of elements (including cells with missing values) and a Series property that returns the number of rows (including rows with missing values). df = sns.load_dataset(\"titanic\") df.groupby(\"sex\").count() survived pclass age sibsp parch fare embarked class who adult_male deck embark_town alive alone sex female 314 314 261 314 314 314 312 314 314 314 97 312 314 314 male 577 577 453 577 577 577 577 577 577 577 106 577 577 577 df.groupby(\"sex\").size() sex female 314 male 577 dtype: int64 ","date":"2021-03-12","objectID":"/fluent-pandas/:3:1","tags":["python, datascience"],"title":"Fluent Pandas","uri":"/fluent-pandas/"},{"categories":null,"content":"Naming columns def spread(s): return s.max() - s.min() df.groupby(\"species\").agg( mean_sepal_length=(\"sepal_length\", \"mean\"), max_petal_width=(\"petal_width\", \"max\"), spread_petal_width=(\"petal_width\", spread), ) mean_sepal_length max_petal_width spread_petal_width species setosa 5.006 0.6 0.5 versicolor 5.936 1.8 0.8 virginica 6.588 2.5 1.1 ","date":"2021-03-12","objectID":"/fluent-pandas/:3:2","tags":["python, datascience"],"title":"Fluent Pandas","uri":"/fluent-pandas/"},{"categories":null,"content":"MultiIndex Working with indices, expecially column indices, and especially with hierarchical ones, is an area of Pandas I keep finding perplexing. The point of this notebook is to help my future self. df = sns.load_dataset(\"iris\") df.head(2) sepal_length sepal_width petal_length petal_width species 0 5.1 3.5 1.4 0.2 setosa 1 4.9 3.0 1.4 0.2 setosa Create hierarchical column names df = df.set_index(\"species\") tuples = [tuple(c) for c in df.columns.str.split(\"_\")] df.columns = pd.MultiIndex.from_tuples(tuples) df.head(2) sepal petal length width length width species setosa 5.1 3.5 1.4 0.2 setosa 4.9 3.0 1.4 0.2 Flatten column names names = [\"_\".join(c) for c in df.columns] df.columns = names df.reset_index(inplace=True) df.head(2) species sepal_length sepal_width petal_length petal_width 0 setosa 5.1 3.5 1.4 0.2 1 setosa 4.9 3.0 1.4 0.2 Flattening using method (from here) df.set_axis(df.columns.map(\"_\".join), axis=1) or, of course, with a list comprehension, like so: df.set_axis([\"_\".join(c) for c in df.columns], axis=1) ","date":"2021-03-12","objectID":"/fluent-pandas/:4:0","tags":["python, datascience"],"title":"Fluent Pandas","uri":"/fluent-pandas/"},{"categories":null,"content":"Mappings ","date":"2021-03-12","objectID":"/fluent-pandas/:5:0","tags":["python, datascience"],"title":"Fluent Pandas","uri":"/fluent-pandas/"},{"categories":null,"content":"apply vs map vs applymap apply applies a function along an axis of a dataframe or on series values map applies a correspondance to each value in a series applymap applies a function to each element in a dataframe data = df.loc[:2, [\"gender\", \"merchant\"]] gender = {\"m\": \"male\", \"f\": \"female\"} data gender merchant 0 m aviva 1 m tesco 2 m mcdonalds data.apply(lambda x: x.map(gender)) gender merchant 0 male NaN 1 male NaN 2 male NaN data.gender.map(gender) 0 male 1 male 2 male Name: gender, dtype: object data.applymap(gender.get) gender merchant 0 male None 1 male None 2 male None get turns a dictionary into a function that takes a key and returns its corresponding value if the key is in the dictionary and a default value otherwise. ","date":"2021-03-12","objectID":"/fluent-pandas/:5:1","tags":["python, datascience"],"title":"Fluent Pandas","uri":"/fluent-pandas/"},{"categories":null,"content":"Sources Python for Data Analysis Python Data Science Handbook (PDSH) Pandas cookbook ","date":"2021-03-12","objectID":"/fluent-pandas/:6:0","tags":["python, datascience"],"title":"Fluent Pandas","uri":"/fluent-pandas/"},{"categories":null,"content":"Overview of Python libraries There are a number of different libraries to work with dates and times: time provides time-related functions. datetime provides classes to work with dates and times dateutil is a third-party library that provides powerful extension to datetime. pandas provides extensive functionality to work with timeseries and dates. import numpy as np import pandas as pd ","date":"2021-01-23","objectID":"/python-dates-and-times/:1:0","tags":["python"],"title":"Dates and times in Python","uri":"/python-dates-and-times/"},{"categories":null,"content":"The basics of working with dates and times in datetime I don‚Äôt usually work with (timezone-) aware dates, so these notes focus on the naive date, time, datetime, and timedelta objects. ","date":"2021-01-23","objectID":"/python-dates-and-times/:2:0","tags":["python"],"title":"Dates and times in Python","uri":"/python-dates-and-times/"},{"categories":null,"content":"date objects from datetime import date d = date.today() d datetime.date(2023, 2, 12) Useful class attributes and instance methods d.year, d.month, d.day, d.isoweekday() (2023, 1, 22, 7) d.replace(year=d.year + 1).strftime(\"%A %d %B %Y\") 'Monday 22 January 2024' ","date":"2021-01-23","objectID":"/python-dates-and-times/:2:1","tags":["python"],"title":"Dates and times in Python","uri":"/python-dates-and-times/"},{"categories":null,"content":"time objects from datetime import time t = time.fromisoformat(\"11:23:33\") t.hour, t.minute, t.second, t.replace(minute=55).minute (11, 23, 33, 55) ","date":"2021-01-23","objectID":"/python-dates-and-times/:2:2","tags":["python"],"title":"Dates and times in Python","uri":"/python-dates-and-times/"},{"categories":null,"content":"datetime objects import time from datetime import datetime dt = datetime.fromtimestamp(time.time()) dt.day, dt.year, dt.replace(day=1).strftime(\"%d %B %Y\") (22, 2023, '01 January 2023') ","date":"2021-01-23","objectID":"/python-dates-and-times/:2:3","tags":["python"],"title":"Dates and times in Python","uri":"/python-dates-and-times/"},{"categories":null,"content":"timedelta objects today = date.today() xmas = date.fromisoformat(\"2023-12-25\") td = xmas - today print(f\"Only {td.days} days to Xmas!\") Only 337 days to Xmas! from datetime import timedelta year = timedelta(days=365) year_from_today = today + year year_from_today.strftime(\"%d %b %Y\") '22 Jan 2024' ","date":"2021-01-23","objectID":"/python-dates-and-times/:2:4","tags":["python"],"title":"Dates and times in Python","uri":"/python-dates-and-times/"},{"categories":null,"content":"strftime() and strptime() behaviour A quick reference for strftime() and strptime() codes that I use frequently and keep forgetting. The full list is here. As a reminder: strftime() is an instance method that converts datetime objects into a string of a given format, while strptime() is a class method that parses a string and converts it to datetime. from datetime import datetime now = datetime.strptime(\"11 Dec 2022 09:55\", \"%d %b %Y %H:%M\") now datetime.datetime(2022, 12, 11, 9, 55) fmt = \"%d %b %Y\" now.strftime(fmt) '11 Dec 2022' now.strftime(\"%d %b %y\"), now.strftime(\"%d %b %Y\") ('11 Dec 22', '11 Dec 2022') now.strftime(\"%d %b %Y\"), now.strftime(\"%d %B %Y\") ('11 Dec 2022', '11 December 2022') now.strftime(\"%a\"), now.strftime(\"%A\") ('Sun', 'Sunday') now.strftime(\"%H:%M:%S\"), now.strftime(\"%I:%M%p\") ('09:55:00', '09:55AM') ","date":"2021-01-23","objectID":"/python-dates-and-times/:2:5","tags":["python"],"title":"Dates and times in Python","uri":"/python-dates-and-times/"},{"categories":null,"content":"Powerful parsing with dateutil While datetime can only dates in ISO format, dateutil is more flexibe, which is often useful. from dateutil.parser import parse d = parse(\"22 Dec 2022\") d.year, d.month, d.day (2022, 12, 22) ","date":"2021-01-23","objectID":"/python-dates-and-times/:3:0","tags":["python"],"title":"Dates and times in Python","uri":"/python-dates-and-times/"},{"categories":null,"content":"Recipees for frequently used tasks Stuff to remember: I mostly work in Pandas, so most of these recipees are Pandas-specific pd.Timestamp is Pandas‚Äôs equivalent of Python‚Äôs datetime.datetime ","date":"2021-01-23","objectID":"/python-dates-and-times/:4:0","tags":["python"],"title":"Dates and times in Python","uri":"/python-dates-and-times/"},{"categories":null,"content":"Parsing a human-readable date string Using datetime from datetime import datetime date = \"22 Jan 2022\" datetime.strptime(date, \"%d %b %Y\") datetime.datetime(2022, 1, 22, 0, 0) Using dateutil from dateutil.parser import parse parse(date) datetime.datetime(2022, 1, 22, 0, 0) Using pandas pd.Timestamp(date) Timestamp('2022-01-22 00:00:00') ","date":"2021-01-23","objectID":"/python-dates-and-times/:4:1","tags":["python"],"title":"Dates and times in Python","uri":"/python-dates-and-times/"},{"categories":null,"content":"Creating date and period ranges Create quarterly date rand and change format to first day of quarter in day-month-year pd.period_range(\"Jan 2023\", \"July 2024\", freq=\"Q\").asfreq(\"d\", how=\"start\") PeriodIndex(['2023-01-01', '2023-04-01', '2023-07-01', '2023-10-01', '2024-01-01', '2024-04-01', '2024-07-01'], dtype='period[D]') ","date":"2021-01-23","objectID":"/python-dates-and-times/:4:2","tags":["python"],"title":"Dates and times in Python","uri":"/python-dates-and-times/"},{"categories":null,"content":"Date offsets Period differences create Date offsets. dates = pd.period_range(start=\"2023\", periods=10) d = dates.max() - dates.min() print(d) print(type(d)) d.n \u003c9 * Days\u003e \u003cclass 'pandas._libs.tslibs.offsets.Day'\u003e 9 ","date":"2021-01-23","objectID":"/python-dates-and-times/:4:3","tags":["python"],"title":"Dates and times in Python","uri":"/python-dates-and-times/"},{"categories":null,"content":"My hdf5 cheatsheet. import h5py import numpy as np ","date":"2021-01-13","objectID":"/hdf5/:0:0","tags":["tools"],"title":"HDF5","uri":"/hdf5/"},{"categories":null,"content":"Create a file f = h5py.File('demo.hdf5', 'w') data = np.arange(10) data array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) f['array'] = data dset = f['array'] dset \u003cHDF5 dataset \"array\": shape (10,), type \"\u003ci8\"\u003e dset[:] array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) dset[[1, 2, 5]] array([1, 2, 5]) Add additional data f['dataset'] = data f['full/dataset'] = data list(f.keys()) ['array', 'dataset', 'full'] grp = f['full'] 'dataset' in grp True list(grp.keys()) ['dataset'] Create dataset dset = f.create_dataset('/full/bigger', (10000, 1000, 1000, 1000), compression='gzip') ","date":"2021-01-13","objectID":"/hdf5/:1:0","tags":["tools"],"title":"HDF5","uri":"/hdf5/"},{"categories":null,"content":"Set attributes dset.attrs \u003cAttributes of HDF5 object at 140618810188336\u003e Atributes again have dictionary structure, so can add attribute like so: dset.attrs['sampling frequency'] = 'Every other week between 1 Jan 2001 and 7 Feb 2010' dset.attrs['PI'] = 'Fabian' list(dset.attrs.items()) for i in dset.attrs.items(): print(i) ('PI', 'Fabian') ('sampling frequency', 'Every other week between 1 Jan 2001 and 7 Feb 2010') ","date":"2021-01-13","objectID":"/hdf5/:2:0","tags":["tools"],"title":"HDF5","uri":"/hdf5/"},{"categories":null,"content":"Open file f.close() f = h5py.File('demo.hdf5', 'r') list(f.keys()) ['array', 'dataset', 'full'] dset = f['array'] hdf5 files are organised in a hierarchy - that‚Äôs what the ‚Äúh‚Äù stands for. dset.name '/array' root = f['/'] list(root.keys()) ['array', 'dataset', 'full'] list(f['full'].keys()) ['bigger', 'dataset'] ","date":"2021-01-13","objectID":"/hdf5/:3:0","tags":["tools"],"title":"HDF5","uri":"/hdf5/"},{"categories":null,"content":"Sources Managing Large Datasets with Python and HDF5 - O‚ÄôReilly Webcast ","date":"2021-01-13","objectID":"/hdf5/:4:0","tags":["tools"],"title":"HDF5","uri":"/hdf5/"},{"categories":null,"content":" from imports import * %config InlineBackend.figure_format = 'retina' %load_ext autoreload %autoreload 2 The autoreload extension is already loaded. To reload it, use: %reload_ext autoreload ","date":"2020-11-19","objectID":"/numpy-essentials/:0:0","tags":["python, datascience"],"title":"Documenting Sample Selection","uri":"/numpy-essentials/"},{"categories":null,"content":"Questions What‚Äôs the main difference between a Python list and a NumPy (or C) array? What is the main trade-off involved? Do array slices return a view or a copy? What can this be useful for? What are four methods to access and modify data in an array? What two features make NumPy arrays more efficient storage containers than lists? Answers The overhead: Python lists are references to objects, each of which has information like type, size, memory location, etc. NumPy arrays can only store a single data type and thus don‚Äôt need this extra overhead for each element. The tradeoff is flexibility vs speed: Python lists are very flexible (can store different types) but slow, NumPy arrays can only store homogenous data but are fast. A view, which allows you to operate on a subset of a large dataset without copying it. Indexing, slicing, boolean indexing, fancy indexing. They have less overhead (see above) and elements are stored in memory contiguously, which means they can be read faster. ","date":"2020-11-19","objectID":"/numpy-essentials/:1:0","tags":["python, datascience"],"title":"Documenting Sample Selection","uri":"/numpy-essentials/"},{"categories":null,"content":"True of false? Indexing and reshaping a = np.arange(1, 10) b = a.reshape(3, 3) b[0, 0] = 99 a[0] == 1 False a = np.arange(3) (a.reshape(1, 3) == a[:, np.newaxis]).all() False Slicing a = np.arange(10) all(a[5:1:-2] == [5, 3]) True Explanation: slicing works like [start:stop:stride], with start being included and stop being excluded in the result, and with defaults being, respectively, 0, length of array, and 1. If the stride is negative, then the defaults for start and stop get reversed, and start is now the endpoint of array but will still be included (it‚Äôs still the start element), and stop is now the start point (but, if specified, will still be ommitted). Fancy indexing a = np.array([1, 2, 3, 4, 5]) idx = [3, 2, 0] all(a[idx] == [4, 3, 2]) False 2d fancy indexing X = np.arange(12).reshape(3, 4) X array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]]) row = np.array([1, 2]) col = np.array([2, 3]) (X[row, col] == [6, 10]).all() False Fancy indexing and simple indexing all(X[2, [3, 2, 1]] == [11, 10, 9]) True Fancy indexing and slicing (X[1:, [1, 2]] == [[5, 6], [9, 10]]).all() True Fancy indexing and masking mask = np.array([1, 0, 1]) rows = np.array([1, 2]) (X[rows[:, np.newaxis], mask] == [[5, 4, 5], [9, 8, 9]]).all() True # what does buffering have to do with the answer? (see ?np.add.at) x = np.zeros(3) idx = [0, 1, 1, 1] x[idx] += 1 y = np.zeros_like(x) np.add.at(y, idx, 1) (x == y).all() False x = np.array([4, 3, 1, 5, 2]) (x[np.argsort(x)] == np.sort(x)).all() True x = np.arange(1, 5) all(x.reshape(-1, 1) == x.reshape(len(x), 1)) True x = np.arange(2) all(x.repeat(2) == np.tile(x, 2)) False all(np.arange(2).repeat([1, 2]) == np.array([0, 1, 1])) True a = np.arange(5) i = [0, 4] a.put(i, [99, 88]) all(a.take(i) == [99, 88]) True a = np.array([[10, 30], [70, 90]]) imax = np.expand_dims(np.argmax(a, axis=1), axis=1) np.put_along_axis(a, imax, 99, axis=0) (a == [[10, 30], [70, 99]]).all() False As exercise, replace column mins with 55. a = np.arange(6).reshape(2, 3) (a[::-1, ::-1] == [[2, 1, 0], [5, 4, 3]]).all() False x = np.array([3, 1, 4, 5, 2]) all(np.partition(x, 2) == x[np.argpartition(x, 2)]) True ","date":"2020-11-19","objectID":"/numpy-essentials/:2:0","tags":["python, datascience"],"title":"Documenting Sample Selection","uri":"/numpy-essentials/"},{"categories":null,"content":"Summing values Lessons: NumPy methods perform best on NumPy arrays; builtins, on lists. np.add.reduce() is twice as fast as np.sum() mylist, myrange = range(1000), np.arange(1000) %timeit np.add.reduce(myrange) 1.33 ¬µs ¬± 3.88 ns per loop (mean ¬± std. dev. of 7 runs, 1000000 loops each) %timeit np.sum(myrange) 3.39 ¬µs ¬± 107 ns per loop (mean ¬± std. dev. of 7 runs, 100000 loops each) %timeit sum(mylist) 11.6 ¬µs ¬± 175 ns per loop (mean ¬± std. dev. of 7 runs, 100000 loops each) %timeit functools.reduce(operator.add, mylist) 43.9 ¬µs ¬± 301 ns per loop (mean ¬± std. dev. of 7 runs, 10000 loops each) ","date":"2020-11-19","objectID":"/numpy-essentials/:3:0","tags":["python, datascience"],"title":"Documenting Sample Selection","uri":"/numpy-essentials/"},{"categories":null,"content":"Broadcasting First, it‚Äôs helpful to know how NumPy labels dimensions. The image below makes this clear. Image from Elegant SciPy Next, let‚Äôs look at how broadcasting works: Image from Python Data Science Handbook Determining broadcasting compatibility: Comparing dimensions from right to left, ignoring different number of dimensions, arrays are compatible if the dimension of either array is 1 or if they match. ","date":"2020-11-19","objectID":"/numpy-essentials/:4:0","tags":["python, datascience"],"title":"Documenting Sample Selection","uri":"/numpy-essentials/"},{"categories":null,"content":"1D # outer product of two vectors x = np.arange(1, 5).reshape(-1, 1) y = np.arange(2, 6).reshape(1, -1) x * y Explanation: -1 automatically determines the dimension in which it occurs based on all other specified dimensions. When creating x above, we create an np-array of length 4 and then specify that we want a single column while specifying -1 for the row dimension, so we end up with four rows. ","date":"2020-11-19","objectID":"/numpy-essentials/:4:1","tags":["python, datascience"],"title":"Documenting Sample Selection","uri":"/numpy-essentials/"},{"categories":null,"content":"2D a = np.arange(12).reshape(4, -1) # demeaning columns a - a.mean(0) #¬†demeaning rows row_means = a.mean(1) a - row_means[:, np.newaxis] ","date":"2020-11-19","objectID":"/numpy-essentials/:4:2","tags":["python, datascience"],"title":"Documenting Sample Selection","uri":"/numpy-essentials/"},{"categories":null,"content":"3D rng = np.random.default_rng(2312) a = rng.integers(1, 10, size=24).reshape(4, 3, 2) a Taking means along a dimension What does it mean to take the mean of the zeroth dimension? It means to ‚Äúcollapse‚Äù that dimension down to size 0 or to ‚Äúflatten‚Äù the array in that dimension, and to take the mean of each ‚Äústack‚Äù of values that was flattened. In the image above, imagine pressing down vertically from the top of the array, flattening the zeroth dimension and ending up with a shape of (3, 2). As we do this, we take the mean of each of the 3 x 2 = 6 stacks of values that we compressed on our way down. These are the means of dimension zero. In our array a, the stack in the top right corner, [0, 0], is [1, 6, 6, 1], with a mean of 3.5, so the [0, 0] element of our new flattened shape is 9, as we can see below. mean0 = a.mean(0) mean0 Demeaning an axis To demean axis 0 of a we can now simply subtract our means from the original shape. (If we were to take the mean again, each element in the resulting (3, 2) array would be 0, so demeaning worked.) (a - mean0) (a - mean0).mean(0) Broadcasting along an axis We just did this above. The reason demeaning worked is because by broadcasting rules, our smaller (3, 2) array of means got padded in position 0 to (1, 3, 2), and then broadcasted (stretched) along the first dimension to match the (4, 3, 2) shape of a. Thus, to broadcast along axis 0, we need an array of shape (3, 2) or (1, 3, 2). Similarly, to broadcast along axis 1, we need an array of shape (4, 1, 2); to broadcast along axis 2, an array of shape (4, 3, 1). To practice, let‚Äôs demean axis 1. a mean1 = a.mean(1) mean1 This has shape (4, 2), which means we can‚Äôt demean directly. a - mean1 Why didn‚Äôt this work? The smaller (4, 2) array got padded in position 0 to (1, 4, 2), which can‚Äôt be expanded to match the (4, 3, 2) shape of a. What we need ‚Äì and we already knew this ‚Äì is an array of shape (4, 1, 2). We can produce one by simply adding a dimension to our mean array. mean1 = mean1[:, np.newaxis, :] mean1 Visually, in the image above, we have now separated out the two means of each of the four layers of values, and can now broadcast these along dimension 1. Another way of thinking about this is to think of each of the our layers as its own 2D shape. In that case, all we did was calculate column means by calculating means along the first axis (just as we would in a single 2D shape, except that the first axis there would be axis 0 instead of 1), and now we are broadcasting these mean values along the rows to demean the columns. a - mean1 Again, we can see that the mean of each of the 4 x 2 = 8 blocks of size 3 is 0, just as we‚Äôd expect. Just as an exercise, let‚Äôs demean axis 2, too: a - a.mean(2)[:, :, np.newaxis] Setting array values arr = np.zeros((4, 3)) arr[:] = 5 arr columns = np.array([1, 2, 3, 4]) arr[:] = columns[:, np.newaxis] arr arr[:2] = [[8], [9]] arr ","date":"2020-11-19","objectID":"/numpy-essentials/:4:3","tags":["python, datascience"],"title":"Documenting Sample Selection","uri":"/numpy-essentials/"},{"categories":null,"content":"Understanding ravel() ","date":"2020-11-19","objectID":"/numpy-essentials/:5:0","tags":["python, datascience"],"title":"Documenting Sample Selection","uri":"/numpy-essentials/"},{"categories":null,"content":"Basics Ravel (meaning to untangle) flattens arrays. ravel() returns a one dimensional array (i.e.¬†a list) of the values of the input array in the specified order. Basically: row major order (also C order) proceeds row-wise, column major order (also Fortran order) proceeds column-wise. NumPy usually stores arrays in row-order, so that ravel() can produce a view without the need to produce a copy. If an array is stored differently (maybe because it was created from a slice), ravel() might have to produce a copy first. Especially when working with higher dimensional data, the following is a helpful reminder of how order determines the result: row major order traverses data from high to low dimensions; column major, from low to high dimensions. a = np.array([[1, 2, 3], [4, 5, 6]]) a.ravel() array([1, 2, 3, 4, 5, 6]) Different order types # (C-style) row-major order, the default a.ravel(order='C') array([1, 2, 3, 4, 5, 6]) # (Fortran-style) column-major order a.ravel(order='F') array([1, 4, 2, 5, 3, 6]) # Read as F if column-major in memory, else read as C a.ravel(order='A') array([1, 2, 3, 4, 5, 6]) # Order as layed out in memory a.ravel(order='K') array([1, 2, 3, 4, 5, 6]) (a.ravel() == a.flatten()).all() and (a.flatten() == a.reshape(-1)).all() True Then on to https://stackoverflow.com/questions/38143717/groupby-in-python-pandas-fast-way x = np.arange(12).reshape(3, 1, 4) x array([[[ 0, 1, 2, 3]], [[ 4, 5, 6, 7]], [[ 8, 9, 10, 11]]]) x.ravel() array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]) x.ravel(order='F') array([ 0, 4, 8, 1, 5, 9, 2, 6, 10, 3, 7, 11]) ","date":"2020-11-19","objectID":"/numpy-essentials/:5:1","tags":["python, datascience"],"title":"Documenting Sample Selection","uri":"/numpy-essentials/"},{"categories":null,"content":"ravel_multi_index() tbd ","date":"2020-11-19","objectID":"/numpy-essentials/:5:2","tags":["python, datascience"],"title":"Documenting Sample Selection","uri":"/numpy-essentials/"},{"categories":null,"content":"Fluency exercises Code snippets are from Jake Vanderplas‚Äô Python Data Science Handbook. ","date":"2020-11-19","objectID":"/numpy-essentials/:6:0","tags":["python, datascience"],"title":"Documenting Sample Selection","uri":"/numpy-essentials/"},{"categories":null,"content":"Draw and plot a 100 points from a multivariate normal distribution and highlight a random sample of 20 rng = np.random.default_rng(2312) mean = [0, 0] cov = [[1, 2], [2, 5]] data = rng.multivariate_normal(mean, cov, size=100) sample = rng.choice(data, 20) fmt = dict(s=150, facecolor='none', edgecolor='green') plt.scatter(data[:,0], data[:,1], alpha=0.5) plt.scatter(sample[:,0], sample[:,1], alpha=0.5, **fmt); ","date":"2020-11-19","objectID":"/numpy-essentials/:6:1","tags":["python, datascience"],"title":"Documenting Sample Selection","uri":"/numpy-essentials/"},{"categories":null,"content":"Manually draw a histogram of 1000 random values using only numpy functions and plt.plot(). rng = np.random.default_rng(2312) x = rng.normal(size=1000) # sort data into 20 bins bins = np.linspace(-5, 5, 20) labels = bins.searchsorted(x) # count observations by label counts = np.zeros_like(bins) np.add.at(counts, labels, 1) plt.plot(bins, counts, drawstyle='steps', lw=5); ","date":"2020-11-19","objectID":"/numpy-essentials/:6:2","tags":["python, datascience"],"title":"Documenting Sample Selection","uri":"/numpy-essentials/"},{"categories":null,"content":"Write a simple selection sort algorithm def selection_sort(x): for i in range(len(x)): swap = i + np.argmin(x[i:]) x[i], x[swap] = x[swap], x[i] return x x = np.array([1, 4, 3, 2, 5]) selection_sort(x) array([1, 2, 3, 4, 5]) Explain in words why a, b = b, a works. Because of Python‚Äôs evaluation order when performing value assignment, its tuple creation syntax, and tuple unpacking. When performing value assignment, Python evaluates the right-hand side first, and in this case produces the tuple (b, a) because two comma-separated values are evaluated as a tuple. Next, Python assigns the tuple to the the left-hand side. Because left-hand side consists of more than one element Python attempts tupe unpacking, assigning the first value of the tuple to the first element on the left-hand side, and the second element to the second. ","date":"2020-11-19","objectID":"/numpy-essentials/:6:3","tags":["python, datascience"],"title":"Documenting Sample Selection","uri":"/numpy-essentials/"},{"categories":null,"content":"Use np and plt functions to calculate the k nearest neighbours of 10 random points and visualise the result rng = np.random.default_rng(2312) x = rng.random(size=(10, 2)) # calculate squared distances squared_dist = ((x[np.newaxis, :, :] - x[:, np.newaxis, :]) ** 2).sum(-1) # identify k nearest neighbhours k = 2 nearest_partition = np.argpartition(squared_dist, k + 1) # plot points and connections to nearest neighbours plt.scatter(x[:, 0], x[:, 1]) for i in range(len(x)): for j in nearest_partition[i, :k + 1]: plt.plot(*zip(x[i], x[j]), color='green') Understanding the solution The most mind-bending bit for me is the calculation of the point-wise differences. To wrap my head around this, I start with a simple case: a = np.arange(5) a To get the element-wise differences for a one-dimensional array, it‚Äôs easier to visualise what to do: turn the array into a column and row vector and then use broadcasting to calculate each of the differences (broadcasting will expand each of the vectors into a 5x5 matrix, and then perform element-wise matrix subtraction). diffs = a[:, np.newaxis] - a[np.newaxis, :] diffs From here, we can then square the values and retrieve the nearest neighbour for each element of the original array: k = 2 squared_diffs = diffs ** 2 nearest_partition = np.argpartition(squared_diffs, k + 1, axis=1) nearest_partition I still occasionally get tripped up when interpreting the above: focusing on the first row, the first element, 1, says that in the partitioned array, element 1 from the squared_diffs array will be in position zero. The next element, 0, says that element zero from squared_diffs will be in position one, and so on. With this, we can now extract the k nearest neighbours for each element in our original array. for i in range(len(a)): neighbours = [] for j in nearest_partition[i, :k + 1]: if a[j] != a[i]: neighbours.append(a[j]) print(a[i], neighbours) To understand how to find nearest neighbouts in three dimensions, let‚Äôs start with a toy example. a = np.arange(4).reshape(2, 2) a Intuitively, what we want to do is build two cubes ‚Äì one built from the original array broadcasted vertically and one from the original array flipped on one of its edges and then broadcasted horizontally ‚Äì and then perform element-wise subtraction. Or, more acurately, we want to create arrays such that broadcasting builds appropriate cubes for us before it performs the element-wise subtraction. But for intuitions sake, let‚Äôs build the cubes ourselves, so we can see what happens. # broadcast array vertically (i.e. add additional layer on top of original) vertical_cube = np.broadcast_to(a[np.newaxis, :, :], (2, 2, 2)) vertical_cube # flip array on its (0, 1) edge and stretch horizontally horizontal_cube = np.broadcast_to(a[:, np.newaxis, :], (2, 2, 2)) horizontal_cube a[:, None, :] - a[:, :, None] We can now do the same for our original array. x # Add a new zeroth dimension, to get shape (1, 10, 2) x[np.newaxis, :, :] # Add a new first dimension, to flip the array on its upper edge and create a (10, 1, 2) shape x[:, np.newaxis, :] When we subtract the second from the first array, broadcasting will expand both arrays to shape (10, 10, 2), thus building our vertical (from the first array) and horizontal (from the second array) cubes on which it then performs element-wise subtraction. (We didn‚Äôt actually need to create the new zeroth dimension in our first array, because broadcasting pads the shorter array with new dimensions on the left and would thus perform this step for us.) The result is what we want: the difference in each dimension for each of the ten points to all of the ten points. Once we have this, we can square the values and sum them along dimension 2 (the last dimension, hence the use of -1 in the code above), which gives us the distances between the points. x[np.newaxis, :, :] - x[:, np.newaxis, :] ","date":"2020-11-19","objectID":"/numpy-essentials/:6:4","tags":["python, datascience"],"title":"Documenting Sample Selection","uri":"/numpy-essentials/"},{"categories":null,"content":"Find index of values in array I want to find a list of indices representing the index of each row of one array in another array (From here). x = np.array([[4, 2], [9, 3], [8, 5], [3, 3], [5, 6]]) tofind = np.array([[4, 2], [3, 3], [5, 6]]) # desired result: [0, 3, 4] # approach 1: loop result = [] for a in tofind: i = np.argwhere([((x - a) == 0).all(1)])[0][1] result.append(i) result # approach 2: broadcasting np.where((x == tofind[:, np.newaxis]).all(-1))[1] ","date":"2020-11-19","objectID":"/numpy-essentials/:6:5","tags":["python, datascience"],"title":"Documenting Sample Selection","uri":"/numpy-essentials/"},{"categories":null,"content":"Sources Python Data Science Handbook Python for Data Analysis Elegant SciPy ","date":"2020-11-19","objectID":"/numpy-essentials/:7:0","tags":["python, datascience"],"title":"Documenting Sample Selection","uri":"/numpy-essentials/"},{"categories":["craft"],"content":"This is my cheetsheet for all things Unix. I use it to keep track of useful things I learn and want to remember. ","date":"2020-11-12","objectID":"/unix-basics/:0:0","tags":null,"title":"Unix basics","uri":"/unix-basics/"},{"categories":["craft"],"content":"Basics A process is a running instance of a program. Everything is a fiele (the keyboard is read-only, the screen write only) man \u003ccommand\u003e opens the manual for \u003ccommand\u003e. man -k \u003csearch term\u003e lists all commands with \u003csearch term\u003e in the manual pages. ","date":"2020-11-12","objectID":"/unix-basics/:1:0","tags":null,"title":"Unix basics","uri":"/unix-basics/"},{"categories":["craft"],"content":"Stuff I use often and tend to forget Creating a soft link: ln -s \u003cfile\u003e \u003clink\u003e. If you use . for \u003clink\u003e, a link with the same name as \u003cfile\u003e will be created in the current location. Renaming multiple files: rename 's/\u003cpattern to replace\u003e/\u003cnew pattern\u003e' [files]. For example, to replace foo with bar in all Python files, use rename 's/foo/bar/' *.py. Notice that the string command passed is a vim substitution pattern. Copying and pasting from and to the clipboard: pbcopy and pbpaste allow you to copy from and paste to the terminal. I often use pwd | pbcopy to get a directory path for use elsewhere, and pbpaste \u003e .gitignore to create a gitignore file from a template (e.g.¬†from gitignore.io). Using the test utility: Use [[ condition ]] instead of [ condition ]. They are both test utilities, but the former is an extension of the latter that‚Äôs supported in all shells I‚Äôd ever use (see here). Moving current process to background: use ctrl-z to move current job to background, jobs to list running background jobs, and fg \u003cjob id\u003e to move job to foreground. ","date":"2020-11-12","objectID":"/unix-basics/:2:0","tags":null,"title":"Unix basics","uri":"/unix-basics/"},{"categories":["craft"],"content":"Bash scripting ' and \": single quotes are used to interpret all content literally, while double quotes allow for variable substitution. For example, echo '$PATH' will print $PATH, while echo \"$PATH\" will print the value of $PATH. $( command ): saves command output into a variable. For example, myvar=$(ls) will save the output of ls into myvar. export var: makes var available to child process. For example, export PATH=$PATH:/usr/local/bin will add /usr/local/bin to the PATH variable. let var=expr: assigns result of expression to a variable. For example, let var=5+5 will assign 10 to var. expr: prints result of expression. For example, expr 5 + 5 will print 10. $(( expression )): returns the result of expression. For example, echo $(( 5 + 5 )) will print 10. Create a basic function in bash: function_name () { \u003ccommands\u003e } ","date":"2020-11-12","objectID":"/unix-basics/:3:0","tags":null,"title":"Unix basics","uri":"/unix-basics/"},{"categories":["craft"],"content":"Permissions Three actions: r (read), w (write), x (execute). Three types of users: owner or user (u), group (g), and others (o). (a) applies to all types. Permission info is 10 characters long: first character is file type (- for file, d for directory), the remaining ones are rwx permissions for owner, group, and others, with letter indicating permission on, hyphen indicating permission off. Changing persmission: chmod \u003cuser type\u003e\u003cadd or remove\u003e\u003cpermission type\u003e. User type defaults to a. Example: chmod g+w adds write permission for group, chmod u-x removes execute permission for owner, chmod a+rwx grants all permission to everyone. chmod stands for change file mode bits. Shortcuts: Remember the following: Octal Binary 0 000 1 001 2 010 3 011 4 100 5 101 6 110 7 111 This is useful because we can use the binary numbers to refer to rwx and the Octal ones as shortcuts (e.g.¬†5 is r-x). Further using the order of users as ugo, and using one Octal shortcut for each user, we can quickly set permissions for all users (e.g.¬†753 is rwxr-x-wx). Directory permissions: r means you can read content (e.g.¬†do ls), w means you can write (e.g.¬†create files or subdirectories), and x means you can enter (e.g.¬†cd). ","date":"2020-11-12","objectID":"/unix-basics/:4:0","tags":null,"title":"Unix basics","uri":"/unix-basics/"},{"categories":["craft"],"content":"Shebang The shebang character sequence (#!) is used in the first line of a script to specify the interpreter that executes the commands in the script. There are two options, well explained here (and also, a bit more elaborately, here: one is the specify the absoute path to the interpreter (#!/bin/zsh), the other is to use the env utility to search for the specified interpreter in all directories in the $PATH variable and use the first occurrence that is found (#!/usr/bin/env zsh). The second option trades security for portability and is what most people seem to recommend. So this is what I use most of the time. ","date":"2020-11-12","objectID":"/unix-basics/:5:0","tags":null,"title":"Unix basics","uri":"/unix-basics/"},{"categories":["craft"],"content":"Command-line utilities ","date":"2020-11-12","objectID":"/unix-basics/:6:0","tags":null,"title":"Unix basics","uri":"/unix-basics/"},{"categories":["craft"],"content":"Getting help man Displays and formats the on-line manual pages. Invoke by typing man followed by the name of the command you want to know more about. info Displays and formats the entire documentation of a command. ","date":"2020-11-12","objectID":"/unix-basics/:6:1","tags":null,"title":"Unix basics","uri":"/unix-basics/"},{"categories":["craft"],"content":"Searching fzf A command-line fuzzy finder that filters the results from a given input based on additional user input. For example, ls | fzf will display all files in the current directory and subdirectories, and then filter the results based on the user input. By default, it searches for files from the current directories. Advanced example uses here. I have created shortcuts that allow me to search and preview (with bat) all files in the current directory and subdirectories with fp (‚Äúfile preview‚Äù), and do the same but then open the selected file with nvim with fv (‚Äúfile vim‚Äù). grep and rg Searches for a pattern in a file or files. To find all markdown files in the current directory and subdirectories that contain the word ‚Äúpython‚Äù: grep -r --inlcude=\"*.md\" \"python\" . rg is a faster alternative to grep. find and fd find is a command-line utility that searches for files in a directory hierarchy. fd is a simple, fast and user-friendly alternative to find. ","date":"2020-11-12","objectID":"/unix-basics/:6:2","tags":null,"title":"Unix basics","uri":"/unix-basics/"},{"categories":["craft"],"content":"File manipulation awk A programming language designed for text processing and typically used as a data extraction and reporting tool (it takes its name from the initials of its three creators). It is a standard feature of most Unix-like operating systems, and particularly useful for processing text files. It operates on a line-by-line basis and splits each line into fields. For example, to print the first and second fields of each line of a file: awk '{print $1, $2}' file.txt sort Sorts lines of text files. For example, to sort the lines of a file in reverse order: sort -r file.txt uniq Filters adjacent matching lines from input. For example, to print only unique lines of a (sorted) file: uniq file.txt cat Concatenates files and prints on the standard output. For example, to print the contents of a file: cat file.txt sed A stream editor for filtering and transforming text. It reads text, line by line, from standard input and modifies it according to an expression, before outputting it again. For example, to replace all occurrences of ‚Äúfoo‚Äù with ‚Äúbar‚Äù in a file: sed -i 's/foo/bar/g' file.txt To remove the first line of the input file: sed -i '1d' file.txt ","date":"2020-11-12","objectID":"/unix-basics/:6:3","tags":null,"title":"Unix basics","uri":"/unix-basics/"},{"categories":["craft"],"content":"Process management ps ps stands for ‚Äúprocess status‚Äù and, by default, shows all processes with controlling terminals. ","date":"2020-11-12","objectID":"/unix-basics/:6:4","tags":null,"title":"Unix basics","uri":"/unix-basics/"},{"categories":["craft"],"content":"Miscellaneous eval Evaluates the arguments as a shell command. For example, to run the command ls -l: eval ls -l This is useful when you want to run a command that is stored in a variable: cmd=\"ls -l\" eval $cmd diff Compares files line by line. xargs A command-line utility that reads data from standard input and executes a command separately for each element in the input (by default, elements are separated by blanks). For example, to create directories ‚Äòone‚Äô, ‚Äôtwo‚Äô, and ‚Äôthree‚Äô: echo \"one two three\" | xargs mkdir Or, to delete all files in the current directory and subdirectories that contain the word ‚Äúpython‚Äù: grep -r --inlcude=\"*.md\" \"python\" . | xargs rm ","date":"2020-11-12","objectID":"/unix-basics/:6:5","tags":null,"title":"Unix basics","uri":"/unix-basics/"},{"categories":["craft"],"content":"Custom scripts To practice the use of the command-line utilities, I have created a few custom scripts that I use on a regular basis. They are stored in my dotfiles repo. ","date":"2020-11-12","objectID":"/unix-basics/:7:0","tags":null,"title":"Unix basics","uri":"/unix-basics/"},{"categories":["craft"],"content":"Kill processes by name Inspired by this post, I create a script to easily kill processes by name. #!/bin/zsh # A simple script to kill a process by name. Use [tab] to select # multiple processes and press [enter] to kill them or [esc] to cancel. local pid=$(ps -e | sed 1d | fzf -m --header='[kill:process]' | awk '{print $1}') if [[ -n \"$pid\" ]] then echo $pid | xargs kill else echo \"No process selected\" fi The script saves all selected processes in the variable pid and then kills them. $( expression ) saves the output of the entire expression in the variable pid. ps -e lists all processes (by default, only processes with controlling terminals are shown, so the -e flag is passed to show all processes). sed 1d removes the first line of the output (which contains the column names). fzf -m --header='[kill:process]' displays all processes in a fuzzy finder and allows the user to select multiple processes. awk '{print $1}' prints the first column of the output (which contains the process ids). The test condition evaluation utility [[ is used to check if the variable pid is not empty. This is achieved using the -n flag, which returns true if length of the following string is non-zero (see man test). echo $pid | xargs kill passes the selected process ids to the kill command one by one. ","date":"2020-11-12","objectID":"/unix-basics/:7:1","tags":null,"title":"Unix basics","uri":"/unix-basics/"},{"categories":["craft"],"content":"Resources Ryan‚Äôs bash-scripting tutorial Ryan‚Äôs linux tutorial ","date":"2020-11-12","objectID":"/unix-basics/:8:0","tags":null,"title":"Unix basics","uri":"/unix-basics/"},{"categories":null,"content":"EAFP \u003e LBYL Just try to do what you expect to work and handle exceptions later if they occurr - ‚Äúit‚Äôs easier to ask for forgiveness than permission‚Äù (EAFP) rather than ‚Äúlook before you leap‚Äù (LBYL). def dont(m, n): if n == 0: print(\"Can't divide by 0\") return None return m / n def do(m, n): try: return m / n except ZeroDivisionError: print(\"Can't divide by 0\") return None Other examples try: dangerous_code() except SomeError: handle_error() except (OneError, OtherError): handle_multiple_errors() try: os.remove(\"filename\") except FileNotFoundError: pass ","date":"2020-11-03","objectID":"/idiomatic-python/:1:0","tags":["python"],"title":"Idiomatic Python","uri":"/idiomatic-python/"},{"categories":null,"content":"Long strings Use a separate set of parenthesis for each line and wrap them all in a set of parenthesis. my_very_big_string = ( \"For a long time I used to go to bed early. Sometimes, \" \"when I had put out my candle, my eyes would close so quickly \" \"that I had not even time to say ‚ÄúI‚Äôm going to sleep.‚Äù\" ) ","date":"2020-11-03","objectID":"/idiomatic-python/:2:0","tags":["python"],"title":"Idiomatic Python","uri":"/idiomatic-python/"},{"categories":null,"content":"Generator expression instead of if-else membership testing cities = [\"Berlin\", \"New York\", \"London\"] text = \"Time Square is in New York\" def dont(cities, text): for city in cities: if city in text: return True def do(cities, text): return any(city in text for city in cities) ","date":"2020-11-03","objectID":"/idiomatic-python/:3:0","tags":["python"],"title":"Idiomatic Python","uri":"/idiomatic-python/"},{"categories":null,"content":"Negated logical operators Use if x not in y instead of if not x in y, since they are semantically identical but the former makes it clear that not in is a single operator and reads like English. (Idion from PEP8, rationale from Alex Martelli) ","date":"2020-11-03","objectID":"/idiomatic-python/:4:0","tags":["python"],"title":"Idiomatic Python","uri":"/idiomatic-python/"},{"categories":null,"content":"Using argparse There are different ways to use argparse. I usually use def parse_args(argv): parser = argparse.ArgumentParser() parser.add_argument(\"path\") def main(path): pass # something using path I use this as my default approach because it‚Äôs appropriately simple for my normal use case and it makes testing main() easy. Alternatively, I sometimes use the below (based on this discussion) def main(argv=None): if argv is None: argv = sys.argv args = parse_args(argv) # do stuff with args ","date":"2020-11-03","objectID":"/idiomatic-python/:5:0","tags":["python"],"title":"Idiomatic Python","uri":"/idiomatic-python/"},{"categories":null,"content":"Use ast.literal_eval() instead of eval() Why? Basically, because eval is very dangerous and would happile evaluate a string like os.system(rm -rf /), while ast.literal_eval will only evaluate Python literals. ","date":"2020-11-03","objectID":"/idiomatic-python/:6:0","tags":["python"],"title":"Idiomatic Python","uri":"/idiomatic-python/"},{"categories":null,"content":" A collection of subtle (or not so subtle) mistakes I made and puzzles I‚Äôve come across. import pandas as pd import numpy as np ","date":"2020-10-07","objectID":"/python-subtleties/:0:0","tags":["python"],"title":"Python subtleties","uri":"/python-subtleties/"},{"categories":null,"content":"Changing a mutable element of an immutable sequence The puzzle is from page 40 in Fluent Python. t = (1, 2, [3, 4]) t[2] += [5, 6] TypeError: 'tuple' object does not support item assignment type(t).__name__ 'tuple' t (1, 2, [3, 4, 5, 6]) What‚Äôs going on here? As part of the assignment, Python does the following: Performs augmented addition on the value of t[2], which works because that value is the list [3, 4], which is mutable. Then it tries to assign the result from 1 to t[2], which doesn‚Äôt work, because t is immutable. But because the 2nd element in t is not the list itself but a reference to it, and because the list was changed in step 1, the value of t[2] has changed, too. A great way to visualise the process is to see what happens under the hood using the amazing Python Tutor. ","date":"2020-10-07","objectID":"/python-subtleties/:1:0","tags":["python"],"title":"Python subtleties","uri":"/python-subtleties/"},{"categories":null,"content":"NANs are True I have a dataframe with some data: df = pd.DataFrame({'data': list('abcde')}) df data 0 a 1 b 2 c 3 d 4 e I can shift the data column: df.data.shift() 0 NaN 1 a 2 b 3 c 4 d Name: data, dtype: object I want to add a check column that tells me where the shift is missing: df['check'] = np.where(df.data.shift(), 'ok', 'missing') df data check 0 a ok 1 b ok 2 c ok 3 d ok 4 e ok That‚Äôs not what I wanted. The reason it happens is that missing values that aren‚Äôt None evaluate to True (follows from the docs). One way to see this: [e for e in [np.nan, 'hello', True, None] if e] [nan, 'hello', True] Hence, to get the check I wanted I should do this: df['correct_check'] = np.where(df.data.shift().notna(), 'ok', 'missing') df data check correct_check 0 a ok missing 1 b ok ok 2 c ok ok 3 d ok ok 4 e ok ok ","date":"2020-10-07","objectID":"/python-subtleties/:2:0","tags":["python"],"title":"Python subtleties","uri":"/python-subtleties/"},{"categories":null,"content":"Truthy vs True As follows clearly from the docs, True is one of many values that evaluate to True. This seems clear enough. Yet I just caught myself getting confused by the following: I have a list of values that I want to filter for Truthy elements ‚Äì elements that evaluate to True: mylist = [np.nan, 'hello', True, None] [e for e in mylist if e] [nan, 'hello', True] This works as intended. For a moment, however, I got confused by the following: [e for e in mylist if e is True] [True] I expected it to yield the same result as the above. But it doesn‚Äôt becuase it only returns valus that actually are True, as in having the same object ID as the value True (this Stack Overflow answer makes the point nicely). We can see this below: [id(e) for e in mylist] [4599359344, 4859333552, 4556488160, 4556589160] id(True) 4556488160 ","date":"2020-10-07","objectID":"/python-subtleties/:3:0","tags":["python"],"title":"Python subtleties","uri":"/python-subtleties/"},{"categories":null,"content":"Functions as first-class objects In Python, functions are first-class objects. First-Class functions: ‚ÄúA programming language is said to have first class functions if it treats functions as first-class citizens.‚Äù First-Class Citizen (Programming): ‚ÄúA first-class citizen (sometimes calles first-class object) is an entity which supports all the operations generally available to other entities, such as being being passes as an argument, returned from a function, and assigned to a variable.‚Äù Assign function to variables def square(x): return x * x f = square(5) # Assign function *output* to variable f = square # Assign function to variable (no (), as this triggers execution) print(square) print(f) # f points to same object as square print(f.__name__) f(5) \u003cfunction square at 0x106f968b0\u003e \u003cfunction square at 0x106f968b0\u003e square 25 Use function as argument in function print(map(lambda x: x ** 2, [1, 2, 3])) filter(lambda x: x % 2 == 0, [1, 2, 3]) \u003cmap object at 0x106fd07c0\u003e \u003cfilter at 0x106f9ca90\u003e def do_twice(fn, *args): fn(*args) fn(*args) do_twice(print, \"Hello world!\") Hello world! Hello world! Return function from function def create_adder(n): def adder(m): return n + m return adder add_5 = create_adder(5) print(add_5.__name__) add_5(10) adder 15 ","date":"2020-10-05","objectID":"/python-functions/:1:0","tags":["python"],"title":"Python functions","uri":"/python-functions/"},{"categories":null,"content":"Attributes of user-defined functions def greeter(text=\"Hello\"): \"\"\"Print greeting text.\"\"\" print(text) def printer(func): print(\"Docstring:\", func.__doc__) print(\"Name:\", func.__name__) print(\"Attributes:\", func.__dict__) print(\"Code:\", func.__code__) print(\"Defaults:\", func.__defaults__) print(\"Globals:\", func.__globals__.keys()) print(\"Closures:\", func.__closure__) printer(greeter) Docstring: Print greeting text. Name: greeter Attributes: {} Code: \u003ccode object greeter at 0x7fbc814a3ea0, file \"\u003cipython-input-28-dd4fd47dbd24\u003e\", line 1\u003e Defaults: ('Hello',) Globals: dict_keys(['__name__', '__doc__', '__package__', '__loader__', '__spec__', '__builtin__', '__builtins__', '_ih', '_oh', '_dh', 'In', 'Out', 'get_ipython', 'exit', 'quit', '_', '__', '___', '_i', '_ii', '_iii', '_i1', 'greeter', 'printer', '_i2', '_i3', '_i4', '_i5', '_5', '_i6', '_6', '_i7', '_7', '_i8', 'c', '_i9', '_i10', '_i11', '_i12', '_i13', '_i14', '_i15', '_i16', '_16', '_i17', '_i18', '_18', '_i19', '_19', '_i20', '_20', '_i21', '_21', '_i22', '_i23', '_23', '_i24', '_i25', '_i26', '_26', '_i27', '_i28']) Closures: None The global attribute points to the global namespace in which the function was defined. Some of these attributes return objects that have attributes themselves: print(greeter.__doc__.upper()) print(greeter.__doc__.splitlines()) PRINT GREETING TEXT. ['Print greeting text.', ' '] print(greeter.__code__.co_argcount) print(greeter.__code__.co_code) 1 b't\\x00|\\x00\\x83\\x01\\x01\\x00d\\x01S\\x00' ","date":"2020-10-05","objectID":"/python-functions/:2:0","tags":["python"],"title":"Python functions","uri":"/python-functions/"},{"categories":null,"content":"Function scoping rules Each time a function executes, a new local namespace is created that contans the function arguments and all variables that are assigned inside the function body. Variables that are assigned inside the function body are local variables; those assigned outside the function body are global variables. To resolve names, the interpreter first checks the local namespace, then the global namespace, and then the build-in namespace. To change the value of a global variable inside the (local) function context, use the global declaration. For nested functions, the interpreter resolves names by checking the namespaces of all enclosing functions from the innermost to the outermost, and then then global and built-in namespaces. To reassign the value of a local variable defined in an enclosing function use the nonlocal declaration. ","date":"2020-10-05","objectID":"/python-functions/:3:0","tags":["python"],"title":"Python functions","uri":"/python-functions/"},{"categories":null,"content":"Use of global Because a is assigned inside the function body, it is a local variable in the local namespace of the f function and a completely separate object from the variable a defined in the global namespace. a = 5 def f(): a = 10 f() a 5 global declares the a variable created inside the function body to be a global variable, making it reassing the previously declared global variable of the same name. a = 5 def f(): global a a = 10 f() a 10 ","date":"2020-10-05","objectID":"/python-functions/:3:1","tags":["python"],"title":"Python functions","uri":"/python-functions/"},{"categories":null,"content":"Use of nonlocal: Because n reassigned inside decrement, it is a local variable of that function‚Äôs namespace. But because n -= 1 is the same as n = n - 1 and Python first runs the right hand side of assignment statements, when running n - 1 throws an error because there it is asked to reference the variable n that hasn‚Äôt been assigned yet. def countdown(n): def display(): print(n) def decrement(): n -= 1 while n: display() decrement() countdown(3) 3 UnboundLocalError: local variable 'n' referenced before assignment nonlocal declares n to be a variable in the scope of an enclosing function of decrement. def countdown(n): def display(): print(n) def decrement(): nonlocal n n -= 1 while n: display() decrement() countdown(3) 3 2 1 ","date":"2020-10-05","objectID":"/python-functions/:3:2","tags":["python"],"title":"Python functions","uri":"/python-functions/"},{"categories":null,"content":"Closures A closure is the object that results from packaging up the statements that make up a function with the environment in which it executes. David Beazley, Python Essential Reference A closure is a function that has access to variables that are neither global nor local (i.e.¬†not defined in the function‚Äôs body). Luciano Ramalho, Fluent Python A closure is an inner function that remembers and has access to variables in the local scope in which it was created, even after the outer function has finished executing. - ‚ÄòA closure closes over the free variables from the environment in which they were creates‚Äô. - A free variable is a variable defined inside the scope of the outer function that is not an argument in the inner function. Below, the ‚Äòfree variable‚Äô is message. Stanford CS course How I think of them (not sure that‚Äôs correct): The closure attribute of a user-defined function is non-empty if the function is defined inside the scope of an outer function and references a local variable defined in the scope of the outer function. From the point of view of the inner function, said variable is a nonlocal variable. A user-defined function is a closure if its closure attribute is non-empty. In the first case below, the inner function doesn‚Äôt reference a nonlocal variable and thus isn‚Äôt a closure. In the second case, it does reference a nonlocal variable, which gets stored in its closure attribute, which makes inner a closure. def outer(): x = 5 def inner(): print(\"Hello\") return inner def outer2(): x = 5 def inner(): print(x) return inner g = outer() print(g.__closure__) g = outer2() print(g.__closure__[0].cell_contents) None 5 ","date":"2020-10-05","objectID":"/python-functions/:4:0","tags":["python"],"title":"Python functions","uri":"/python-functions/"},{"categories":null,"content":"Mutable function parameters Notes based on chapter 8 in Fluent Python. Functions that take mutable objects as arguments require caution, because function arguments are aliases for the passed arguments (i.e.¬†they refer to the original object). This can cause unintended behaviour in two types of situations: When setting a mutable object as default When aliasing a mutable object passed to the constructor ","date":"2020-10-05","objectID":"/python-functions/:5:0","tags":["python"],"title":"Python functions","uri":"/python-functions/"},{"categories":null,"content":"Setting a mutable object as default class HauntedBus: def __init__(self, passengers=[]): self.passengers = passengers def pick(self, name): self.passengers.append(name) def drop(self, name): self.passengers.remove(name) bus1 = HauntedBus([\"pete\", \"lara\", \"nick\"]) bus1.drop(\"nick\") print(bus1.passengers) bus2 = HauntedBus() bus2.pick(\"heather\") print(bus2.passengers) bus3 = HauntedBus() bus3.pick(\"katy\") print(bus3.passengers) ['pete', 'lara'] ['heather'] ['heather', 'katy'] Between bus 1 and 2, all works as intended, since we passed our own list when creating bus 1. Then things get a bit weird, though: how did Heather get into bus 3? When we define the HauntedBus class, we create a single empty list that is kept in the background and will be used whenever we instantiate a new bus without a custom passenger list. Importantly, all such buses will operate on the same list. We can see this by checking the object ids of the three buses‚Äô passenger lists: assert bus1.passengers is not bus2.passengers assert bus2.passengers is bus3.passengers This shows that while the passenger list of bus 1 and 2 are not the same object, the lists of bus 2 and 3 are. Once we know that, the above behaviour makes sense: all passenger list operations on buses without a custom list operate on the same list. Anohter way of seeing this by inspecting the default dict of HauntedBus after our operations abve. HauntedBus.__init__.__defaults__ (['heather', 'katy'],) The above shows that after the bus3.pick('katy') call above, the default list is now changed, and will be inherited by future instances of HauntedBus. bus4 = HauntedBus() bus4.passengers ['heather', 'katy'] This behaviour is an example of why it matters whether we think of variables as boxes or labels. If we think that variables are boxes, then the above bevaviour doesn‚Äôt make sense, since each passenger list would be its own box with its own content. But when we think of variables as labels ‚Äì the correct way to think about them in Python ‚Äì then the behaviour makes complete sense: each time we instantiate a bus without a custom passenger list, we create a new label ‚Äì of the form name-of-bus.passengers ‚Äì for the empty list we created when we loaded or created HauntedBus. What to do to avoid the unwanted behaviour? The solution is to create a new empty list each time no list is provided. class Bus: def __init__(self, passengers=None): if passengers is None: self.passengers = [] else: self.passengers = list(passengers) def pick(self, name): self.passengers.append(name) def drop(self, name): self.passengers.remove(name) bus1 = Bus() bus1.pick(\"tim\") bus2 = Bus() bus2.passengers [] ","date":"2020-10-05","objectID":"/python-functions/:5:1","tags":["python"],"title":"Python functions","uri":"/python-functions/"},{"categories":null,"content":"Aliasing a mutable object argument inside the function The init method of the above class copies the passed passenger list by calling list(passengers). This is critical. If, instead of copying we alias the passed list, we change lists defined outside the function that are passed as arguments, which is probably not what we want. class Bus: def __init__(self, passengers=None): if passengers is None: self.passengers = [] else: self.passengers = passengers def pick(self, name): self.passengers.append(name) def drop(self, name): self.passengers.remove(name) team = [\"hugh\", \"lisa\", \"gerd\", \"adam\", \"emily\"] bus = Bus(team) bus.drop(\"hugh\") team ['lisa', 'gerd', 'adam', 'emily'] Again, the reason for this is that self.passengers is an alias for passengers, which is itself an alias for team, so that all operations we perfom on self.passengers are actually performed on team. The identity check below shows what the passengers attribute of bus is indeed the same object as the team list. bus.passengers is team True To summarise: unless there is a good reason for an exception, for functions that take mutable objects as arguments do the following: Create a new object each time a class is instantiated by using None as the default parameter, rather than creating the object at the time of the function definition. Make a copy of the mutable object for processing inside the function to leave the original object unchanged. ","date":"2020-10-05","objectID":"/python-functions/:5:2","tags":["python"],"title":"Python functions","uri":"/python-functions/"},{"categories":null,"content":"Functional programming A programming paradigm built on the idea of composing programs of functions, rather than sequential steps of execution. Advantages of functional programming: Simplifies debugging Shorter and cleaner code Modular and reusable code Memory and speed considerations: Map/filter more memory efficient because they compute elements only when called, while list comprehension buffers them all. Call to map/filter if you pass lambda comes with extra function overhead which list comprehension doesn‚Äôt have, which makes the latter usually faster. See here for more. ","date":"2020-10-05","objectID":"/python-functions/:6:0","tags":["python"],"title":"Python functions","uri":"/python-functions/"},{"categories":null,"content":"Exercises Mapping is the process of applying a function elementwise to an array and storing the result. Apply the len function to each item in a and return a list of lengths. a = [\"a\", \"ab\", \"abc\", \"abcd\"] Procedural approach. def lengths(items): result = [] for item in items: result.append(len(item)) return result lengths(a) [1, 2, 3, 4] List comprehension. def lengths(items): return [len(item) for item in items] lengths(a) [1, 2, 3, 4] Functional approach. def lengths(items): return list(map(len, items)) lengths(a) [1, 2, 3, 4] Filtering is the process of extracting items from an iterable which satisfy a certain condition. Filter all elements of even length from a. Procedural approach. def even_lengths(items): result = [] for item in items: if len(item) % 2 == 0: result.append(item) return result even_lengths(a) ['ab', 'abcd'] List comprehension. def even_lengths(items): return [item for item in items if len(item) % 2 == 0] even_lengths(a) ['ab', 'abcd'] Functional approach. def even_lengths(items): return list(filter(lambda x: len(x) % 2 == 0, items)) even_lengths(a) ['ab', 'abcd'] ","date":"2020-10-05","objectID":"/python-functions/:6:1","tags":["python"],"title":"Python functions","uri":"/python-functions/"},{"categories":null,"content":"Sources Fluent Python Python Cookbook Python Essential Reference Learning Python Stanford CS41 ","date":"2020-10-05","objectID":"/python-functions/:7:0","tags":["python"],"title":"Python functions","uri":"/python-functions/"},{"categories":null,"content":" Based on excellent materials materials from Daniel Chen here and talk here from imports import * %load_ext autoreload %autoreload 2 ","date":"2020-09-22","objectID":"/tidy-data/:0:0","tags":["python, datascience"],"title":"Tidy data in Pandas","uri":"/tidy-data/"},{"categories":null,"content":"Creating tidy data Definition by Hadley Wickham here: Each variable is a column Each observation is a row Each type of observational unit is a table. ","date":"2020-09-22","objectID":"/tidy-data/:1:0","tags":["python, datascience"],"title":"Tidy data in Pandas","uri":"/tidy-data/"},{"categories":null,"content":"Columns are values pew = pd.read_csv('https://raw.githubusercontent.com/chendaniely/pydatadc_2018-tidy/master/data/pew.csv') pew.head() religion \u003c$10k $10-20k $20-30k $30-40k $40-50k $50-75k $75-100k $100-150k \u003e150k Don't know/refused 0 Agnostic 27 34 60 81 76 137 122 109 84 96 1 Atheist 12 27 37 52 35 70 73 59 74 76 2 Buddhist 27 21 30 34 33 58 62 39 53 54 3 Catholic 418 617 732 670 638 1116 949 792 633 1489 4 Don‚Äôt know/refused 15 14 15 11 10 35 21 17 18 116 # Create a single income column pew.melt( id_vars='religion', var_name='income', value_name='count' ).head() religion income count 0 Agnostic \u003c$10k 27 1 Atheist \u003c$10k 12 2 Buddhist \u003c$10k 27 3 Catholic \u003c$10k 418 4 Don‚Äôt know/refused \u003c$10k 15 billboard = pd.read_csv('https://raw.githubusercontent.com/chendaniely/pydatadc_2018-tidy/master/data/billboard.csv') billboard.columns Index(['year', 'artist', 'track', 'time', 'date.entered', 'wk1', 'wk2', 'wk3', 'wk4', 'wk5', 'wk6', 'wk7', 'wk8', 'wk9', 'wk10', 'wk11', 'wk12', 'wk13', 'wk14', 'wk15', 'wk16', 'wk17', 'wk18', 'wk19', 'wk20', 'wk21', 'wk22', 'wk23', 'wk24', 'wk25', 'wk26', 'wk27', 'wk28', 'wk29', 'wk30', 'wk31', 'wk32', 'wk33', 'wk34', 'wk35', 'wk36', 'wk37', 'wk38', 'wk39', 'wk40', 'wk41', 'wk42', 'wk43', 'wk44', 'wk45', 'wk46', 'wk47', 'wk48', 'wk49', 'wk50', 'wk51', 'wk52', 'wk53', 'wk54', 'wk55', 'wk56', 'wk57', 'wk58', 'wk59', 'wk60', 'wk61', 'wk62', 'wk63', 'wk64', 'wk65', 'wk66', 'wk67', 'wk68', 'wk69', 'wk70', 'wk71', 'wk72', 'wk73', 'wk74', 'wk75', 'wk76'], dtype='object') idvars = billboard.columns[~bb.columns.str.startswith('wk')] tidy_billboard = billboard.melt( id_vars=idvars, var_name='week', value_name='rating' ) tidy_billboard.head() year artist track time date.entered week rating 0 2000 2 Pac Baby Don't Cry (Keep... 4:22 2000-02-26 wk1 87.0 1 2000 2Ge+her The Hardest Part Of ... 3:15 2000-09-02 wk1 91.0 2 2000 3 Doors Down Kryptonite 3:53 2000-04-08 wk1 81.0 3 2000 3 Doors Down Loser 4:24 2000-10-21 wk1 76.0 4 2000 504 Boyz Wobble Wobble 3:35 2000-04-15 wk1 57.0 ","date":"2020-09-22","objectID":"/tidy-data/:2:0","tags":["python, datascience"],"title":"Tidy data in Pandas","uri":"/tidy-data/"},{"categories":null,"content":"Multiple variables stored in one column ebola = pd.read_csv('https://raw.githubusercontent.com/chendaniely/pydatadc_2018-tidy/master/data/country_timeseries.csv') ebola.head() Date Day Cases_Guinea Cases_Liberia Cases_SierraLeone Cases_Nigeria Cases_Senegal Cases_UnitedStates Cases_Spain Cases_Mali Deaths_Guinea Deaths_Liberia Deaths_SierraLeone Deaths_Nigeria Deaths_Senegal Deaths_UnitedStates Deaths_Spain Deaths_Mali 0 1/5/2015 289 2776.0 NaN 10030.0 NaN NaN NaN NaN NaN 1786.0 NaN 2977.0 NaN NaN NaN NaN NaN 1 1/4/2015 288 2775.0 NaN 9780.0 NaN NaN NaN NaN NaN 1781.0 NaN 2943.0 NaN NaN NaN NaN NaN 2 1/3/2015 287 2769.0 8166.0 9722.0 NaN NaN NaN NaN NaN 1767.0 3496.0 2915.0 NaN NaN NaN NaN NaN 3 1/2/2015 286 NaN 8157.0 NaN NaN NaN NaN NaN NaN NaN 3496.0 NaN NaN NaN NaN NaN NaN 4 12/31/2014 284 2730.0 8115.0 9633.0 NaN NaN NaN NaN NaN 1739.0 3471.0 2827.0 NaN NaN NaN NaN NaN # Tidying ebola step by step tidy_ebola = ebola.melt(id_vars=['Date', 'Day'], value_name='Cases') tidy_ebola[['Statistic', 'Country']] = (tidy_ebola.variable .str.split('_', expand=True)) tidy_ebola.drop('variable', axis=1, inplace=True) tidy_ebola.head() Date Day Cases Statistic Country 0 1/5/2015 289 2776.0 Cases Guinea 1 1/4/2015 288 2775.0 Cases Guinea 2 1/3/2015 287 2769.0 Cases Guinea 3 1/2/2015 286 NaN Cases Guinea 4 12/31/2014 284 2730.0 Cases Guinea # Using a pipeline (ebola .melt(id_vars=['Date', 'Day'], value_name='Cases') .assign(Statistic = lambda df: df.variable.str.split('_', expand=True)[0]) .assign(Country = lambda df: df.variable.str.split('_', expand=True)[1]) .drop('variable', axis=1) ).head() Date Day Cases Statistic Country 0 1/5/2015 289 2776.0 Cases Guinea 1 1/4/2015 288 2775.0 Cases Guinea 2 1/3/2015 287 2769.0 Cases Guinea 3 1/2/2015 286 NaN Cases Guinea 4 12/31/2014 284 2730.0 Cases Guinea ","date":"2020-09-22","objectID":"/tidy-data/:3:0","tags":["python, datascience"],"title":"Tidy data in Pandas","uri":"/tidy-data/"},{"categories":null,"content":"Variables are stored in both rows and columns weather = pd.read_csv('https://raw.githubusercontent.com/chendaniely/pydatadc_2018-tidy/master/data/weather.csv') weather.head() id year month element d1 d2 d3 d4 d5 d6 d7 d8 d9 d10 d11 d12 d13 d14 d15 d16 d17 d18 d19 d20 d21 d22 d23 d24 d25 d26 d27 d28 d29 d30 d31 0 MX17004 2010 1 tmax NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 27.8 NaN 1 MX17004 2010 1 tmin NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 14.5 NaN 2 MX17004 2010 2 tmax NaN 27.3 24.1 NaN NaN NaN NaN NaN NaN NaN 29.7 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 29.9 NaN NaN NaN NaN NaN NaN NaN NaN 3 MX17004 2010 2 tmin NaN 14.4 14.4 NaN NaN NaN NaN NaN NaN NaN 13.4 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 10.7 NaN NaN NaN NaN NaN NaN NaN NaN 4 MX17004 2010 3 tmax NaN NaN NaN NaN 32.1 NaN NaN NaN NaN 34.5 NaN NaN NaN NaN NaN 31.1 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN (weather .melt( id_vars=['id', 'year', 'month', 'element'], var_name='day', value_name='temp' ) .pivot_table( index=['id', 'year', 'month', 'day'], columns='element', values='temp' ) .reset_index() .assign(day = lambda df: df.day.str.extract('(\\d+)')) ).head() element id year month day tmax tmin 0 MX17004 2010 1 30 27.8 14.5 1 MX17004 2010 2 11 29.7 13.4 2 MX17004 2010 2 2 27.3 14.4 3 MX17004 2010 2 23 29.9 10.7 4 MX17004 2010 2 3 24.1 14.4 ","date":"2020-09-22","objectID":"/tidy-data/:4:0","tags":["python, datascience"],"title":"Tidy data in Pandas","uri":"/tidy-data/"},{"categories":null,"content":"Multiple types of observational units are stored in a single table tidy_billboard.head() tidy_bb = tidy_billboard tidy_bb.head() year artist track time date.entered week rating 0 2000 2 Pac Baby Don't Cry (Keep... 4:22 2000-02-26 wk1 87.0 1 2000 2Ge+her The Hardest Part Of ... 3:15 2000-09-02 wk1 91.0 2 2000 3 Doors Down Kryptonite 3:53 2000-04-08 wk1 81.0 3 2000 3 Doors Down Loser 4:24 2000-10-21 wk1 76.0 4 2000 504 Boyz Wobble Wobble 3:35 2000-04-15 wk1 57.0 bb_songs = ( tidy_bb[['year', 'artist', 'track', 'time', 'date.entered']] .drop_duplicates() .assign(id = lambda df: range(len(df))) ) bb_songs.head() year artist track time date.entered id 0 2000 2 Pac Baby Don't Cry (Keep... 4:22 2000-02-26 0 1 2000 2Ge+her The Hardest Part Of ... 3:15 2000-09-02 1 2 2000 3 Doors Down Kryptonite 3:53 2000-04-08 2 3 2000 3 Doors Down Loser 4:24 2000-10-21 3 4 2000 504 Boyz Wobble Wobble 3:35 2000-04-15 4 bb_ratings = ( tidy_bb .merge(bb_songs) .loc[:, ['week', 'rating', 'id']] ) bb_ratings.head() week rating id 0 wk1 87.0 0 1 wk2 82.0 0 2 wk3 72.0 0 3 wk4 77.0 0 4 wk5 87.0 0 ","date":"2020-09-22","objectID":"/tidy-data/:5:0","tags":["python, datascience"],"title":"Tidy data in Pandas","uri":"/tidy-data/"},{"categories":null,"content":"SQLite Based on docs. import os import sys src_path = os.path.abspath(os.path.join(\"..\")) sys.path.append(src_path) import sqlite3 import pandas as pd from src import config # parameters SAMPLE = \"777\" df = pd.read_parquet(os.path.join(config.TEMPDIR, f\"data_{SAMPLE}.parquet\")) print(df.shape) df.head(2) (562996, 22) user_id transaction_date amount transaction_description merchant_name auto_tag tag manual_tag postcode credit_debit ... account_created user_registration_date year_of_birth account_id merchant_business_line latest_balance transaction_id account_last_refreshed account_type gender 0 60777 2014-11-27 100.0 xxxxxx xxxx5014 internet transfer no merchant transfers broadband no tag n16 0 debit ... 2015-02-12 2014-05-23 1988.0 378967 personal 0.0 58866450 2017-04-04 07:33:00 savings m 1 60777 2014-11-27 -250.0 \u003cmdbremoved\u003e no merchant savings (general) savings (general) no tag n16 0 credit ... 2015-02-12 2014-05-23 1988.0 378968 no merchant business line 3000.0 58866344 2017-04-04 07:33:00 savings m 2 rows √ó 22 columns pd.read_sql(\"select * from pragma_table_info('outcomes')\", conn).name.values array(['user_id'], dtype=object) ","date":"2020-09-12","objectID":"/sql-in-python/:1:0","tags":["tools"],"title":"SQL in Python","uri":"/sql-in-python/"},{"categories":null,"content":"Connect to database db_path = os.path.join(config.DATADIR, f\"{SAMPLE}.db\") conn = sqlite3.connect(db_path) c = conn.cursor() def create_user_list(sample): \"\"\"Create list of users of sample.\"\"\" file_name = f'data_{sample}.parquet' file_path = os.path.join(config.TEMPDIR, file_name) df = pd.read_parquet(file_path) return df.user_id.unique()drop_duplicates().sort_values() samples = ['XX7', 'X77', '777'] for sample in ['777']: create_user_list(sample).to_csv('/Users/fgu/tmp/test.csv', index=False) path = os.path.join(config.DATADIR, \"users_777.csv\") pd.read_csv(path) Unnamed: 0 user_id 0 0 777 1 0 1777 2 7116 7777 3 0 8777 4 1228 10777 ... ... ... 179 129064 578777 180 133470 579777 181 135176 582777 182 136885 586777 183 139404 587777 184 rows √ó 2 columns ","date":"2020-09-12","objectID":"/sql-in-python/:1:1","tags":["tools"],"title":"SQL in Python","uri":"/sql-in-python/"},{"categories":null,"content":"Create tables Create tables with user_ids ids = pd.Series({\"user_id\": df.user_id.unique()}) tables = [\"targets\", \"predictions\", \"outcomes\"] for table in tables: ids.to_sql(table, conn, index=False) conn.execute(f\"create index idx_{table}_user_id on {table}(user_id)\") pd.Series({\"user_id\": df.user_id.unique()}) user_id [60777, 64777, 777, 7777, 71777, 76777, 50777,... dtype: object pd.read_sql(\"select * from sqlite_master\", conn) type name tbl_name rootpage sql 0 table targets targets 2 CREATE TABLE \"targets\" (\\n\"user_id\" INTEGER\\n) 1 index idx_targets_user_id targets 3 CREATE INDEX idx_targets_user_id on targets(us... 2 table predictions predictions 4 CREATE TABLE \"predictions\" (\\n\"user_id\" INTEGE... 3 index idx_predictions_user_id predictions 5 CREATE INDEX idx_predictions_user_id on predic... 4 table outcomes outcomes 6 CREATE TABLE \"outcomes\" (\\n\"user_id\" INTEGER\\n) 5 index idx_outcomes_user_id outcomes 7 CREATE INDEX idx_outcomes_user_id on outcomes(... pd.read_sql(\"select * from targets\", conn) user_id 0 777 1 1777 2 7777 3 8777 4 10777 ... ... 179 578777 180 579777 181 582777 182 586777 183 587777 184 rows √ó 1 columns ","date":"2020-09-12","objectID":"/sql-in-python/:1:2","tags":["tools"],"title":"SQL in Python","uri":"/sql-in-python/"},{"categories":null,"content":"Add tables def db_tables(): res = conn.execute(\"select name from sqlite_master where type = 'table'\") return [r[0] for r in res.fetchall()] db_tables() ['targets', 'predictions', 'outcomes', 'tmp'] def db_tables(): query = \"select name from sqlite_master where type = 'table'\" return pd.read_sql(query, conn).name.values db_tables() array(['targets', 'predictions', 'outcomes', 'tmp'], dtype=object) def add_table(table, table_name): \"\"\"Add table to database.\"\"\" if table_name not in db_tables(): table.to_sql(table_name, conn, index=False) ","date":"2020-09-12","objectID":"/sql-in-python/:1:3","tags":["tools"],"title":"SQL in Python","uri":"/sql-in-python/"},{"categories":null,"content":"Add columns conn.execute(\"select name from sqlite_master\").fetchall() [('targets',), ('predictions',), ('outcomes',), ('tmp',)] def tab_cols(table, conn): query = \"select name from pragma_table_info(?)\" return pd.read_sql(query, conn, params=(table,)).name.values tab_cols(\"outcomes\", conn) array(['user_id', 'spendmax', 'spendmin', 'spendmean'], dtype=object) def tab_cols(table): \"\"\"List table columns.\"\"\" res = c.execute(\"select name from pragma_table_info(?)\", (table,)) return [n[0] for n in res.fetchall()] tab_cols(\"outcomes\") ['user_id', 'spendmax', 'spendmin', 'spendmean'] def add_column(df, table): \"\"\"Add column to table. Input: pd.DataFrame with columns ['user_id', 'col_name']. \"\"\" col_name = df.columns[1] if col_name not in tab_cols(table): df.to_sql(\"tmp\", conn, index=False) conn.executescript( f\"\"\" alter table {table} add column {col_name}; update {table} set {col_name} = ( select {col_name} from tmp where {table}.user_id = tmp.user_id); drop table tmp; \"\"\" ) def add_table(table): \"\"\"Add table to database.\"\"\" def spendmax(df): return ( df.groupby(\"user_id\") .apply(lambda u: u[u.amount \u003e 0].amount.max()) .rename(\"spendmax\") .reset_index() ) def spendmin(df): return ( df.groupby(\"user_id\") .apply(lambda u: u[u.amount \u003e 0].amount.min()) .rename(\"spendmin\") .reset_index() ) def spendmean(df): return ( df.groupby(\"user_id\") .apply(lambda u: u[u.amount \u003e 0].amount.mean()) .rename(\"spendmean\") .reset_index() ) outcomes = [spendmax, spendmin, spendmean] for outcome in outcomes: add_column(outcome(df), \"outcomes\") display(pd.read_sql(\"select * from outcomes\", conn)) user_id spendmax spendmin spendmean 0 60777 47885.449219 0.02 92.656956 1 64777 10000.000000 0.01 51.317209 2 777 4898.879883 0.01 54.275365 3 7777 22300.000000 0.07 111.685793 4 71777 4000.000000 0.01 146.197356 ... ... ... ... ... 179 299777 20265.000000 0.10 79.127393 180 8777 14998.000000 0.01 378.858950 181 80777 17633.800781 0.03 130.221119 182 83777 35000.000000 0.01 86.623218 183 86777 8000.000000 0.04 93.136604 184 rows √ó 4 columns user_id spendmax spendmin spendmean 0 60777 47885.449219 0.02 92.656956 1 64777 10000.000000 0.01 51.317209 2 777 4898.879883 0.01 54.275365 3 7777 22300.000000 0.07 111.685793 4 71777 4000.000000 0.01 146.197356 ... ... ... ... ... 179 299777 20265.000000 0.10 79.127393 180 8777 14998.000000 0.01 378.858950 181 80777 17633.800781 0.03 130.221119 182 83777 35000.000000 0.01 86.623218 183 86777 8000.000000 0.04 93.136604 184 rows √ó 4 columns user_id spendmax spendmin spendmean 0 60777 47885.449219 0.02 92.656956 1 64777 10000.000000 0.01 51.317209 2 777 4898.879883 0.01 54.275365 3 7777 22300.000000 0.07 111.685793 4 71777 4000.000000 0.01 146.197356 ... ... ... ... ... 179 299777 20265.000000 0.10 79.127393 180 8777 14998.000000 0.01 378.858950 181 80777 17633.800781 0.03 130.221119 182 83777 35000.000000 0.01 86.623218 183 86777 8000.000000 0.04 93.136604 184 rows √ó 4 columns ","date":"2020-09-12","objectID":"/sql-in-python/:1:4","tags":["tools"],"title":"SQL in Python","uri":"/sql-in-python/"},{"categories":null,"content":"Connection info PRAGMA is your friend for this and many other pieces of metadata. List databases attached to the current connection pd.read_sql_query(\"select * from pragma_database_list\", conn) seq name file 0 0 main /Users/fgu/Library/Mobile Documents/com~apple~... ","date":"2020-09-12","objectID":"/sql-in-python/:1:5","tags":["tools"],"title":"SQL in Python","uri":"/sql-in-python/"},{"categories":null,"content":"Database info List tables attached to a database pd.read_sql_query( \"\"\" select * from sqlite_master where type = 'table' and name not like 'sqlite_%' \"\"\", conn, ) type name tbl_name rootpage sql 0 table targets targets 2 CREATE TABLE targets(\\n user_id int... 1 table predict predict 3 CREATE TABLE predict(\\n user_id int... 2 table outcomes outcomes 4 CREATE TABLE outcomes(\\n user_id in... List indices attached to database pd.read_sql_query( \"\"\" select * from sqlite_master where type = 'index' \"\"\", conn, ) type name tbl_name rootpage sql ","date":"2020-09-12","objectID":"/sql-in-python/:1:6","tags":["tools"],"title":"SQL in Python","uri":"/sql-in-python/"},{"categories":null,"content":"Table info List columns attached to a table table = (\"targets\",) pd.read_sql_query(\"select * from pragma_table_info(?)\", conn, params=table) cid name type notnull dflt_value pk 0 0 user_id integer 0 None 1 Get structure of a table print(c.execute(\"select sql from sqlite_master where name = ?\", table).fetchall()[0][0]) CREATE TABLE targets( user_id integer primary key ) Indices associated with a table c.execute(\"pragma index_list('targets')\").fetchall() [] ","date":"2020-09-12","objectID":"/sql-in-python/:1:7","tags":["tools"],"title":"SQL in Python","uri":"/sql-in-python/"},{"categories":null,"content":"Misc usefuls stuff Move content of one table to another table c.execute(\"\"\"insert into new_table select * from old_table\"\"\") Vacuum database regularly after altering tables or columns to free up overhead and reduce disk space. c.execute(\"vacuum;\") \u003csqlite3.Cursor at 0x11c74a650\u003e ","date":"2020-09-12","objectID":"/sql-in-python/:2:0","tags":["tools"],"title":"SQL in Python","uri":"/sql-in-python/"},{"categories":null,"content":"Enable foreign key constraints c.execute(\"select * from pragma_foreign_keys\").fetchall() [(0,)] c.execute(\"pragma foreign_keys=on\") c.execute(\"select * from pragma_foreign_keys\").fetchall() [(1,)] c.execute(\"pragma foreign_keys=off\") c.execute(\"select * from pragma_foreign_keys\").fetchall() [(0,)] ","date":"2020-09-12","objectID":"/sql-in-python/:2:1","tags":["tools"],"title":"SQL in Python","uri":"/sql-in-python/"},{"categories":null,"content":"Use namedtuple() from collections import namedtuple TableInfo = namedtuple(\"TableInfo\", \"cid, name, type, notnull, dflt_value, pk\") def tab_cols(table): \"\"\"List table columns.\"\"\" raw_cols = c.execute(\"select * from pragma_table_info(?)\", (table,)).fetchall() named_cols = map(TableInfo._make, raw_cols) return [(c.name, c.type, c.pk) for c in named_cols] tab_cols(\"outcomes\") [('user_id', 'integer', 1)] The above deliberately overuses namedtuple for practice. ","date":"2020-09-12","objectID":"/sql-in-python/:2:2","tags":["tools"],"title":"SQL in Python","uri":"/sql-in-python/"},{"categories":null,"content":"Use namedtuple This is useful, and we can guess that the second element in each tuple is the column name. But it would be nice to know what the remaining information is, and then to be able to refer to different pieces of information by their name. To find out what each piece of information is, we can either check out the PRAGMA docs or, what I find even more useful, can use Pandas like so: (ideally, there would be a way to retrieve column names directly from the query, but I haven‚Äôt been able to find any way to do so.) import pandas as pd tabinf = pd.read_sql_query(\"select * from pragma_table_info('stocks')\", conn) tabinf cid name type notnull dflt_value pk 0 0 name text 0 None 0 1 1 quantity real 0 None 0 2 2 price real 0 None 0 To label the pieces in the table_cols function, we can store the column names and create a namedtuple() (docs). from collections import namedtuple TableInfo = namedtuple(\"TableInfo\", \"cid, name, type, notnull, dflt_value, pk\") Mapping each tuple in the list that table_info() returns to our named tuple, we get the following: for a in map(TableInfo._make, table_cols(table)): print(a) TableInfo(cid=0, name='name', type='text', notnull=0, dflt_value=None, pk=0) TableInfo(cid=1, name='quantity', type='real', notnull=0, dflt_value=None, pk=0) TableInfo(cid=2, name='price', type='real', notnull=0, dflt_value=None, pk=0) Or, what we really want: for a in map(TableInfo._make, table_cols(table)): print(a.name) name quantity price We can now update our table_cols function. ","date":"2020-09-12","objectID":"/sql-in-python/:2:3","tags":["tools"],"title":"SQL in Python","uri":"/sql-in-python/"},{"categories":null,"content":"Adding a table with a row of data # Create cursor c = conn.cursor() # Create table c.execute( \"\"\"CREATE TABLE stocks (date text, trans text, symbol text, qty real, price real)\"\"\" ) # Insert a row of data c.execute(\"INSERT INTO stocks VALUES ('2006-01-05','BUY','RHAT',100,35.14)\") # Save (commit) changes conn.commit() # Close connection conn.close() To check that the database now contains our stocks table, list all its tables. conn = sqlite3.connect(db_path) c = conn.cursor() c.execute(\"select name from sqlite_master where type = 'table'\").fetchall() [('stocks',)] ","date":"2020-09-12","objectID":"/sql-in-python/:2:4","tags":["tools"],"title":"SQL in Python","uri":"/sql-in-python/"},{"categories":null,"content":"Retrieving data conn = sqlite3.connect(db_path) c = conn.cursor() c.execute(\"SELECT * FROM stocks\").fetchall() [('2006-01-05', 'BUY', 'RHAT', 100.0, 35.14)] When adding Python variables to the query, never use string substitution directly like so: symbol = \"RHAT\" c.execute(f\"select * from stocks where symbol = '{symbol}'\").fetchall() [('2006-01-05', 'BUY', 'RHAT', 100.0, 35.14)] While this works, it‚Äôs vulnerable to injection attacks. Use parameter substition instead. Either using question marks like so symbol = (\"RHAT\",) c.execute(\"select * from stocks where symbol = ?\", symbol).fetchall() [('2006-01-05', 'BUY', 'RHAT', 100.0, 35.14)] or using named placedholders like so c.execute(\"select * from stocks where symbol = :symbol\", {\"symbol\": \"RHAT\"}).fetchall() [('2006-01-05', 'BUY', 'RHAT', 100.0, 35.14)] ","date":"2020-09-12","objectID":"/sql-in-python/:2:5","tags":["tools"],"title":"SQL in Python","uri":"/sql-in-python/"},{"categories":null,"content":"Why do I need fetchall() after cursor.execute()? Because the curse.execute() returns an iterater object containing all query results. ","date":"2020-09-12","objectID":"/sql-in-python/:2:6","tags":["tools"],"title":"SQL in Python","uri":"/sql-in-python/"},{"categories":null,"content":"Using namedtuples EmployeeRecord = namedtuple(\"EmployeeRecord\", \"name, age, title, department, paygrade\") import csv for emp in map(EmployeeRecord._make, csv.reader(open(\"employees.csv\", \"rb\"))): print emp.name, emp.title import sqlite3 conn = sqlite3.connect(\"/companydata\") cursor = conn.cursor() cursor.execute(\"SELECT name, age, title, department, paygrade FROM employees\") for emp in map(EmployeeRecord._make, cursor.fetchall()): print emp.name, emp.title ","date":"2020-09-12","objectID":"/sql-in-python/:2:7","tags":["tools"],"title":"SQL in Python","uri":"/sql-in-python/"},{"categories":null,"content":"Using Pandas Pandas is a very handy way to interact with databased in Python, as it makes dumping and retrieving dataframes very easy. import pandas as pd pd.read_sql_query(\"SELECT * FROM stocks\", conn) date trans symbol qty price newcol 0 2006-01-05 BUY RHAT 100.0 35.14 None ","date":"2020-09-12","objectID":"/sql-in-python/:3:0","tags":["tools"],"title":"SQL in Python","uri":"/sql-in-python/"},{"categories":null,"content":"SQLAlchemy Summary of this video. ","date":"2020-09-12","objectID":"/sql-in-python/:4:0","tags":["tools"],"title":"SQL in Python","uri":"/sql-in-python/"},{"categories":null,"content":"SQL best practices Avoid * in queries to have full control of returned columns (e.g.¬†in case where table changes). ","date":"2020-09-12","objectID":"/sql-in-python/:5:0","tags":["tools"],"title":"SQL in Python","uri":"/sql-in-python/"},{"categories":null,"content":"Sources SQLite docs sqite3 docs sqlite tutorial ","date":"2020-09-12","objectID":"/sql-in-python/:6:0","tags":["tools"],"title":"SQL in Python","uri":"/sql-in-python/"},{"categories":null,"content":"=== Old notes to integrate === # Selecting rows from a table SELECT col FROM table; SELECT col1, col2 FROM table; SELECT * FROM table LIMIT 10; SELECT DISTINCT col_values FROM table; # Counting SELECT COUNT(*) FROM table; # Count rows of table SELECT COUNT(col) FROM table; # Count non-missing values in col SELECT COUNT(DISTINCT col) FROM table; # Count distinct values in col # Filtering SELECT * FROM table WHERE col1 \u003e 2010: # Get rows for which col1 \u003e 2010 SELECT COUNT(*) FROM table WHERE x \u003c y # Count number of rows for which x \u003c y SELECT * FROM table WHERE x \u003e Y AND y \u003c z SELECT * FROM table WHERE x \u003e Y OR y \u003c z SELECT * FROM table WHERE x BETWEEN a AND b # between a and b inclusive SELECT * FROM table WHERE x IN (a, b, c) SELECT title FROM films WHERE (release_year = 1994 OR release_year = 1995) AND (certification = 'PG' OR certification = 'R'); # Filter based on results from aggregate function SELECT release_year FROM films GROUP BY release_year HAVING COUNT(title) \u003e 10; # Missing values SELECT COUNT(*) FROM people WHERE birthdate IS NULL; SELECT name FROM people WHERE birthdate IS NOT NULL; # Wildcards SELECT name FROM companies WHERE name LIKE 'Data%'; # % matches zero, one, or many characters SELECT name FROM companies WHERE name LIKE 'DataC_mp'; # _ matches exactly one character SELECT name FROM people WHERE name NOT LIKE 'A%'; # Aggregate functions SELECT AVG(budget) # Also MAX, MIN, SUM, FROM films; # Aliasing SELECT MAX(budget) AS max_budget, MAX(duration) AS max_duration FROM films; # Arithmetic SELECT COUNT(deathdate) * 100.0 / COUNT(*) AS percentage_dead FROM people # Order by SELECT title FROM films ORDER BY release_year; SELECT title FROM films ORDER BY release_year DESC; # Group by SELECT sex, count(*) FROM employees GROUP BY sex; SELECT release_year, MAX(budget) FROM films GROUP BY release_year; # Building a database ####################### # Create tables CREATE TABLE professors ( firstname text, lastname text ); # Alter tables ALTER TABLE table_name ADD COLUMN column_name data_type; ALTER TABLE table_name DROP COLUMN column_name; ALTER TABLE table_name RENAME COLUMN old_name TO new_name; DROP TABLE table_name # Insert values INSERT INTO transactions (transaction_date, amount, fee) VALUES ('2018-09-24', 5454, '30'); SELECT transaction_date, amount + CAST(fee AS integer) AS net_amount FROM transactions; # Migrating data INSERT INTO target_table SELECT DISTINCT column_names FROM source_table; # Integrity constraints # 1. Attribute constraints (data types) # 2. Key constraints (primary keys) # 3. Referential integrity constraints (enforced through foreign keys) # Attribute constraints ALTER TABLE professors ALTER COLUMN firstname TYPE varchar(16) USING SUBSTRING(firstname FROM 1 FOR 16) ALTER TABLE professors ALTER COLUMN firstname SET NOT NULL; ALTER TABLE universities ADD CONSTRAINT university_shortname_unq UNIQUE(university_shortname); # Key constraints # Superkey: each combination of attributes that identifies rows uniquely # Candidate key: a superkey from which no column can be removed # Primary key: one candidate key chosen to act as primary key # Surrogate key: artificially created key (eg due to unsuitable candidate keys) # Foreign keys: points to the primary key of another table ALTER TABLE organizations RENAME COLUMN organization TO id; ALTER TABLE organizations ADD CONSTRAINT organization_pk PRIMARY KEY (id); ALTER TABLE affiliations DROP CONSTRAINT affiliations_organizations_id_fkey; ALTER TABLE professors ADD COLUMN ID serial UPDATE table_name SET new_var = CONCAT(v1, v2); -- Add a professor_id column that references id in professors table ALTER TABLE affiliations ADD COLUMN professor_id integer REFERENCES professors (id); -- Rename the organization column to organization_id ALTER TABLE affiliations RENAME organization TO organization_id; -- Add a foreign key on organization_id ALTER TABLE affiliations ADD CONSTRAINT affiliations_organization_fkey FOREIGN KEY (organization_id) REFERENCES organizat","date":"2020-09-12","objectID":"/sql-in-python/:7:0","tags":["tools"],"title":"SQL in Python","uri":"/sql-in-python/"},{"categories":null,"content":" import numpy as np import pandas as pd ","date":"2020-08-12","objectID":"/documenting-sample-selection/:0:0","tags":["python, datascience"],"title":"Documenting Sample Selection","uri":"/documenting-sample-selection/"},{"categories":null,"content":"Problem I have a dataframe on which I perform a series of data selection steps. What I want is to automatically build a table for the appendix of my paper that tells me the number of users left in the data after each selection step. Here‚Äôs a mock dataset: df = (pd.DataFrame({'user_id': [1, 2, 3, 4] * 2, 'data': np.random.rand(8)}) .sort_values('user_id')) df user_id data 0 1 0.107515 4 1 0.306182 1 2 0.184724 5 2 0.217231 2 3 0.688004 6 3 0.284524 3 4 0.990159 7 4 0.466758 here some selection functions: def first_five(df): return df[:5] def n_largest(df, n=3): return df.loc[df.data.nlargest(n).index] def select_sample(df): return ( df .pipe(first_five) .pipe(n_largest) ) select_sample(df) user_id data 2 3 0.688004 4 1 0.306182 5 2 0.217231 ","date":"2020-08-12","objectID":"/documenting-sample-selection/:1:0","tags":["python, datascience"],"title":"Documenting Sample Selection","uri":"/documenting-sample-selection/"},{"categories":null,"content":"Solution If we have a single dataframe on which to perform selection, as in the setting above, we can use a decorator and a dictionary. As a first step, let‚Äôs build a decorator that prints out the number of users after applying each function: from functools import wraps def user_counter(func): @wraps(func) def wrapper(*args, **kwargs): df = func(*args, **kwargs) num_users = df.user_id.nunique() print(f'{func.__name__}: {num_users}') return df return wrapper @user_counter def first_five(df): return df[:5] @user_counter def n_largest(df, n=3): return df.loc[df.data.nlargest(n).index] def select_sample(df): return ( df .pipe(first_five) .pipe(n_largest) ) select_sample(df) first_five: 3 n_largest: 3 user_id data 2 3 0.688004 4 1 0.306182 5 2 0.217231 That‚Äôs already nice. But I need those counts for the data appendix of my paper, so what I really want is to store the counts in a container that I can turn into a table. To do this, we can store the counts in a dictionary instead of printing them. counts = dict() def user_counter(func): @wraps(func) def wrapper(*args, **kwargs): df = func(*args, **kwargs) num_users = df.user_id.nunique() counts.update({func.__name__: num_users}) return df return wrapper @user_counter def first_five(df): return df[:5] @user_counter def n_largest(df, n=3): return df.loc[df.data.nlargest(n).index] def select_sample(df): return ( df .pipe(first_five) .pipe(n_largest) ) display(select_sample(df)) counts user_id data 2 3 0.688004 4 1 0.306182 5 2 0.217231 {'first_five': 3, 'n_largest': 3} Next, I want to add the number of users at the beginning and the end of the process (the count at the end is identical with the final step, but I think it‚Äôs worth adding so readers can easily spot the final numbers). counts = dict() def user_counter(func): @wraps(func) def wrapper(*args, **kwargs): df = func(*args, **kwargs) num_users = df.user_id.nunique() counts.update({func.__name__: num_users}) return df return wrapper @user_counter def first_five(df): return df[:5] @user_counter def n_largest(df, n=3): return df.loc[df.data.nlargest(n).index] def add_user_count(df, step): num_users = df.user_id.nunique() counts.update({step: num_users}) return df def select_sample(df): return ( df .pipe(add_user_count, 'start') .pipe(first_five) .pipe(n_largest) .pipe(add_user_count, 'end') ) display(select_sample(df)) counts user_id data 2 3 0.688004 4 1 0.306182 5 2 0.217231 {'start': 4, 'first_five': 3, 'n_largest': 3, 'end': 3} We‚Äôre nearly there. Let‚Äôs turn this into a table that we can store to disk (as a Latex table, say) and automatically import in our paper. table = pd.DataFrame(counts.items(), columns=['Processing step', 'Number of unique users']) table Processing step Number of unique users 0 start 4 1 first_five 3 2 n_largest 3 3 end 3 Finally, let‚Äôs make sure readers of our paper (and we ourselves a few weeks from now) actually understand what‚Äôs going on at each step. description = { 'start': 'Raw dataset', 'first_five': 'Keep first five observations', 'n_largest': 'Keep three largest datapoints', 'end': 'Final dataset' } table['Processing step'] = table['Processing step'].map(description) table Processing step Number of unique users 0 Raw dataset 4 1 Keep first five observations 3 2 Keep three largest datapoints 3 3 Final dataset 3 That‚Äôs it. We can can now export this as a Latex table (or some other format) and automatically load it in our paper. ","date":"2020-08-12","objectID":"/documenting-sample-selection/:2:0","tags":["python, datascience"],"title":"Documenting Sample Selection","uri":"/documenting-sample-selection/"},{"categories":null,"content":"Multiple datasets Instead of having a single dataframe on which to perform selection, I actually have multiple pieces of a large dataframe (because the full dataframe doesn‚Äôt fit into memory). What I want is to perform the data selection on each chunk separately but have the values in the counter object add up so that ‚Äì at the end ‚Äì the counts represent the counts for the full dataset. The solution here is to use collection.Counter() instead of a dictionary. So, my setup is akin to the following: large_df = pd.DataFrame({'user_id': list(range(12)), 'data': np.random.rand(12)}) large_df user_id data 0 0 0.507218 1 1 0.933454 2 2 0.740951 3 3 0.654135 4 4 0.952187 5 5 0.807332 6 6 0.742915 7 7 0.344259 8 8 0.134813 9 9 0.952129 10 10 0.859282 11 11 0.376175 buckets = pd.cut(large_df.user_id, bins=2) raw_pieces = [data for key, data in large_df.groupby(buckets)] for piece in raw_pieces: display(piece) user_id data 0 0 0.507218 1 1 0.933454 2 2 0.740951 3 3 0.654135 4 4 0.952187 5 5 0.807332 user_id data 6 6 0.742915 7 7 0.344259 8 8 0.134813 9 9 0.952129 10 10 0.859282 11 11 0.376175 What happens if we use a dict() as our counts object as we did above. counts = dict() def user_counter(func): @wraps(func) def wrapper(*args, **kwargs): df = func(*args, **kwargs) num_users = df.user_id.nunique() counts.update({func.__name__: num_users}) return df return wrapper @user_counter def first_five(df): return df[:5] @user_counter def n_largest(df, n=3): return df.loc[df.data.nlargest(n).index] def add_user_count(df, step='start'): num_users = df.user_id.nunique() counts.update({step: num_users}) return df def select_sample(df): return ( df .pipe(add_user_count) .pipe(first_five) .pipe(n_largest) .pipe(add_user_count, 'end') ) selected_pieces = [] for piece in raw_pieces: selected_pieces.append(select_sample(piece)) print(counts) df = pd.concat(selected_pieces) {'start': 6, 'first_five': 5, 'n_largest': 3, 'end': 3} {'start': 6, 'first_five': 5, 'n_largest': 3, 'end': 3} The counts are replaced rather than added up, which is how updating works for a dictionary: m = dict(a=1, b=2) n = dict(b=3, c=4) m.update(n) m {'a': 1, 'b': 3, 'c': 4} collections.Counter() (docs) solve this problem. import collections counts = collections.Counter() def user_counter(func): @wraps(func) def wrapper(*args, **kwargs): df = func(*args, **kwargs) num_users = df.user_id.nunique() counts.update({func.__name__: num_users}) return df return wrapper @user_counter def first_five(df): return df[:5] @user_counter def n_largest(df, n=3): return df.loc[df.data.nlargest(n).index] def add_user_count(df, step='start'): num_users = df.user_id.nunique() counts.update({step: num_users}) return df def select_sample(df): return ( df .pipe(add_user_count) .pipe(first_five) .pipe(n_largest) .pipe(add_user_count, 'end') ) selected_pieces = [] for piece in raw_pieces: selected_pieces.append(select_sample(piece)) print(counts) df = pd.concat(selected_pieces) Counter({'start': 6, 'first_five': 5, 'n_largest': 3, 'end': 3}) Counter({'start': 12, 'first_five': 10, 'n_largest': 6, 'end': 6}) Now, updating adds up the values for each key, just as we want. We can add the same formatting as we did above and are done with our table. ","date":"2020-08-12","objectID":"/documenting-sample-selection/:2:1","tags":["python, datascience"],"title":"Documenting Sample Selection","uri":"/documenting-sample-selection/"},{"categories":null,"content":"Background ","date":"2020-08-12","objectID":"/documenting-sample-selection/:3:0","tags":["python, datascience"],"title":"Documenting Sample Selection","uri":"/documenting-sample-selection/"},{"categories":null,"content":"Other cool stuff Counter() can do o = collections.Counter(a=1, b=2) p = collections.Counter(b=3, c=-4) o.update(p) o Counter({'a': 1, 'b': 5, 'c': -4}) Counters can also do cool things like this: list(o.elements()) ['a', 'b', 'b', 'b', 'b', 'b'] o.most_common(2) [('b', 5), ('a', 1)] o - p Counter({'a': 1, 'b': 2}) ","date":"2020-08-12","objectID":"/documenting-sample-selection/:3:1","tags":["python, datascience"],"title":"Documenting Sample Selection","uri":"/documenting-sample-selection/"},{"categories":null,"content":"Why is counts a global variable? Because I want all decorated functions to write to the same counter object. Often, decorators make use of closures instead, which have access to a nonlocal variable defined inside the outermost function. Let‚Äôs look at what happens if we do this for our user counter. def user_counter(func): counts = collections.Counter() @wraps(func) def wrapper(*args, **kwargs): df = func(*args, **kwargs) num_users = df.user_id.nunique() counts.update({func.__name__: num_users}) print(counts) return df return wrapper @user_counter def first_five(df): return df[:5] @user_counter def largest(df): return df.loc[df.data.nlargest(3).index] def select(df): return ( df .pipe(first_five) .pipe(largest) ) result = select(df) Counter({'first_five': 5}) Counter({'largest': 3}) Now, each decorated function gets its own counter object, which is not what we want here. For more on decorator state retention options, see chapter 39 in Learning Python. ","date":"2020-08-12","objectID":"/documenting-sample-selection/:3:2","tags":["python, datascience"],"title":"Documenting Sample Selection","uri":"/documenting-sample-selection/"},{"categories":null,"content":"What are closures and nonlocal variables? (Disclaimer: Just about all of the text and code on closures is taken ‚Äì sometimes verbatim ‚Äì from chapter 7 in Fluent Python. So the point here is not to produce new insight, but to absorb the material and write an easily accessible note to my future self.) Closures are functions that have access to nonlocal arguments ‚Äì variabls that are neither local nor global, but are defined inside an outer function within which the closure was defined, and to which the closure has access. Let‚Äôs look at an example. A simple function that takes one number as an argument and returns the average of all numbers passed to it since it‚Äôs definition. For this, we need a way to store all previously passed values. One way to do this is to define a class with a call method. class Averager(): def __init__(self): self.series = [] def __call__(self, new_value): self.series.append(new_value) total = sum(self.series) return total / len(self.series) avg = Averager() avg(10), avg(20), avg(30) (10.0, 15.0, 20.0) Another way is to use a closure function and store the series of previously passed numbers as a free variable. def make_averager(): series = [] def averager(new_value): series.append(new_value) total = sum(series) return total / len(series) return averager avg = make_averager() avg(10), avg(20), avg(30) (10.0, 15.0, 20.0) This gives the same result, but is arguably simpler than defining a class. We can improve the above function by storing previous results so that we don‚Äôt have to calculate the new average from scratch at every function call. def make_fast_averager(): count = 0 total = 0 def averager(new_value): nonlocal count, total count += 1 total += new_value return total / count return averager avg = make_fast_averager() avg(10), avg(11), avg(12) (10.0, 10.5, 11.0) %%timeit avg = make_averager() [avg(n) for n in range(10_000)] 233 ms ¬± 8.03 ms per loop (mean ¬± std. dev. of 7 runs, 1 loop each) %%timeit avg = make_fast_averager() [avg(n) for n in range(10_000)] 1.69 ms ¬± 53.5 ¬µs per loop (mean ¬± std. dev. of 7 runs, 1000 loops each) This simple change gives us a massive speedup. Notice the nonlocal statement inside the averager function. Why do we need this? Let‚Äôs see what happens if we don‚Äôt specify it: def make_fast_averager(): count = 0 total = 0 def averager(new_value): count += 1 total += new_value return total / count return averager avg = make_fast_averager() avg(10), avg(11), avg(12) UnboundLocalError: local variable 'count' referenced before assignment How come our fast averager can‚Äôt find count and total even though our slow averager could find series just fine? The answer lies in Python‚Äôs variable scope rules and the difference between assigning to unmutable objects and updating mutable ones. Whenever we assign to a variable inside a function, it is treated as a local variable. count += 1 is the same as count = count + 1, so we are assigning to count, which makes it a local variable (the same goes for total). We are assigning new values to count rather than updaing it because integers are immutable, so we can‚Äôt update it. Lists are mutable, so series.append() doesn‚Äôt create a new list, but merely appends to it, which doesn‚Äôt count as an assignment, so that series is not treated as a local variable. Hence, we need to explicitly tell Python that count and total are nonlocal variables. ","date":"2020-08-12","objectID":"/documenting-sample-selection/:3:3","tags":["python, datascience"],"title":"Documenting Sample Selection","uri":"/documenting-sample-selection/"},{"categories":null,"content":"Main sources Fluent Python Python Cookbook Learning Python ","date":"2020-08-12","objectID":"/documenting-sample-selection/:4:0","tags":["python, datascience"],"title":"Documenting Sample Selection","uri":"/documenting-sample-selection/"},{"categories":null,"content":" A collection of recipes for effectively solving common and not so common problems in Pandas. import numpy as np import pandas as pd import seaborn as sns ","date":"2020-08-09","objectID":"/pandas-cookbook/:0:0","tags":["python"],"title":"Pandas cookbook","uri":"/pandas-cookbook/"},{"categories":null,"content":"Splitting dataframes based on column values You want to split the dataframe every time case equals B and store the resulting dataframes in a list. df = pd.DataFrame( data={ \"case\": [\"A\", \"A\", \"A\", \"B\", \"A\", \"A\", \"B\", \"A\", \"A\"], \"data\": np.random.randn(9), } ) df case data 0 A -0.900566 1 A -1.127128 2 A 0.342823 3 B -0.854102 4 A 1.058840 5 A -0.307737 6 B 0.013741 7 A -1.768852 8 A 0.550569 ","date":"2020-08-09","objectID":"/pandas-cookbook/:1:0","tags":["python"],"title":"Pandas cookbook","uri":"/pandas-cookbook/"},{"categories":null,"content":"Understanding the cookbook solution From the cookbook: dfs = list( zip( *df.groupby( (1 * (df[\"case\"] == \"B\")).cumsum().rolling(window=3, min_periods=1).median() ) ) )[-1] dfs ( case data 0 A -0.900566 1 A -1.127128 2 A 0.342823 3 B -0.854102, case data 4 A 1.058840 5 A -0.307737 6 B 0.013741, case data 7 A -1.768852 8 A 0.550569) This works. But because it‚Äôs so heavily nested and uses methods like rolling() and median() not really designed for that purpose, the code is impossible to interpret at a glance. Let‚Äôs break this down into separate pieces. First, the code creates a grouping variable that changes its value each time case equaled B on the previous row. # Creating grouping variable a = df.case == \"B\" b = 1 * (df.case == \"B\") c = 1 * (df.case == \"B\").cumsum() d = 1 * (df.case == \"B\").cumsum().rolling(window=3, min_periods=1).median() a, b, c, d (0 False 1 False 2 False 3 True 4 False 5 False 6 True 7 False 8 False Name: case, dtype: bool, 0 0 1 0 2 0 3 1 4 0 5 0 6 1 7 0 8 0 Name: case, dtype: int64, 0 0 1 0 2 0 3 1 4 1 5 1 6 2 7 2 8 2 Name: case, dtype: int64, 0 0.0 1 0.0 2 0.0 3 0.0 4 1.0 5 1.0 6 1.0 7 2.0 8 2.0 Name: case, dtype: float64) Series d above is the argument passed to groupby() in the solution. This works, but is a very roundabout way to create such a series. I‚Äôll use a different approach below. Next, the code uses list(), zip(), and argument expansion to pack the data for each group into a single list of dataframes. Let‚Äôs look at these one by one. First, groupby() stores the grouped data as (label, df) tuples. groups = df.groupby(\"case\") for g in groups: print(type(g)) print(g, end=\"\\n\\n\") \u003cclass 'tuple'\u003e ('A', case data 0 A -0.900566 1 A -1.127128 2 A 0.342823 4 A 1.058840 5 A -0.307737 7 A -1.768852 8 A 0.550569) \u003cclass 'tuple'\u003e ('B', case data 3 B -0.854102 6 B 0.013741) Simplified, this is what we work with: groups2 = [(\"g1\", \"data1\"), (\"g2\", \"data2\")] groups2 [('g1', 'data1'), ('g2', 'data2')] Argument expansion unpacks elements from a list as separate arguments that can be passed to a function. In our context here, it turns each (label, df) tuple into a separate object. print(groups2) print(*groups2) [('g1', 'data1'), ('g2', 'data2')] ('g1', 'data1') ('g2', 'data2') zip() stitches together the ith elements of each iterable passed to it, effectively separating the ‚Äúcolumns‚Äù, and returns an iterator. zip(*groups2) \u003czip at 0x164d84200\u003e list(zip(*groups2)) [('g1', 'g2'), ('data1', 'data2')] Putting this all together, zip() is used to separate the group label from the data, and list() consumes the iterator created by zip and displays its content. list(zip(*groups)) [('A', 'B'), ( case data 0 A -0.900566 1 A -1.127128 2 A 0.342823 4 A 1.058840 5 A -0.307737 7 A -1.768852 8 A 0.550569, case data 3 B -0.854102 6 B 0.013741)] Because we only want the data, we select the last element from the list: list(zip(*groups))[-1] ( case data 0 A 0.684978 1 A 0.000269 2 A -1.040497 4 A 0.448596 5 A 0.222168 7 A -2.208787 8 A -0.440758, case data 3 B 0.451358 6 B 1.031011) Now we‚Äôre basically done. What remains is to use the list(zip(*groups)) procedure on the more complicated grouping variable, to obtain the original result. d = 1 * (df.case == \"B\").cumsum().rolling(window=3, min_periods=1).median() groups = df.groupby(d) list(zip(*groups))[-1] ( case data 0 A 0.684978 1 A 0.000269 2 A -1.040497 3 B 0.451358, case data 4 A 0.448596 5 A 0.222168 6 B 1.031011, case data 7 A -2.208787 8 A -0.440758) ","date":"2020-08-09","objectID":"/pandas-cookbook/:1:1","tags":["python"],"title":"Pandas cookbook","uri":"/pandas-cookbook/"},{"categories":null,"content":"Simplifying the code I think this can be made much more readable like so: df case data 0 A -0.900566 1 A -1.127128 2 A 0.342823 3 B -0.854102 4 A 1.058840 5 A -0.307737 6 B 0.013741 7 A -1.768852 8 A 0.550569 grouper = df.case.eq(\"B\").cumsum().shift().fillna(0) dfs = [df for (g, df) in df.groupby(grouper)] dfs [ case data 0 A -0.900566 1 A -1.127128 2 A 0.342823 3 B -0.854102, case data 4 A 1.058840 5 A -0.307737 6 B 0.013741, case data 7 A -1.768852 8 A 0.550569] Where the grouper works like so: dd = df.set_index(\"case\", drop=False) # Use case as index for clarity a = dd.case.eq(\"B\") # Boolean logic b = a.cumsum() # Create groups c = b.shift() # Shift so B included in previous group d = c.fillna(0) # Replace 0th element emptied by shift a, b, c, d (case A False A False A False B True A False A False B True A False A False Name: case, dtype: bool, case A 0 A 0 A 0 B 1 A 1 A 1 B 2 A 2 A 2 Name: case, dtype: int64, case A NaN A 0.0 A 0.0 B 0.0 A 1.0 A 1.0 B 1.0 A 2.0 A 2.0 Name: case, dtype: float64, case A 0.0 A 0.0 A 0.0 B 0.0 A 1.0 A 1.0 B 1.0 A 2.0 A 2.0 Name: case, dtype: float64) ","date":"2020-08-09","objectID":"/pandas-cookbook/:1:2","tags":["python"],"title":"Pandas cookbook","uri":"/pandas-cookbook/"},{"categories":null,"content":"Creating new columns based on existing ones using mappings The below is a straightforward adaptation from the cookbook: df = pd.DataFrame({\"AAA\": [1, 2, 1, 3], \"BBB\": [1, 1, 4, 2], \"CCC\": [2, 1, 3, 1]}) source_cols = [\"AAA\", \"BBB\"] new_cols = [str(c) + \"_cat\" for c in source_cols] cats = {1: \"One\", 2: \"Two\", 3: \"Three\"} dd = df.copy() dd[new_cols] = df[source_cols].applymap(cats.get) dd AAA BBB CCC AAA_cat BBB_cat 0 1 1 2 One One 1 2 1 1 Two One 2 1 4 3 One None 3 3 2 1 Three Two But it made me wonder why applymap required the use of the get method while we can map values of a series like so: s = pd.Series([1, 2, 3, 1]) s.map(cats) 0 One 1 Two 2 Three 3 One dtype: object or so s.map(cats.get) 0 One 1 Two 2 Three 3 One dtype: object The answer is simple: applymap requires a function as argument, while map takes functions or mappings. One limitation of the cookbook solution above is that is doesn‚Äôt seem to allow for default values (notice that 4 gets substituted with ‚ÄúNone‚Äù). One way around this is the following: df[new_cols] = df[source_cols].applymap(lambda x: cats.get(x, \"Hello\")) df AAA BBB CCC AAA_cat BBB_cat 0 1 1 2 One One 1 2 1 1 Two One 2 1 4 3 One Hello 3 3 2 1 Three Two ","date":"2020-08-09","objectID":"/pandas-cookbook/:2:0","tags":["python"],"title":"Pandas cookbook","uri":"/pandas-cookbook/"},{"categories":null,"content":"Creating dummy variables A reminder to my future self, based on this great video from Data School. df = pd.DataFrame( { \"id\": [1, 2, 3, 4, 5], \"quality\": [\"good\", \"excellent\", \"very good\", \"excellent\", \"good\"], } ) df.head() id quality 0 1 good 1 2 excellent 2 3 very good 3 4 excellent 4 5 good Pandas makes creating dummies easy: pd.get_dummies(df.quality) excellent good very good 0 0 1 0 1 1 0 0 2 0 0 1 3 1 0 0 4 0 1 0 If you want to label the source of the data, you can use the prefix argument: pd.get_dummies(df.quality, prefix=\"quality\") quality_excellent quality_good quality_very good 0 0 1 0 1 1 0 0 2 0 0 1 3 1 0 0 4 0 1 0 Often when we work with dummies from a variable with $n$ distinct values, we create $n-1$ dummies and treat the remaining group as the reference group. Pandas provides a convenient way to do this: pd.get_dummies(df.quality, prefix=\"quality\", drop_first=True) quality_good quality_very good 0 1 0 1 0 0 2 0 1 3 0 0 4 1 0 Usually, we‚Äôll want to use the dummies with the rest of the data, so it‚Äôs conveninet to have them in the original dataframe. One way to do this is to use concat like so: dummies = pd.get_dummies(df.quality, prefix=\"quality\", drop_first=True) df_with_dummies = pd.concat([df, dummies], axis=1) df_with_dummies.head() id quality quality_good quality_very good 0 1 good 1 0 1 2 excellent 0 0 2 3 very good 0 1 3 4 excellent 0 0 4 5 good 1 0 This works. But Pandas provides a much easier way: df_with_dummies1 = pd.get_dummies(df, columns=[\"quality\"], drop_first=True) df_with_dummies1 id quality_good quality_very good 0 1 1 0 1 2 0 0 2 3 0 1 3 4 0 0 4 5 1 0 That‚Äôs it. In one line we get a new dataframe that includes the dummies and excludes the original quality column. ","date":"2020-08-09","objectID":"/pandas-cookbook/:3:0","tags":["python"],"title":"Pandas cookbook","uri":"/pandas-cookbook/"},{"categories":null,"content":"Pivot tables df = sns.load_dataset(\"planets\") print(df.shape) df.head(3) (1035, 6) method number orbital_period mass distance year 0 Radial Velocity 1 269.300 7.10 77.40 2006 1 Radial Velocity 1 874.774 2.21 56.95 2008 2 Radial Velocity 1 763.000 2.60 19.84 2011 Create a table that shows the number of planets discovered by each method in each decade # method 1: groupby decade = df.year // 10 * 10 decade = decade.astype(\"str\") + \"s\" decade.name = \"decade\" df.groupby([\"method\", decade]).number.sum().unstack().fillna(0).astype(\"int\") decade 1980s 1990s 2000s 2010s method Astrometry 0 0 0 2 Eclipse Timing Variations 0 0 5 10 Imaging 0 0 29 21 Microlensing 0 0 12 15 Orbital Brightness Modulation 0 0 0 5 Pulsar Timing 0 9 1 1 Pulsation Timing Variations 0 0 1 0 Radial Velocity 1 52 475 424 Transit 0 0 64 712 Transit Timing Variations 0 0 0 9 # method 2: pivot table df.pivot_table( values=\"number\", index=\"method\", columns=decade, aggfunc=\"sum\", fill_value=0 ) decade 1980s 1990s 2000s 2010s method Astrometry 0 0 0 2 Eclipse Timing Variations 0 0 5 10 Imaging 0 0 29 21 Microlensing 0 0 12 15 Orbital Brightness Modulation 0 0 0 5 Pulsar Timing 0 9 1 1 Pulsation Timing Variations 0 0 1 0 Radial Velocity 1 52 475 424 Transit 0 0 64 712 Transit Timing Variations 0 0 0 9 ","date":"2020-08-09","objectID":"/pandas-cookbook/:4:0","tags":["python"],"title":"Pandas cookbook","uri":"/pandas-cookbook/"},{"categories":null,"content":"Another example From here df = pd.DataFrame( data={ \"province\": [\"ON\", \"QC\", \"BC\", \"AL\", \"AL\", \"MN\", \"ON\"], \"city\": [ \"Toronto\", \"Montreal\", \"Vancouver\", \"Calgary\", \"Edmonton\", \"Winnipeg\", \"Windsor\", ], \"sales\": [13, 6, 16, 8, 4, 3, 1], } ) df province city sales 0 ON Toronto 13 1 QC Montreal 6 2 BC Vancouver 16 3 AL Calgary 8 4 AL Edmonton 4 5 MN Winnipeg 3 6 ON Windsor 1 You want to group sales by province and get subtotal for total state. table = ( df.pivot_table( values=\"sales\", index=\"province\", columns=\"city\", aggfunc=\"sum\", margins=True ) .stack() .drop(\"All\") ) table province city AL Calgary 8.0 Edmonton 4.0 All 12.0 BC Vancouver 16.0 All 16.0 MN Winnipeg 3.0 All 3.0 ON Toronto 13.0 Windsor 1.0 All 14.0 QC Montreal 6.0 All 6.0 dtype: float64 ","date":"2020-08-09","objectID":"/pandas-cookbook/:4:1","tags":["python"],"title":"Pandas cookbook","uri":"/pandas-cookbook/"},{"categories":null,"content":"Counting number of equal adjacent values In the below column, cumulatively count the number of adjacent equal values. df = pd.DataFrame([1, 5, 7, 7, 1, 1, 1, 0], columns=[\"a\"]) df a 0 1 1 5 2 7 3 7 4 1 5 1 6 1 7 0 Solution based on this Stack Overflow answer: df[\"count\"] = df.groupby((df.a != df.a.shift()).cumsum()).cumcount().add(1) df a count 0 1 1 1 5 1 2 7 1 3 7 2 4 1 1 5 1 2 6 1 3 7 0 1 ","date":"2020-08-09","objectID":"/pandas-cookbook/:5:0","tags":["python"],"title":"Pandas cookbook","uri":"/pandas-cookbook/"},{"categories":null,"content":"Reset cumsum() at each missing value s = pd.Series([1.0, 3.0, 1.0, np.nan, 1.0, 1.0, 1.0, 1.0, np.nan, 1.0]) s 0 1.0 1 3.0 2 1.0 3 NaN 4 1.0 5 1.0 6 1.0 7 1.0 8 NaN 9 1.0 dtype: float64 What I don‚Äôt want: s.cumsum() 0 1.0 1 4.0 2 5.0 3 NaN 4 6.0 5 7.0 6 8.0 7 9.0 8 NaN 9 10.0 dtype: float64 Instead, I want to reset the counter to zero after each missing value. Solution from this SO answer. cumsum = s.cumsum().ffill() reset = -cumsum[s.isna()].diff().fillna(cumsum) result = s.where(s.notna(), reset).cumsum() result 0 1.0 1 4.0 2 5.0 3 0.0 4 1.0 5 2.0 6 3.0 7 4.0 8 0.0 9 1.0 dtype: float64 ","date":"2020-08-09","objectID":"/pandas-cookbook/:6:0","tags":["python"],"title":"Pandas cookbook","uri":"/pandas-cookbook/"},{"categories":null,"content":"Apply ","date":"2020-08-09","objectID":"/pandas-cookbook/:7:0","tags":["python"],"title":"Pandas cookbook","uri":"/pandas-cookbook/"},{"categories":null,"content":"Using apply with groupby From the cookbook: df = pd.DataFrame( { \"animal\": \"cat dog cat fish dog cat cat\".split(), \"size\": list(\"SSMMMLL\"), \"weight\": [8, 10, 11, 1, 20, 12, 12], \"adult\": [False] * 5 + [True] * 2, } ) df animal size weight adult 0 cat S 8 False 1 dog S 10 False 2 cat M 11 False 3 fish M 1 False 4 dog M 20 False 5 cat L 12 True 6 cat L 12 True # Return size of heaviest animal df.groupby(\"animal\").apply(lambda g: g.loc[g.weight.idxmax(), \"size\"]) animal cat L dog M fish M dtype: object ","date":"2020-08-09","objectID":"/pandas-cookbook/:7:1","tags":["python"],"title":"Pandas cookbook","uri":"/pandas-cookbook/"},{"categories":null,"content":"Expanding apply Assume you want to calculate the cumulative return from a series of one-period returns in an expanding fashion ‚Äì in each period, you want the cumulative return up to that period. s = pd.Series([i / 100.0 for i in range(1, 4)]) s 0 0.01 1 0.02 2 0.03 dtype: float64 The solution is given here. import functools def cum_return(x, y): return x * (1 + y) def red(x): res = functools.reduce(cum_return, x, 1) return res s.expanding().apply(red, raw=True) 0 1.010000 1 1.030200 2 1.061106 dtype: float64 I found that somewhere between bewildering and magical. To see what‚Äôs going on, it helps to add a few print statements: import functools def cum_return(x, y): print(\"x:\", x) print(\"y:\", y) return x * (1 + y) def red(x): print(\"Series:\", x) res = functools.reduce(cum_return, x, 1) print(\"Result:\", res) print() return res s.expanding().apply(red, raw=True) Series: [0.01] x: 1 y: 0.01 Result: 1.01 Series: [0.01 0.02] x: 1 y: 0.01 x: 1.01 y: 0.02 Result: 1.0302 Series: [0.01 0.02 0.03] x: 1 y: 0.01 x: 1.01 y: 0.02 x: 1.0302 y: 0.03 Result: 1.061106 0 1.010000 1 1.030200 2 1.061106 dtype: float64 This makes transparent how reduce works: it takes the starting value (1 here) as the initial x value and the first value of the series as y value, and then returns the result of cum_returns. Next, it uses that result as x, and the second element in the series as y, and calculates the new result of cum_returns. This is then repeated until it has run through the entire series. What surprised me is to see that reduce always starts the calculation from the beginning, rather than re-using the last calculated result. This seems inefficient, but is probably necessary for some reason. ","date":"2020-08-09","objectID":"/pandas-cookbook/:7:2","tags":["python"],"title":"Pandas cookbook","uri":"/pandas-cookbook/"},{"categories":null,"content":"Sort by sum of group values df = pd.DataFrame( { \"code\": [\"foo\", \"bar\", \"baz\"] * 2, \"data\": [0.16, -0.21, 0.33, 0.45, -0.59, 0.62], \"flag\": [False, True] * 3, } ) df code data flag 0 foo 0.16 False 1 bar -0.21 True 2 baz 0.33 False 3 foo 0.45 True 4 bar -0.59 False 5 baz 0.62 True g = df.groupby(\"code\") sort_order = g[\"data\"].transform(sum).sort_values().index df.loc[sort_order] code data flag 1 bar -0.21 True 4 bar -0.59 False 0 foo 0.16 False 3 foo 0.45 True 2 baz 0.33 False 5 baz 0.62 True ","date":"2020-08-09","objectID":"/pandas-cookbook/:7:3","tags":["python"],"title":"Pandas cookbook","uri":"/pandas-cookbook/"},{"categories":null,"content":"Get observation with largest data entry for each group g = df.groupby(\"code\") g.apply(lambda g: g.loc[g.data.idxmax()]) code data flag code bar bar -0.21 True baz baz 0.62 True foo foo 0.45 True ","date":"2020-08-09","objectID":"/pandas-cookbook/:7:4","tags":["python"],"title":"Pandas cookbook","uri":"/pandas-cookbook/"},{"categories":null,"content":"Expanding group operations Based on this answer. df = pd.DataFrame( { \"code\": [\"foo\", \"bar\", \"baz\"] * 4, \"data\": [0.16, -0.21, 0.33, 0.45, -0.59, 0.62] * 2, \"flag\": [False, True] * 6, } ) df code data flag 0 foo 0.16 False 1 bar -0.21 True 2 baz 0.33 False 3 foo 0.45 True 4 bar -0.59 False 5 baz 0.62 True 6 foo 0.16 False 7 bar -0.21 True 8 baz 0.33 False 9 foo 0.45 True 10 bar -0.59 False 11 baz 0.62 True g = df.groupby(\"code\") def helper(g): s = g.data.expanding() g[\"exp_mean\"] = s.mean() g[\"exp_sum\"] = s.sum() g[\"exp_count\"] = s.count() return g g.apply(helper).sort_values(\"code\") code data flag exp_mean exp_sum exp_count 1 bar -0.21 True -0.210000 -0.21 1.0 4 bar -0.59 False -0.400000 -0.80 2.0 7 bar -0.21 True -0.336667 -1.01 3.0 10 bar -0.59 False -0.400000 -1.60 4.0 2 baz 0.33 False 0.330000 0.33 1.0 5 baz 0.62 True 0.475000 0.95 2.0 8 baz 0.33 False 0.426667 1.28 3.0 11 baz 0.62 True 0.475000 1.90 4.0 0 foo 0.16 False 0.160000 0.16 1.0 3 foo 0.45 True 0.305000 0.61 2.0 6 foo 0.16 False 0.256667 0.77 3.0 9 foo 0.45 True 0.305000 1.22 4.0 ","date":"2020-08-09","objectID":"/pandas-cookbook/:7:5","tags":["python"],"title":"Pandas cookbook","uri":"/pandas-cookbook/"},{"categories":null,"content":"Get observation with largest data entry for each group df = pd.DataFrame({\"a\": [4, 5, 6, 7], \"b\": [10, 20, 30, 40], \"c\": [100, 50, -30, -50]}) df a b c 0 4 10 100 1 5 20 50 2 6 30 -30 3 7 40 -50 myval = 34 df.loc[(df.c - myval).abs().argsort()] a b c 1 5 20 50 2 6 30 -30 0 4 10 100 3 7 40 -50 Reminder of what happens here: a = (df.c - myval).abs() b = a.argsort() a, b (0 66 1 16 2 64 3 84 Name: c, dtype: int64, 0 1 1 2 2 0 3 3 Name: c, dtype: int64) argsort returns a series of indexes, so that df[b] returns an ordered dataframe. The first element in b thus refers to the index of the smallest element in a. ","date":"2020-08-09","objectID":"/pandas-cookbook/:7:6","tags":["python"],"title":"Pandas cookbook","uri":"/pandas-cookbook/"},{"categories":null,"content":"Creating separate dataframe for each group df = sns.load_dataset(\"iris\") pieces = dict(list(df.groupby(\"species\"))) pieces[\"setosa\"].head(3) sepal_length sepal_width petal_length petal_width species 0 5.1 3.5 1.4 0.2 setosa 1 4.9 3.0 1.4 0.2 setosa 2 4.7 3.2 1.3 0.2 setosa ","date":"2020-08-09","objectID":"/pandas-cookbook/:7:7","tags":["python"],"title":"Pandas cookbook","uri":"/pandas-cookbook/"},{"categories":null,"content":"Aggregating From here df = pd.DataFrame( { \"StudentID\": [\"x1\", \"x10\", \"x2\", \"x3\", \"x4\", \"x5\", \"x6\", \"x7\", \"x8\", \"x9\"], \"StudentGender\": [\"F\", \"M\", \"F\", \"M\", \"F\", \"M\", \"F\", \"M\", \"M\", \"M\"], \"ExamYear\": [ \"2007\", \"2007\", \"2007\", \"2008\", \"2008\", \"2008\", \"2008\", \"2009\", \"2009\", \"2009\", ], \"Exam\": [ \"algebra\", \"stats\", \"bio\", \"algebra\", \"algebra\", \"stats\", \"stats\", \"algebra\", \"bio\", \"bio\", ], \"Participated\": [ \"no\", \"yes\", \"yes\", \"yes\", \"no\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", ], \"Passed\": [\"no\", \"yes\", \"yes\", \"yes\", \"no\", \"yes\", \"yes\", \"yes\", \"no\", \"yes\"], }, columns=[ \"StudentID\", \"StudentGender\", \"ExamYear\", \"Exam\", \"Participated\", \"Passed\", ], ) df.columns = [str.lower(c) for c in df.columns] df studentid studentgender examyear exam participated passed 0 x1 F 2007 algebra no no 1 x10 M 2007 stats yes yes 2 x2 F 2007 bio yes yes 3 x3 M 2008 algebra yes yes 4 x4 F 2008 algebra no no 5 x5 M 2008 stats yes yes 6 x6 F 2008 stats yes yes 7 x7 M 2009 algebra yes yes 8 x8 M 2009 bio yes no 9 x9 M 2009 bio yes yes numyes = lambda x: sum(x == \"yes\") df.groupby(\"examyear\").agg({\"participated\": numyes, \"passed\": numyes}) participated passed examyear 2007 2 2 2008 3 3 2009 3 2 ","date":"2020-08-09","objectID":"/pandas-cookbook/:8:0","tags":["python"],"title":"Pandas cookbook","uri":"/pandas-cookbook/"},{"categories":null,"content":"Count days since last occurrence of event We want a count of the number of days passed since the last event happened. dfr = pd.DataFrame( { \"date\": pd.date_range(start=\"1 Jan 2022\", periods=8, freq=\"d\"), \"event\": [np.nan, np.nan, 1, np.nan, np.nan, np.nan, 4, np.nan], \"result\": [np.nan, np.nan, 0, 1, 2, 3, 0, 1], } ) dfr date event result 0 2022-01-01 NaN NaN 1 2022-01-02 NaN NaN 2 2022-01-03 1.0 0.0 3 2022-01-04 NaN 1.0 4 2022-01-05 NaN 2.0 5 2022-01-06 NaN 3.0 6 2022-01-07 4.0 0.0 7 2022-01-08 NaN 1.0 df = dfr.iloc[:, :-1] df date event 0 2022-01-01 NaN 1 2022-01-02 NaN 2 2022-01-03 1.0 3 2022-01-04 NaN 4 2022-01-05 NaN 5 2022-01-06 NaN 6 2022-01-07 4.0 7 2022-01-08 NaN def add_days_since_event(df): ones = df.event.ffill().where(lambda s: s.isna(), 1) cumsum = ones.cumsum() reset = -cumsum[df.event.notna()].diff().fillna(cumsum).sub(1) df[\"result\"] = ones.where(df.event.isna(), reset).cumsum() return df add_days_since_event(df) date event result 0 2022-01-01 NaN NaN 1 2022-01-02 NaN NaN 2 2022-01-03 1.0 0.0 3 2022-01-04 NaN 1.0 4 2022-01-05 NaN 2.0 5 2022-01-06 NaN 3.0 6 2022-01-07 4.0 0.0 7 2022-01-08 NaN 1.0 ","date":"2020-08-09","objectID":"/pandas-cookbook/:9:0","tags":["python"],"title":"Pandas cookbook","uri":"/pandas-cookbook/"},{"categories":null,"content":"Sources Python for Data Analysis Python Data Science Handbook Pandas cookbook Data School - Data science best practice with Pandas ","date":"2020-08-09","objectID":"/pandas-cookbook/:10:0","tags":["python"],"title":"Pandas cookbook","uri":"/pandas-cookbook/"},{"categories":null,"content":"Introduction and definitions Why do we estimate f? Purpose of ml is often to infer a function f that describes the relationship between target and features. Can estimate f for (1) prediction or (2) inference or both. How do we estimate f? 3 basic approaches: parametric (assume shape of f and estimate coefficients), non-parametric (also estimate shape of f), semi-parametric. Accuracy depends on (1) irreducible error (variance of error term) and (2) reducible error (appropriateness of our model and its assumptions) Ingredients to statistical learning Specify aim Gather and pre-process data Select a learning algorithm Apply learning algorithm to data to build (and train) a model Assess model performance (by testing) and tune model Make predictions Types of learning Supervised (labelled examples) Unsupervised (unlabelled examples) Semi-supervised (labelled and unlabelled examples) Reinforcement The trade-off between prediction accuracy and model interpretability Linear models vs non-linear models (hard to interpret models often predict more accurately). Supervised vs.¬†unsupervised learning Regression vs classification problems Classification assigns categorical labels, regression real-valued labels to unlabelled examples. Hyperparameters vs parameters Hyperparameters determine how the algorithm works and are set by the researcher. Parameters determine the shape of the model and are estimated. Model-based vs instance-based learning Model-based algorithms estimate and then use parameters to make predictions (i.e.¬†can discard training data once you have estimate), instance-based algorithms (e.g.¬†KNN) use the entire training dataset. Deep vs shallow learning Shallow learning algorithms learn parameters directly from features, deep learning algorithms (deep neural network) learn them from the output of preceeding layers. Sources of prediction error If we think about $Y = f(X) + \\epsilon$ Reducible error stems from our imperfect ability to estimate the model (the systematic relationship between X and y) i.e.¬†the difference between $f$ and $\\hat{f}$. Irreducible error stems from the fact that even if we could perfectly model the relationship between X and Y, Y would still be a function of the error term $\\epsilon$, which we cannot reduce. This could be variables other than X that predict Y, or random variation in Y (e.g.¬†purchasing behaviour on given day influenced by car breakdown). Practical tips One way to get a sense of how non-linear the problem is, is to compare the MSE of a simple linear model and a more complex model. If the two are very close, then assuming linearity and using a simple model is preferrable. Feature engineering ","date":"2020-04-01","objectID":"/machine-learning-basics/:0:0","tags":["datascience"],"title":"Machine learning basics","uri":"/machine-learning-basics/"},{"categories":null,"content":"General points Having many uninformative features in your model might lower performance as it makes is more likely that the model overfits (e.g.¬†including country names when predicting life satisfaction and using OECD sample finds that ‚Äúw‚Äù in country name predicts high life satisfaction because of Norway, Switzerland, New Zealand, and Sweden. But this doesn‚Äôt generalize to Rwanda and Zimbabwe). Hence, the more uninformative features, the bigger the chance that the model will find a pattern by chance in one of them in your training set. ","date":"2020-04-01","objectID":"/machine-learning-basics/:1:0","tags":["datascience"],"title":"Machine learning basics","uri":"/machine-learning-basics/"},{"categories":null,"content":"Process in practice Brainstorm features Decide what features to create Create the features Test impact on model performance Interacting on features is useful Iterate (go to 3 or 1) Stuff to try: Ratios Counts Cutoff points Iterate on features (change cutoff and see whether it makes a difference) Rate of Change in Values Range of Values Density within Intervals of Time Proportion of Instances of Commonly Occurring Values Time Between Occurrences of Commonly Occurring Values ","date":"2020-04-01","objectID":"/machine-learning-basics/:2:0","tags":["datascience"],"title":"Machine learning basics","uri":"/machine-learning-basics/"},{"categories":null,"content":"Data processing Data transformations for individual predictors Data transformations for multiple predictors Dealing with missing values Removing predictors Adding predictors Binning predictors ","date":"2020-04-01","objectID":"/machine-learning-basics/:3:0","tags":["datascience"],"title":"Machine learning basics","uri":"/machine-learning-basics/"},{"categories":null,"content":"Measuring predictor importance Numeric outcomes Categorical outcomes Other approaches ","date":"2020-04-01","objectID":"/machine-learning-basics/:4:0","tags":["datascience"],"title":"Machine learning basics","uri":"/machine-learning-basics/"},{"categories":null,"content":"Feature selection Consequences of using non-informative predictors Approaches for reducing the number of predictors Wrapper methods Filter methods Selection bias ","date":"2020-04-01","objectID":"/machine-learning-basics/:5:0","tags":["datascience"],"title":"Machine learning basics","uri":"/machine-learning-basics/"},{"categories":null,"content":"Factors that can affect model performance Type III errors Measurement errors in the outcome Measurement errors in the predictors Distretising continuous outcomes When should you trust your model‚Äôs prediction? The impact of a large sample Model selection and assessment ","date":"2020-04-01","objectID":"/machine-learning-basics/:6:0","tags":["datascience"],"title":"Machine learning basics","uri":"/machine-learning-basics/"},{"categories":null,"content":"Notes The ultimate trade-off for model performance is between bias and variance. Simple models will have higher bias and lower variance, more complex models lower bias but higher variance (becuse they tend to overfit the training data). Two things mainly determine model performance: Model complexity (validation curves in PDSH) Training data size (learning curves in PDSH) What to do when model is underperforming? Use a more complicated/flexible model Use a less complicated/flexible model Gather more training samples (make data longer) Gather more data for additional features for each sample (make data wider) Train and test vs cross-validation These approaches are complementary, as nicely explained in the Scikit-learn docs: the generall process is to split the data into a training and a testing dataset, and then to use cross-validation on the training data to find the optimal hyperparameters. ","date":"2020-04-01","objectID":"/machine-learning-basics/:7:0","tags":["datascience"],"title":"Machine learning basics","uri":"/machine-learning-basics/"},{"categories":null,"content":"Assessing model accuracy Measuring the quality of fit MSE for regressions. Error rate for classification. Beware of overfitting! Maximise mse or error rate for testing data, not for training data (overfitting). Overfitting definition: situation where a simpler model with a worse training score would have achieved a better testing score. The bias-variance trade-off MSE comprises (1) squared bias of estimate, (2) variance of estimate, and (3) variance of error. We want to minimise 1 and 2 (3 is fixed). Relationship between MSE and model complexity is U-shaped, because variance increases and bias decreases with complexity. We want to find optimal balance. Classification setting Bayesian classifier as unattainable benchmark (we don‚Äôt know P(y|x)). KNN one approach to estimate P(y|x), then use bayesian classifier. Intuition as for MSE error: testing error rate is U-shaped in K (higher K means more flexibel model). ","date":"2020-04-01","objectID":"/machine-learning-basics/:8:0","tags":["datascience"],"title":"Machine learning basics","uri":"/machine-learning-basics/"},{"categories":null,"content":"Cross validation Use cases: Model assessment (‚Äúhow good is model fit?‚Äù) Model selection (hypterparameter tuning) ","date":"2020-04-01","objectID":"/machine-learning-basics/:9:0","tags":["datascience"],"title":"Machine learning basics","uri":"/machine-learning-basics/"},{"categories":null,"content":"Holdout set approach How it works: Split data into training and test data, fit model on the training data, assess on the test data. Advantages: Conceptually simple and computationally cheap. Disadvantages: Very sensitive to train-test split, especially for small samples. Because we only use a fraction of the available data to train the model, model fit is poorer comparated to using entire dataset, and thus we tend to overestimate test error. ","date":"2020-04-01","objectID":"/machine-learning-basics/:9:1","tags":["datascience"],"title":"Machine learning basics","uri":"/machine-learning-basics/"},{"categories":null,"content":"Leave one out cross-validation How it works: If we have n observations, fit model using n-1 observations and calculate MSE for nth observation. Repeat n times and then average MSEs to get test error rate. Advantage: Deals with both problems of holdout set approach. Disadvantage: Computationally expensive or prohibitive for large ns ","date":"2020-04-01","objectID":"/machine-learning-basics/:9:2","tags":["datascience"],"title":"Machine learning basics","uri":"/machine-learning-basics/"},{"categories":null,"content":"k-fold cross-validation How it works: Similar to LOOCV (which is a special case where k = n), but we split data into k groups (folds), and then use each of them in turn as the test training set while training the model on the remaining k-1 folds. We again average the k MSEs to get the overall test error rate. Advantage: Computationally cheaper than LOOCV Beats LOOCV in terms of MSE because it trades-off bias and variance in a more balanced way: LOOCV has virtually no bias (as we use almost the entire data to fit the model each time) but has higher variance because all k MSE estimates are highly correlated (because each model is fit with almost identical data). In contrast, training data for k-fold CV is less similar, meaning MSEs are less correlated, meaning reduction in variance from averaging is greater. ","date":"2020-04-01","objectID":"/machine-learning-basics/:9:3","tags":["datascience"],"title":"Machine learning basics","uri":"/machine-learning-basics/"},{"categories":null,"content":"Bootstrap Use cases Model assessment (‚Äúhow good is parameter fit?‚Äù) How it works To get an estimate for our model fit, we would ideally want to fit the model on many random population samples, so that we could then calculate means and standard deviations of our model assessment metric of interest. We can‚Äôt draw multiple samples from the population, but the bootstrap mimics this approach by repeatedly sampling (with replacement) from our original sample. ","date":"2020-04-01","objectID":"/machine-learning-basics/:10:0","tags":["datascience"],"title":"Machine learning basics","uri":"/machine-learning-basics/"},{"categories":null,"content":"Confusion matrix import seaborn as sns sns.set() import matplotlib.pyplot as plt from sklearn.datasets import load_breast_cancer from sklearn.metrics import confusion_matrix from sklearn.model_selection import train_test_split from sklearn.naive_bayes import GaussianNB data = load_breast_cancer() Xtrain, Xtest, ytrain, ytest = train_test_split( data.data, data.target, random_state=23124 ) model = GaussianNB() model.fit(Xtrain, ytrain) ymodel = model.predict(Xtest) mat = confusion_matrix(ytest, ymodel) sns.heatmap(mat, square=True, annot=True, cbar=False, cmap=\"Blues\") plt.xlabel(\"Predicted value\") plt.ylabel(\"True value\"); ","date":"2020-04-01","objectID":"/machine-learning-basics/:11:0","tags":["datascience"],"title":"Machine learning basics","uri":"/machine-learning-basics/"},{"categories":null,"content":"Terminology Can think of ‚Äú+‚Äù as ‚Äúdiseases‚Äù or ‚Äúalternative hypothesis‚Äù, and of ‚Äú-‚Äù as ‚Äúno disease‚Äù or ‚Äúnull hypothesis‚Äù. Predicted - + True - True negative (TN) False positive (FP) N + False negative (FN) True positive (TP) P N* P* True positive rate: $\\frac{TP}{TP + FN} = \\frac{TP}{P}$, is the proportion of positives that we correctly predict as such. Also called sensitivity, of hit rate, or recall. False positive rate: $\\frac{FP}{FP + TN} = \\frac{FP}{N}$, is the proportion of negatives events we incorrectly predict as positives. Also called the alarm rate or inverted specificity, where specificity is $\\frac{TN}{FP + TN} = \\frac{TN}{N}$. Precision: $\\frac{TP}{TP + FP} = \\frac{TP}{P*}$. Precision and recall originate in the field of information retrieval (e.g.¬†getting documents from a query). In its original context, precision is useful documents as a proportion of all retrieved documents, recall the retrieved useful documents as a proportion of all available useful documents. In the context of machine learning, we can think of precision as positive predictive power (how good is the model at predicting the positive class), while recall is the same as sensitivity ‚Äì the proportion of all events that were successfully predicted. ","date":"2020-04-01","objectID":"/machine-learning-basics/:11:1","tags":["datascience"],"title":"Machine learning basics","uri":"/machine-learning-basics/"},{"categories":null,"content":"ROC curves and AUC Plots the trade-off between the false positive rate (x-axis) and the true positive rate (y-axis) - the trade-off between the false alarm rate and the hit rate. The ROC is useful because it directly shows false/true negatives (on the x-axis) and false/true positives (on the y-axis) for different thresholds and thus helps choose the best threshold, and because the area under the curve (AUC) can be interpreted as an overall model summary, and thus allows us to compare different models. import seaborn as sns sns.set() import matplotlib.pyplot as plt from sklearn.datasets import load_breast_cancer from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import confusion_matrix, plot_roc_curve from sklearn.model_selection import train_test_split from sklearn.naive_bayes import GaussianNB data = load_breast_cancer() Xtrain, Xtest, ytrain, ytest = train_test_split( data.data, data.target, random_state=23124 ) model = GaussianNB() model.fit(Xtrain, ytrain) ymodel = model.predict(Xtest) roc = plot_roc_curve(model, Xtest, ytest) rfc = RandomForestClassifier(n_estimators=10, random_state=42) rfc.fit(Xtrain, ytrain) ax = plt.gca() _ = plot_roc_curve(rfc, Xtest, ytest, ax=ax, alpha=0.8) roc.plot(ax=ax, alpha=0.8); ","date":"2020-04-01","objectID":"/machine-learning-basics/:12:0","tags":["datascience"],"title":"Machine learning basics","uri":"/machine-learning-basics/"},{"categories":null,"content":"Precision-recall curves The precision-recall curve is particularly useful when we have much more no-event than event cases. In this case, we‚Äôre often not interested much in correctly predicting no-events but focused on correctly predicting events. Because neither precision nor recall use true negatives in their calculations, they are well suited to this context (paper). The precision recall curve plots precision (y-axis) and recall (x-axis). An unskilled model is a horizontal line at hight equal to the proportion of events in the sample. We can use ROC to compare models as different thresholds, or the F-score (the harmonic mean between precision and recall) at a specific threshold. Use precision-recall curves if classes are imbalanced, in which case ROC can be misleading (see example in last section here). Shape of curve: recall increases monotonically as we lower the threshold (move from left to right in the graph) because we‚Äôll find more and more true positives (‚Äúputting ones from the false negative to the true positive bucket‚Äù, which increases the numerator but leaves the denominator unchanged), but precision needn‚Äôt fall monotonically, because we also increase false positives (both the numerator and the denominator increase as we lower the threshold, and the movement of recall depends on which increases faster, which depends on the sequence of ordered predicted events). See here for a useful example. from sklearn.metrics import plot_precision_recall_curve data = load_breast_cancer() Xtrain, Xtest, ytrain, ytest = train_test_split( data.data, data.target, random_state=23124 ) model = GaussianNB() model.fit(Xtrain, ytrain) ymodel = model.predict(Xtest) plot_precision_recall_curve(model, Xtest, ytest); ","date":"2020-04-01","objectID":"/machine-learning-basics/:13:0","tags":["datascience"],"title":"Machine learning basics","uri":"/machine-learning-basics/"},{"categories":null,"content":"Learning curves Sources An introduction to statistical learning Hands on machine learning with scikit-learn, keras, and tenserflow Python Data Science Handbook The hundred-page machine learning book Applied predictive modeling video mlmastery article ","date":"2020-04-01","objectID":"/machine-learning-basics/:14:0","tags":["datascience"],"title":"Machine learning basics","uri":"/machine-learning-basics/"},{"categories":null,"content":"Frequently used interaction patterns with AWS. ","date":"2020-03-14","objectID":"/aws/:0:0","tags":["tools"],"title":"AWS","uri":"/aws/"},{"categories":null,"content":"CLI To create a new bucket, use aws s3 mb bucketname. To add a subfolder to a bucket, use aws s3api put-object --bucket bucketname --key foldername Setup There are multiple ways to access your AWS account. I store config and credential files in ~/.aws as discussed here. AWS access methods find these files automatically so I don‚Äôt have to worry about that. What I do have to worry about is choosing the appropriate profile depending on what AWS account I want to interact with (e.g.¬†my personal one or one for work). This is different for each library, so I cover this below. s3fs Built by people at Dask, s3fs is built on top of botocore and provides a convenient way to interact with S3. It can read and ‚Äì I think ‚Äì write data, but there are easier ways to do that, and I use the library mainly to navigate buckets and list content. ","date":"2020-03-14","objectID":"/aws/:1:0","tags":["tools"],"title":"AWS","uri":"/aws/"},{"categories":null,"content":"Navigate buckets import s3fs # establish connection fs = s3fs.S3FileSystem() # count items in s3 root bucket fs.ls('/fgu-samples') ['fgu-samples/transactions.parquet'] To choose a profile other than default, use: # connect using a different profile fs = s3fs.S3FileSystem(profile='tracker-fgu') len(fs.ls('/')) 6 #¬†Read and write directly from Pandas Pandas can read and write files to and from S3 directly if you provide the file name as s3://\u003cbucket\u003e/\u003cfilename\u003e. By default, Pandas uses the default profile to access S3. Recent versions of Pandas have a storage_options parameter that can be used to provide, among other things, a profile name. ","date":"2020-03-14","objectID":"/aws/:2:0","tags":["tools"],"title":"AWS","uri":"/aws/"},{"categories":null,"content":"Basics import pandas as pd # read using default profile fp = 's3://fgu-samples/transactions.parquet' df = pd.read_parquet(fp) df.shape (1168943, 21) # read using custom profile fp = 's3://temp-mdb/data_XX7.parquet' df = pd.read_parquet(fp, storage_options = dict(profile='tracker-fgu')) df.shape (9388334, 23) This works well for simple jobs, but in a large project, passing the profile information to each read and write call is cumbersome and ugly. ","date":"2020-03-14","objectID":"/aws/:3:0","tags":["tools"],"title":"AWS","uri":"/aws/"},{"categories":null,"content":"Simple improvement using functools.partial functools.partialprovides a simple solution, as it allows me to create a custom function with a frozen storage options argument. import functools options = dict(storage_options=dict(profile='tracker-fgu')) read_parquet_s3 = functools.partial(pd.read_parquet, **options) df = read_parquet_s3(fp) df.shape (9388334, 23) ","date":"2020-03-14","objectID":"/aws/:4:0","tags":["tools"],"title":"AWS","uri":"/aws/"},{"categories":null,"content":"More flexible solution with custom function Often, I run projects on my Mac for testing and a virtual machine to run the full code. In this case, I need a way to automatically provide the correct profile name. s3 = s3fs.S3FileSystem(profile='tracker-fgu') s3.ls('/raw-mdb/') ['raw-mdb/data_777.csv', 'raw-mdb/data_X77.csv'] import functools import platform def get_aws_profile(): \"\"\" Return access point specific AWS profile to use for S3 access. \"\"\" if platform.node() == 'FabsMacBook.local': profile = 'tracker-fgu' else: profile = 'default' return profile class s3: \"\"\" Create read and write functions with frozen AWS profile. \"\"\" def __init__(self): self.profile = get_aws_profile() self.options = dict(storage_options=dict(profile=self.profile)) def read_csv(self): return functools.partial(pd.read_csv, **self.options) def make_s3_funcs(): \"\"\" Return readers and writers with frozen AWS profile name. \"\"\" # identify required profile (depends on project) if platform.node() == 'FabsMacBook.local': profile = 'tracker-fgu' else: profile = 'default' # create partial readers and writers options = dict(storage_options=dict(profile=profile)) read_csv_s3 = functools.partial(pd.read_csv, **options) write_csv_s3 = functools.partial(pd.write_csv, **options) read_parquet_s3 = functools.partial(pd.read_parquet, **options) write_parquet_s3 = functools.partial(pd.write_parquet, **options) fp = 's3://raw-mdb/data_777.csv' s3.read_csv(fp) AttributeError: 'str' object has no attribute 'options' The above is not ideal, as it requires cumbersome unpacking of return. Maybe using decorator is better. awswrangler A new library from AWS labs for Pandas interaction with a number of AWS services. Looks very promising, but haven‚Äôt had any use for it thus far. ","date":"2020-03-14","objectID":"/aws/:5:0","tags":["tools"],"title":"AWS","uri":"/aws/"},{"categories":null,"content":" This notebook contains my replication of this blog post by Jake VanderPlas on using data from bicycle traffic across Seattle‚Äôs Fremont Bridge to learn about commuting patterns. import os import ssl from urllib.request import urlretrieve import altair as alt import matplotlib.pyplot as plt import numpy as np import pandas as pd import seaborn from pandas.tseries.holiday import USFederalHolidayCalendar from seattlecycling.data import get_fremont_data from seattlecycling.toolbox import csnap from sklearn.decomposition import PCA from sklearn.mixture import GaussianMixture as GMM from vega_datasets import data as vega_data seaborn.set() Helper functions fremont_url = ( \"https://data.seattle.gov/api/views/65db-xm6k/rows.csv?accessType=DOWNLOAD\" ) def get_fremont_data( filename=\"seattle_weather_fremont.csv\", url=fremont_url, force_download=False ): \"\"\"Download and cache the fremont bridge data Parameters ---------- filename : string (optional) location to store the data url : string (optional) web location of the data force_download : Boolean (optional) if True, force redownload of data Returns ------- data : pandas.DataFrame The fremont bridge data \"\"\" # Solve problem with SSL certificate verification if not os.environ.get(\"PYTHONHTTPSVERIFY\", \"\") and getattr( ssl, \"_create_unverified_context\", None ): ssl._create_default_https_context = ssl._create_unverified_context # Download and prepare data if force_download or not os.path.exists(filename): urlretrieve(url, filename) data = pd.read_csv(\"seattle_weather_fremont.csv\", index_col=\"Date\") try: data.index = pd.to_datetime(data.index, format=\"%m/%d/%Y %I:%M:%S %p\") except TypeError: data.index = pd.to_datetime(data.index) data.columns = [\"total\", \"west\", \"east\"] return data def hours_of_daylight(date, axis=23.44, latitude=47.61): \"\"\"Compute the hours of daylight for the given date\"\"\" diff = date - pd.datetime(2000, 12, 21) day = diff.total_seconds() / 24.0 / 3600 day %= 365.25 m = 1.0 - np.tan(np.radians(latitude)) * np.tan( np.radians(axis) * np.cos(day * np.pi / 182.625) ) m = max(0, min(m, 2)) return 24.0 * np.degrees(np.arccos(1 - m)) / 180.0 def print_rms(var): \"\"\"Calculates and prints the root-mean-square about the trend line\"\"\" rms = np.std(var) print(\"Root-mean-square about trend: {0: .0f} riders\".format(rms)) def csnap(df, fn=lambda x: x.shape, msg=None): \"\"\" Custom Help function to print things in method chaining. Returns back the df to further use in chaining. \"\"\" if msg: print(msg) display(fn(df)) return df ","date":"2020-02-07","objectID":"/seattle-bicycle-traffic/:0:0","tags":["datascience"],"title":"Seattle bicycle traffic","uri":"/seattle-bicycle-traffic/"},{"categories":null,"content":"Unsupervised exploration # Load data start = \"1 Oct 2012\" end = \"30 Jun 2015\" data = get_fremont_data() data = data.loc[start:end] data.head(3) /var/folders/xg/n9p73cf50s52twlnz7z778vr0000gn/T/ipykernel_3899/1377138058.py:7: FutureWarning: Value based partial slicing on non-monotonic DatetimeIndexes with non-existing keys is deprecated and will raise a KeyError in a future Version. data = data.loc[start:end] total west east Date 2012-10-03 00:00:00 13.0 4.0 9.0 2012-10-03 01:00:00 10.0 4.0 6.0 2012-10-03 02:00:00 2.0 1.0 1.0 # A first look at the data data.resample(\"w\").sum().plot() \u003cmatplotlib.axes._subplots.AxesSubplot at 0x1a23e32850\u003e Same graph as above using Altair # Create a melted dataset melted = ( data.resample(\"w\") .sum() .reset_index() .rename(columns={\"Date\": \"date\"}) .melt( id_vars=\"date\", var_name=[\"side\"], value_name=\"crossings\", value_vars=[\"total\", \"east\", \"west\"], ) ) melted.head() date side crossings 0 2012-10-07 total 14292.0 1 2012-10-14 total 16795.0 2 2012-10-21 total 15509.0 3 2012-10-28 total 13437.0 4 2012-11-04 total 12194.0 # Produce same graph as above alt.Chart(melted).mark_line().encode(x=\"date\", y=\"crossings\", color=\"side\") ","date":"2020-02-07","objectID":"/seattle-bicycle-traffic/:1:0","tags":["datascience"],"title":"Seattle bicycle traffic","uri":"/seattle-bicycle-traffic/"},{"categories":null,"content":"Visualising the data # Create a datframe with days as dates and index and hours as columns pivoted = data.pivot_table( [\"west\", \"east\", \"total\"], index=data.index.date, columns=data.index.hour, fill_value=0, ) pivoted.head() east ... west Date 0 1 2 3 4 5 6 7 8 9 ... 14 15 16 17 18 19 20 21 22 23 2012-10-03 9 6 1 3 1 10 50 95 146 104 ... 77 72 133 192 122 59 29 25 24 5 2012-10-04 11 0 6 3 1 11 51 89 134 94 ... 63 73 114 154 137 57 27 31 25 11 2012-10-05 7 4 3 2 2 7 37 101 119 81 ... 63 80 120 144 107 42 27 11 10 16 2012-10-06 7 5 2 2 1 2 15 16 47 55 ... 89 115 107 107 41 40 25 18 14 15 2012-10-07 5 5 1 2 2 3 8 12 26 36 ... 126 122 132 118 68 26 19 12 9 5 5 rows √ó 72 columns # Put raw values in a matrix X = pivoted.values X.shape (1001, 72) # Use PCA to reduce dimensionality (keep 90 percent of the variance) Xpca = PCA(0.9).fit_transform(X) Xpca.shape (1001, 2) total_trips = X.sum(1) plt.scatter(Xpca[:, 0], Xpca[:, 1], c=total_trips, cmap=\"Paired\") plt.colorbar(label=\"Total trips\") \u003cmatplotlib.colorbar.Colorbar at 0x1a26ec9dd0\u003e What can we learn from this graph? We can see that the days fall into two quite distinct cluster, one with a higher number of trips and one with a lower number of trips, that the number of trips increases along the length of each projected cluster (i.e.¬†as we move away from the origin), and that close to the origin, the groups are less distinguishable. Overall, we can see that there are, in effect, two types of days for Seattle cyclists. This is indeed pretty cool. ","date":"2020-02-07","objectID":"/seattle-bicycle-traffic/:1:1","tags":["datascience"],"title":"Seattle bicycle traffic","uri":"/seattle-bicycle-traffic/"},{"categories":null,"content":"Unsupervised clustering # Use a Gaussian mixture model to separate days into two clusters gmm = GMM(2, covariance_type=\"full\", random_state=0) gmm.fit(Xpca) cluster_label = gmm.predict(Xpca) plt.scatter(Xpca[:, 0], Xpca[:, 1], c=cluster_label, cmap=\"Paired\") \u003cmatplotlib.collections.PathCollection at 0x1a249be050\u003e # Add cluster labels to original data pivoted[\"cluster\"] = cluster_label data = data.join(pivoted[\"cluster\"], on=data.index.date) data.head() total west east cluster Date 2012-10-03 00:00:00 13.0 4.0 9.0 0 2012-10-03 01:00:00 10.0 4.0 6.0 0 2012-10-03 02:00:00 2.0 1.0 1.0 0 2012-10-03 03:00:00 5.0 2.0 3.0 0 2012-10-03 04:00:00 7.0 6.0 1.0 0 # Plot hourly = data.groupby([\"cluster\", data.index.time]).mean() fig, ax = plt.subplots(1, 2, figsize=(14, 5)) hourly_ticks = 4 * 60 * 60 * np.arange(6) for i in range(2): hourly.loc[i].plot(ax=ax[i], xticks=hourly_ticks) ax[i].set_title(\"Cluster {0}\".format(i)) ax[i].set_ylabel(\"Average hourly trips\") First plot shows a sharp bimodal pattern, indicative of a communing pattern (with the majority of people riding west in the morning and east in the evening), while the second plot shows a wide unimodel pattern, indicative of weekend days and holidays. ","date":"2020-02-07","objectID":"/seattle-bicycle-traffic/:1:2","tags":["datascience"],"title":"Seattle bicycle traffic","uri":"/seattle-bicycle-traffic/"},{"categories":null,"content":"Uncovering work habits # Check whether two clusters correspond to weekend and weekdays dayofweek = pd.to_datetime(pivoted.index).dayofweek plt.scatter(Xpca[:, 0], Xpca[:, 1], c=dayofweek, cmap=plt.cm.get_cmap(\"Paired\", 7)) cb = plt.colorbar(ticks=range(7)) cb.set_ticklabels([\"Mo\", \"Tu\", \"We\", \"Th\", \"Fr\", \"Sa\", \"Su\"]) Let‚Äôs look more closely at weekdays that follow a weekend pattern, of which there are a few. results = pd.DataFrame( { \"cluster\": pivoted[\"cluster\"], \"is_weekend\": (dayofweek \u003e 4), \"weekday\": pivoted.index.map(lambda x: x.strftime(\"%a\")), } ) results.head() cluster is_weekend weekday 2012-10-03 0 False Wed 2012-10-04 0 False Thu 2012-10-05 0 False Fri 2012-10-06 1 True Sat 2012-10-07 1 True Sun Count number of weekend days with a workday pattern weekend_workdays = results.query(\"cluster == 0 and is_weekend\") len(weekend_workdays) 0 Count number of week days that fall into the weekend / holiday pattern weekday_holidays = results.query(\"cluster == 1 and not is_weekend\") len(weekday_holidays) 23 There were zero weekend days where people in Seattle decided to work, but 23 weekdays that appear to be public holidasy. Let‚Äôs have a look. # Download list of public holidays cal = USFederalHolidayCalendar() holidays = cal.holidays(\"2012\", \"2016\", return_name=True) holidays.head() 2012-01-02 New Years Day 2012-01-16 Martin Luther King Jr. Day 2012-02-20 Presidents Day 2012-05-28 Memorial Day 2012-07-04 July 4th dtype: object # Add the days before and after holidays to the list holidays_all = pd.concat( [ holidays, \"Day before \" + holidays.shift(-1, \"D\"), \"Day after \" + holidays.shift(1, \"D\"), ] ) holidays_all.sort_index(inplace=True) holidays_all.head() 2012-01-01 Day before New Years Day 2012-01-02 New Years Day 2012-01-03 Day after New Years Day 2012-01-15 Day before Martin Luther King Jr. Day 2012-01-16 Martin Luther King Jr. Day dtype: object # A list of holidays on which people in Seattle skip work holidays_all.name = \"name\" joined = weekday_holidays.join(holidays_all) set(joined[\"name\"]) {'Christmas', 'Day after Christmas', 'Day after Thanksgiving', 'Day before Christmas', 'July 4th', 'Labor Day', 'Memorial Day', 'New Years Day', 'Thanksgiving'} # A list of holidays on which people in Seattle do go to work set(holidays) - set(joined[\"name\"]) {'Columbus Day', 'Martin Luther King Jr. Day', 'Presidents Day', 'Veterans Day'} ","date":"2020-02-07","objectID":"/seattle-bicycle-traffic/:1:3","tags":["datascience"],"title":"Seattle bicycle traffic","uri":"/seattle-bicycle-traffic/"},{"categories":null,"content":"What‚Äôs up with Fridays? # Plot Fridays separately fridays = dayofweek == 4 plt.scatter(Xpca[:, 0], Xpca[:, 1], c=\"gray\", alpha=0.2) plt.scatter(Xpca[fridays, 0], Xpca[fridays, 1], c=\"green\") \u003cmatplotlib.collections.PathCollection at 0x1a26303c10\u003e What‚Äôs going on with the three strange outliers in the right bottom corner? # Get dates for the three outliers weird_fridays = pivoted.index[fridays \u0026 (Xpca[:, 0] \u003e 900)] weird_fridays Index([2013-05-17, 2014-05-16, 2015-05-15], dtype='object') # Plot pattern for three outliers relative to average Friday all_days = data.pivot_table(\"total\", index=data.index.time, columns=data.index.date) all_days.loc[:, weird_fridays].plot() all_days.mean(1).plot(color=\"grey\", lw=4, alpha=0.5, xticks=hourly_ticks) \u003cmatplotlib.axes._subplots.AxesSubplot at 0x1a2631b750\u003e We‚Äôve found Seattle‚Äôs bike to work day. ","date":"2020-02-07","objectID":"/seattle-bicycle-traffic/:1:4","tags":["datascience"],"title":"Seattle bicycle traffic","uri":"/seattle-bicycle-traffic/"},{"categories":null,"content":"Supervised modeling This part contains my replication of this blog post by Jake VanderPlan on using data from bicycle traffic across Seattle‚Äôs Fremont Bridge to learn about commuting patterns. %matplotlib inline import matplotlib.pyplot as plt import numpy as np import pandas as pd from sklearn.linear_model import LinearRegression plt.style.use(\"seaborn\") from seattlecycling.data import get_fremont_data from seattlecycling.toolbox import hours_of_daylight, print_rms # Load data start = \"1 Oct 2012\" end = \"15 May 2014\" data = get_fremont_data() data = data.loc[start:end] data.head(3) total west east Date 2012-10-03 00:00:00 13.0 4.0 9.0 2012-10-03 01:00:00 10.0 4.0 6.0 2012-10-03 02:00:00 2.0 1.0 1.0 # Resample data into daily and weekly totals daily = data.resample(\"d\").sum() weekly = data.resample(\"w\").sum() # A first look at the data weekly.plot() plt.ylabel(\"Weekly rides\"); # Look at rolling weekly mean to smooth out short-term variation data.resample(\"d\").sum().rolling(30, center=True).mean().plot(); Blog post points out that 2014 has seen increased cycle traffic across the bridge. Below we‚Äôre modelling seasonal variation based on what we think influences peoples‚Äô decision whether or not to ride a bike. ","date":"2020-02-07","objectID":"/seattle-bicycle-traffic/:2:0","tags":["datascience"],"title":"Seattle bicycle traffic","uri":"/seattle-bicycle-traffic/"},{"categories":null,"content":"Accounting for hours of daylight # Hours of daylight weekly[\"daylight\"] = list(map(hours_of_daylight, weekly.index)) daily[\"daylight\"] = list(map(hours_of_daylight, daily.index)) weekly[\"daylight\"].plot() plt.ylabel(\"Hours of daylight (Seattle)\"); # Relationship between daylight and cycle traffic plt.scatter(weekly.daylight, weekly.total) plt.xlabel(\"Hours of daylight\") plt.ylabel(\"Weekly bicycle traffic\"); # Adding a linear trend X = weekly[[\"daylight\"]] y = weekly[\"total\"] clf = LinearRegression(fit_intercept=True).fit(X, y) weekly[\"daylight_trend\"] = clf.predict(X) weekly[\"daylight_corrected_total\"] = ( weekly.total - weekly.daylight_trend + np.mean(weekly.daylight_trend) ) xfit = np.linspace(7, 17) yfit = clf.predict(xfit[:, None]) plt.scatter(weekly.daylight, weekly.total) plt.plot(xfit, yfit, \"-k\") plt.title(\"Bycicle traffic through the year\") plt.xlabel(\"Hours of daylight\") plt.ylabel(\"Weekly bicycle traffic\"); clf.coef_[0] 1966.2003072317068 # Plot detrended data trend = clf.predict(weekly[[\"daylight\"]].values) plt.scatter(weekly.daylight, weekly.total - trend + np.mean(trend)) plt.plot(xfit, np.mean(trend) + 0 * yfit, \"-k\") plt.title(\"Weekly traffic through the year (detrended)\") plt.xlabel(\"Hours of daylight\") plt.ylabel(\"Adjusted weekly bicycle traffic\"); In the graph above, we have removed the number of riders per week that correlate with the number of hours of daylight, so that we can think of what is shown of the number of rides per week we‚Äôd expect to see if daylight was not an issue. fix, ax = plt.subplots(1, 2, figsize=(15, 5)) weekly[[\"total\", \"daylight_trend\"]].plot(ax=ax[0]) weekly[\"daylight_corrected_total\"].plot(ax=ax[1]) ax[0].set_ylabel(\"Weekly crossing\") ax[0].set_title(\"Total weekly crossings and trend\") ax[1].set_ylabel(\"Adjusted weekly crossings\") ax[1].set_title(\"Detrended weekly crossings\") print_rms(weekly[\"daylight_corrected_total\"]) Root-mean-square about trend: 2872 riders ","date":"2020-02-07","objectID":"/seattle-bicycle-traffic/:2:1","tags":["datascience"],"title":"Seattle bicycle traffic","uri":"/seattle-bicycle-traffic/"},{"categories":null,"content":"Accounting for day of the week # Plot average number of trips by weekday days = [\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"] daily[\"dayofweek\"] = daily[\"total\"].index.dayofweek grouped = daily.groupby(\"dayofweek\")[\"total\"].mean() grouped.index = days grouped.plot() plt.title(\"Average crossings by weekday\") plt.ylabel(\"Average daily crossings\"); # Account for hours of daylight and day of week simultaneously # Add one-hot indicators of weekdays for i in range(7): daily[days[i]] = (daily.index.dayofweek == i).astype(float) # Detrend on days of week and daylight together X = daily[days + [\"daylight\"]] y = daily[\"total\"] clf = LinearRegression().fit(X, y) daily[\"dayofweek_trend\"] = clf.predict(X) daily[\"dayofweek_corrected\"] = ( daily[\"total\"] - daily[\"dayofweek_trend\"] + daily[\"dayofweek_trend\"].mean() ) # Plot crossings and trend, and detrended data fix, ax = plt.subplots(1, 2, figsize=(15, 5)) daily[[\"total\", \"dayofweek_trend\"]].plot(ax=ax[0]) daily[\"dayofweek_corrected\"].plot(ax=ax[1]) ax[0].set_ylabel(\"Daily crossing\") ax[0].set_title(\"Total daily crossings and trend\") ax[1].set_ylabel(\"Adjusted daily crossings\") ax[1].set_title(\"Detrended daily crossings\") print_rms(daily[\"dayofweek_corrected\"]) Root-mean-square about trend: 652 riders ","date":"2020-02-07","objectID":"/seattle-bicycle-traffic/:2:2","tags":["datascience"],"title":"Seattle bicycle traffic","uri":"/seattle-bicycle-traffic/"},{"categories":null,"content":"Accounting for rainfall and temparature # Read in weather data weather = pd.read_csv( \"seattle_weather_SeaTacWeather.csv\", index_col=\"DATE\", parse_dates=True, usecols=[2, 5, 9, 10], ) weather = weather.loc[start:end] weather.columns = map(str.lower, weather.columns) # Temparatures are in 1/10 deg F; convert to deg C weather[\"tmax\"] = (weather[\"tmax\"] - 32) * 5 / 9 weather[\"tmin\"] = (weather[\"tmin\"] - 32) * 5 / 9 # Rainfall is in inches; convert to mm weather[\"prcp\"] *= 25.4 weather[\"tmax\"].resample(\"w\").max().plot() weather[\"tmin\"].resample(\"w\").min().plot() plt.title(\"Temperature extremes in Seattle\") plt.ylabel(\"Weekly temperature extremes (C)\"); weather[\"prcp\"].resample(\"w\").sum().plot() plt.title(\"Precipitation in Seattle\") plt.ylabel(\"Weekly precipitation in Seattle (mm)\"); # Combine daily and weather dataset daily = daily.join(weather) # Detrend data including weather information columns = days + [\"daylight\", \"tmax\", \"tmin\", \"prcp\"] X = daily[columns] y = daily[\"total\"] clf = LinearRegression().fit(X, y) daily[\"overall_trend\"] = clf.predict(X) daily[\"overall_corrected\"] = ( daily[\"total\"] - daily[\"overall_trend\"] + daily[\"overall_trend\"].mean() ) # Plot crossings and trend, and detrended data fix, ax = plt.subplots(1, 2, figsize=(15, 5)) daily[[\"total\", \"overall_trend\"]].plot(ax=ax[0]) daily[\"overall_corrected\"].plot(ax=ax[1]) ax[0].set_ylabel(\"Daily crossing\") ax[0].set_title(\"Total daily crossings and trend\") ax[1].set_ylabel(\"Adjusted daily crossings\") ax[1].set_title(\"Detrended daily crossings\") print_rms(daily[\"overall_corrected\"]) Root-mean-square about trend: 457 riders # Plot rolling 30 day average daily[\"overall_corrected\"].rolling(30, center=True).mean().plot() plt.title(\"1-month moving average\"); ","date":"2020-02-07","objectID":"/seattle-bicycle-traffic/:2:3","tags":["datascience"],"title":"Seattle bicycle traffic","uri":"/seattle-bicycle-traffic/"},{"categories":null,"content":"Accounting for a steady increase in riders daily[\"daycount\"] = np.arange(len(daily)) columns = days + [\"daycount\", \"daylight\", \"tmax\", \"tmin\", \"prcp\"] X = daily[columns] y = daily[\"total\"] final_model = LinearRegression().fit(X, y) daily[\"final_trend\"] = final_model.predict(X) daily[\"final_corrected\"] = ( daily[\"total\"] - daily[\"final_trend\"] + daily[\"final_trend\"].mean() ) # Plot crossings and trend, and detrended data fix, ax = plt.subplots(1, 2, figsize=(15, 5)) daily[[\"total\", \"final_trend\"]].plot(ax=ax[0]) daily[\"final_corrected\"].plot(ax=ax[1]) ax[0].set_ylabel(\"Daily crossing\") ax[0].set_title(\"Total daily crossings and trend\") ax[1].set_ylabel(\"Adjusted daily crossings\") ax[1].set_title(\"Detrended daily crossings\") print_rms(daily[\"final_corrected\"]) Root-mean-square about trend: 451 riders ","date":"2020-02-07","objectID":"/seattle-bicycle-traffic/:2:4","tags":["datascience"],"title":"Seattle bicycle traffic","uri":"/seattle-bicycle-traffic/"},{"categories":null,"content":"What can the final model tell us? # Compute error variance vy = np.sum((y - daily[\"final_trend\"]) ** 2) / len(y) X2 = np.hstack([X, np.ones((X.shape[0], 1))]) C = vy * np.linalg.inv(np.dot(X2.T, X2)) var = C.diagonal() How does rain affect ridership? ind = columns.index(\"prcp\") slope = final_model.coef_[ind] error = np.sqrt(var[ind]) print( \"{0: .0f} +/- {1: .0f} daily crossings lost per cm of rain\".format( -slope * 10, error * 10 ) ) 331 +/- 28 daily crossings lost per cm of rain The model shows that for every cm of rain, about 300 cyclists stay home or use another mode of transport. How does temparature affect ridership? ind1, ind2 = columns.index(\"tmin\"), columns.index(\"tmax\") slope = final_model.coef_[ind1] + final_model.coef_[ind2] error = np.sqrt(var[ind1] + var[ind2]) print( \"{0:.0f} +/- {1:.0f} riders per ten degrees Celsius\".format(10 * slope, 10 * error) ) 493 +/- 102 riders per ten degrees Celsius How does daylight affect ridership? ind = columns.index(\"daylight\") slopt = final_model.coef_[ind] error = np.sqrt(var[ind]) print(\"{0:.0f} +/- {1:.0f} riders per hour of daylight\".format(slope, error)) 49 +/- 12 riders per hour of daylight Is ridership increasing? ind = columns.index(\"daycount\") slope = final_model.coef_[ind] error = np.sqrt(var[ind]) print(\"{0:.2f} +/- {1:.2f} new riders per day\".format(slope, error)) print(\"{0:.1f} +/- {1:.1f} new riders per week\".format(7 * slope, 7 * error)) print( \"annual change: ({0:.0f} +/- {1:.0f})%\".format( 100 * 365 * slope / daily[\"total\"].mean(), 100 * 365 * error / daily[\"total\"].mean(), ) ) 0.43 +/- 0.11 new riders per day 3.0 +/- 0.8 new riders per week annual change: (7 +/- 2)% ","date":"2020-02-07","objectID":"/seattle-bicycle-traffic/:2:5","tags":["datascience"],"title":"Seattle bicycle traffic","uri":"/seattle-bicycle-traffic/"},{"categories":null,"content":" This notebook replicates Jake VanderPlas‚Äô awesome post on the topic. ","date":"2020-01-14","objectID":"/waiting-time-paradox/:0:0","tags":["datascience"],"title":"Waiting time paradox","uri":"/waiting-time-paradox/"},{"categories":null,"content":"Background The waiting-time paradox is a special case of the inspection paradox, which ‚Äì as VanderPlas succinctly summarises ‚Äì occurs whenever the probability of observing a quantity is related to the quantity being observed. For example: if you sample random students on campus and ask them about the size of their classes you‚Äôll probably get a larger number than if you asked the collge administrator, because you‚Äôre likely to oversample students from large classes. Similarly, you are more likely to arrive at a bus stop during a longer waiting time simply because the waiting time is longer. However, the waiting time paradox claims not only that the experienced waiting time is longer than the average waiting time, but that it is twice as long. ","date":"2020-01-14","objectID":"/waiting-time-paradox/:1:0","tags":["datascience"],"title":"Waiting time paradox","uri":"/waiting-time-paradox/"},{"categories":null,"content":"Simulating wait times import random import matplotlib.pyplot as plt import numpy as np import pandas as pd import seaborn as sns from scipy.stats import norm, poisson plt.style.use(\"seaborn\") Helper functions def simulate_bus_arrivals(n=1000000, tau=10, rseed=231296): \"\"\" Simulates the arrival of n buses that are scheduled to arrive every tau minutes. \"\"\" np.random.RandomState(rseed) return n * tau * np.sort(np.random.rand(n)) def simulate_wait_times(arrival_times, n=1000000, rseed=231286): \"\"\" Calculate the waiting time for each arriving passenger. \"\"\" # Simulate customer arrival times (between 0 and arrival of last bus) np.random.RandomState(rseed) arrival_times = np.array(arrival_times) passenger_times = arrival_times.max() * np.sort(np.random.rand(n)) # Find the index of the next bus for each simulated customer i = np.searchsorted(arrival_times, passenger_times, side=\"right\") return arrival_times[i] - passenger_times # Create bus arrival times and check that frequency is about tau bus_arrival_times = simulate_bus_arrivals(n=1000000, tau=10, rseed=231286) intervals = np.diff(bus_arrival_times) intervals.mean() 9.9999994693088 # Calculate passenger waiting times wait_times = simulate_wait_times(bus_arrival_times) wait_times.mean() 10.00276398808929 This is in line with the paradox: if buses arrive every 10 minutes on average, the the average wait time is not 5 minutes, but 10 mintues. ","date":"2020-01-14","objectID":"/waiting-time-paradox/:2:0","tags":["datascience"],"title":"Waiting time paradox","uri":"/waiting-time-paradox/"},{"categories":null,"content":"Digging deeper To model the above result analytically, let‚Äôs model the process we generated to calculate waiting times and then derive the expected waiting time. For a start, we can write the expected value of intervals $T$ between bus arrivals as $$ E(T) = \\int_{0}^{\\infty} Tp(T)dT $$. In the simulation above, we set $E(T) = \\tau = 10$ minutes. What we‚Äôre after, however, is the expected value of the waiting time for a rider who arrives at the bus stop. To get this, we first model the distribution of experienced waiting times for customers, and then recognise that the expected value of the waiting time is half the expected value for the experienced waiting time. From the inspection paradox, we know that the experienced waiting time depends on the actual intervals between buses, $p(T)$ as well as on the length of the intervals, $T$. So we can write $$ p_{exp}(T) \\propto Tp(T)$$. Substitute the constant of proportionality for the proportional sign seemed obvious for jvdp, but was less so for me, so I‚Äôm gonna back up and explain, based on this super helpful resource. If $ p_{exp}(T) Tp(T)$, then we know that there exists a constant of proportionality $k$ such that $$ p_{exp}(T) = kTp(T)$$. Because $p_{exp}(T)$ is a density, we have $\\int_0^\\infty p_{exp}(T)dT = 1$, which means that $$\\int_0^\\infty kTp(T)dT = 1$$ $$k\\int_0^\\infty Tp(T)dT = 1$$ $$k = \\left[\\int_0^\\infty kTp(T)dT\\right]^{-1}$$ Using this in our expression above, we get $$p_{exp}(T) = \\frac{Tp(T)}{\\int_0^\\infty Tp(T)dT}$$. And using the definition of $E(T)$ above, this simplifies to $$p_{exp}(T) = \\frac{Tp(T)}{E(T)}$$. To find the expected waiting time $E(W)$, the final step is use the fact that the expected value for the observed interval is half the expected interval (if riders experience buses at arriving every 20 minutes, then their expected waiting time is 10 minutes). We can thus write: $$E(W) = \\frac{1}{2} E_{exp}(T) = \\frac{1}{2} \\int_{0}^{\\infty} Tp_{exp}(T)dT$$. Using our expression from above and rewriting gives $$ E(W) = \\frac{1}{2} \\int_{0}^{\\infty} T\\frac{Tp(T)}{E(T)}dT E(W) = \\frac{1}{2E(T)} \\int_{0}^{\\infty} T^2p(T)dT E(W) =\\frac{E(T^2)}{2E(T)} $$. What we now need to do is to find a form for $p(T)$ and compute our integrals. ","date":"2020-01-14","objectID":"/waiting-time-paradox/:3:0","tags":["datascience"],"title":"Waiting time paradox","uri":"/waiting-time-paradox/"},{"categories":null,"content":"Choosing p(T) # Plot distribution of simulated intervals plt.hist(intervals, bins=np.arange(80), alpha=0.5, density=True) plt.axvline(intervals.mean(), color=\"black\", linestyle=\"dotted\") plt.xlabel(\"Interval between arrivals (minutes)\") plt.ylabel(\"Probability density\"); The distribution of intervals is clearly exponential. The original post describes in some more detail why this was to be expected given that we sampled arrival times from a uniform distribution. The short answer is: random selection from a uniform distribution approximates a poisson process, and the distribution of intervals of a poisson process is exponential. Let‚Äôs thus check whether our arrival times do indeed follow a poisson distribution. # Count number of arrivals for each 1-hour interval binsize = 60 binned_counts = np.bincount((np.sort(bus_arrival_times) // binsize).astype(\"int\")) # Plot results and compare with poisson distribution x = np.arange(20) tau = 10 mu = binsize / tau plt.hist( binned_counts, bins=x - 0.5, density=True, alpha=0.5, label=\"Simulation\", ) plt.plot(x, poisson.pmf(x, mu), \"ok\", label=\"Poisson prediction\") plt.legend(); It‚Äôs clear that the poisson distribution approximates our distribution of simulated arrivals very well. This means that we can write the probability distribution of our intervals as ‚Ä¶ ‚Ä¶ jvdp gets $p(T) = \\frac{1}{\\tau}e^{-T/\\tau}$ I can‚Äôt see where this is coming from. Following the logic of this post, I get $p(T) = -e^{T/\\tau}$ ","date":"2020-01-14","objectID":"/waiting-time-paradox/:4:0","tags":["datascience"],"title":"Waiting time paradox","uri":"/waiting-time-paradox/"},{"categories":null,"content":"Waiting times in reality # Load data df = pd.read_csv(\"./data/bus_paradox_arrival_times.csv\") df = df.dropna(axis=0, how=\"any\") df.head() OPD_DATE VEHICLE_ID RTE DIR TRIP_ID STOP_ID STOP_NAME SCH_STOP_TM ACT_STOP_TM 0 2016-03-26 6201 673 S 30908177 431 3RD AVE \u0026 PIKE ST (431) 01:11:57 01:13:19 1 2016-03-26 6201 673 S 30908033 431 3RD AVE \u0026 PIKE ST (431) 23:19:57 23:16:13 2 2016-03-26 6201 673 S 30908028 431 3RD AVE \u0026 PIKE ST (431) 21:19:57 21:18:46 3 2016-03-26 6201 673 S 30908019 431 3RD AVE \u0026 PIKE ST (431) 19:04:57 19:01:49 4 2016-03-26 6201 673 S 30908252 431 3RD AVE \u0026 PIKE ST (431) 16:42:57 16:42:39 ","date":"2020-01-14","objectID":"/waiting-time-paradox/:5:0","tags":["datascience"],"title":"Waiting time paradox","uri":"/waiting-time-paradox/"},{"categories":null,"content":"Data cleanup # Combine time and data into single timestamp df[\"actual\"] = pd.to_datetime(df[\"OPD_DATE\"] + \" \" + df[\"ACT_STOP_TM\"]) df[\"scheduled\"] = pd.to_datetime(df[\"OPD_DATE\"] + \" \" + df[\"SCH_STOP_TM\"]) # Calculate dalay in minutes, adjusting actual day when actual and scheduled span midnight minute = np.timedelta64(1, \"m\") hour = 60 * minute diff_hrs = (df[\"actual\"] - df[\"scheduled\"]) / hour df.loc[diff_hrs \u003c -22, \"actual\"] += 2 * hour df.loc[diff_hrs \u003e 22, \"actual\"] -= 2 * hour df[\"minutes_late\"] = (df[\"actual\"] - df[\"scheduled\"]) / minute # Map internal route codes to external route letters df[\"route\"] = df[\"RTE\"].replace({673: \"C\", 674: \"D\", 675: \"E\"}).astype(\"category\") df[\"direction\"] = ( df[\"DIR\"].replace({\"N\": \"northbound\", \"S\": \"southbound\"}).astype(\"category\") ) # Extract useful columns df = df[[\"route\", \"direction\", \"scheduled\", \"actual\", \"minutes_late\"]] df.head() route direction scheduled actual minutes_late 0 C southbound 2016-03-26 01:11:57 2016-03-26 01:13:19 1.366667 1 C southbound 2016-03-26 23:19:57 2016-03-26 23:16:13 -3.733333 2 C southbound 2016-03-26 21:19:57 2016-03-26 21:18:46 -1.183333 3 C southbound 2016-03-26 19:04:57 2016-03-26 19:01:49 -3.133333 4 C southbound 2016-03-26 16:42:57 2016-03-26 16:42:39 -0.300000 df.info() \u003cclass 'pandas.core.frame.DataFrame'\u003e Int64Index: 38917 entries, 0 to 39156 Data columns (total 5 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 route 38917 non-null category 1 direction 38917 non-null category 2 scheduled 38917 non-null datetime64[ns] 3 actual 38917 non-null datetime64[ns] 4 minutes_late 38917 non-null float64 dtypes: category(2), datetime64[ns](2), float64(1) memory usage: 1.3 MB ","date":"2020-01-14","objectID":"/waiting-time-paradox/:5:1","tags":["datascience"],"title":"Waiting time paradox","uri":"/waiting-time-paradox/"},{"categories":null,"content":"How late are buses? We can look at the direction of each route separately to get a sense of the distribution of delays. We can‚Äôt know this from looking at the data, but the routes are such that the southbound trip for C, and the northbound trip for D and E are near the beginning of the line (i.e.¬†C runs from north to south and back to noth). As jvdp notes, we‚Äôd expect buses to keep more closely to their schedules early in the trip, which is precisely what we can see, which is pretty cool! g = sns.FacetGrid(df, row=\"direction\", col=\"route\") g.map(plt.hist, \"minutes_late\", bins=np.arange(-10, 20)) for ax in g.axes.flat: ax.axvline(0, color=\"k\", linestyle=\"dotted\") g.set_titles(\"{col_name} {row_name}\") g.set_axis_labels(\"Minutes late\", \"Number of buses\"); ","date":"2020-01-14","objectID":"/waiting-time-paradox/:6:0","tags":["datascience"],"title":"Waiting time paradox","uri":"/waiting-time-paradox/"},{"categories":null,"content":"Scheduled and observed arrival intervals # Calculate actual and scheduled intervals def compute_interval(series): minute = np.timedelta64(1, \"m\") return series.sort_values().diff() / minute grouped = df.groupby([\"route\", \"direction\"]) df[\"actual_interval\"] = grouped[\"actual\"].transform(compute_interval) df[\"scheduled_interval\"] = grouped[\"scheduled\"].transform(compute_interval) # Plot actual intervals g = sns.FacetGrid(df, row=\"direction\", col=\"route\") g.map(plt.hist, \"actual_interval\", bins=np.arange(50) + 0.5) g.set_titles(\"{col_name} {row_name}\") g.set_axis_labels(\"Actual intervals (minutes)\", \"Number of buses\"); # Plot scheduled intervals g = sns.FacetGrid(df, row=\"direction\", col=\"route\") g.map(plt.hist, \"scheduled_interval\", bins=np.arange(20) - 0.5) g.set_titles(\"{col_name} {row_name}\") g.set_axis_labels(\"Scheduled intervals (minutes)\", \"Number of buses\"); While actual intervals are clearly not distributed exponentially, the plot of scheduled intervals shows that this might be the case because there are a variety of different scheduled intervals (e.g.¬†it‚Äôs not the case that all buses are scheduled to arrive in 10 minute intervals). Instead, jvdp proposes a clever workaround: group arrivals by route, direction, and scheduled interval, and then stack these observations together as if they happened in sequence. This preserves the properties of the data, while giving us a situation analogous to our simulation above: a series of buses scheduled to arrive at constant intervals. # Check for most frequent intervals df[\"scheduled_interval\"].value_counts().head(3) 15.0 7367 12.0 7062 10.0 4879 Name: scheduled_interval, dtype: int64 def stacked_sequence(data): # Sort by scheduled arrival time data = data.sort_values(\"scheduled\") # Restack data and compute needed variables data[\"scheduled\"] = data[\"scheduled_interval\"].cumsum() data[\"actual\"] = data[\"scheduled\"] + data[\"minutes_late\"] data[\"actual_interval\"] = data[\"actual\"].diff() return data subset = df[df[\"scheduled_interval\"].isin([\"10\", \"15\"])] grouped = subset.groupby([\"route\", \"direction\", \"scheduled_interval\"]) sequenced = grouped.apply(stacked_sequence).reset_index(drop=True) sequenced.head() route direction scheduled actual minutes_late actual_interval scheduled_interval 0 C northbound 10.0 12.400000 2.400000 NaN 10.0 1 C northbound 20.0 27.150000 7.150000 14.750000 10.0 2 C northbound 30.0 26.966667 -3.033333 -0.183333 10.0 3 C northbound 40.0 35.516667 -4.483333 8.550000 10.0 4 C northbound 50.0 53.583333 3.583333 18.066667 10.0 for route in [\"C\", \"D\", \"E\"]: data = sequenced.query(f\"route == '{route}'\") g = sns.FacetGrid(data, row=\"direction\", col=\"scheduled_interval\") g.map(plt.hist, \"actual_interval\", bins=np.arange(-20, 40) - 0.5) g.set_titles(\"{row_name} ({col_name:.0f} mins)\") g.set_axis_labels(\"Actual interval (min)\", \"Number of buses\") g.fig.set_size_inches(8, 4) g.fig.suptitle(f\"{route} line\", y=1.05, fontsize=14) It‚Äôs very clear that arrival intervals do not follow an exponential distribution. Instead, they are almost perfectly normal and peak near the point of their scheduled interval time. ","date":"2020-01-14","objectID":"/waiting-time-paradox/:7:0","tags":["datascience"],"title":"Waiting time paradox","uri":"/waiting-time-paradox/"},{"categories":null,"content":"Simulate waiting times grouped = sequenced.groupby([\"route\", \"direction\", \"scheduled_interval\"]) sims = grouped[\"actual\"].apply(simulate_wait_times) sims.apply(lambda times: \"{0:.1f} +/- {1:.1f}\".format(times.mean(), times.std())) route direction scheduled_interval C northbound 10.0 24.8 +/- 150.5 15.0 12.9 +/- 73.2 southbound 10.0 6.2 +/- 6.0 15.0 10.2 +/- 49.5 D northbound 10.0 5.9 +/- 6.7 15.0 7.9 +/- 5.3 southbound 10.0 238.6 +/- 303.8 15.0 144.5 +/- 252.1 E northbound 10.0 27.4 +/- 113.0 15.0 7.9 +/- 4.9 southbound 10.0 82.6 +/- 199.6 15.0 54.0 +/- 184.5 Name: actual, dtype: object ","date":"2020-01-14","objectID":"/waiting-time-paradox/:8:0","tags":["datascience"],"title":"Waiting time paradox","uri":"/waiting-time-paradox/"},{"categories":null,"content":"Unsolved issues As indicated above, I still can‚Äôt see how jvdp gets to $p(T) = \\frac{1}{\\tau}e^{-T/\\tau}$ for the distribution of intervals. Leaving this for now. There is a problem in the code when I include 12 minute intervals. The output below shows len(arrival_times) and max(i) for each subset of the data. For some myserious reason, C northbound 12 mins interval have a customer for which the next bus is 1234, which is the bus after the last bus, which is not possible. I can‚Äôt figure out how this comes about. So ‚Äì after trying to figure this out for an hour now ‚Äì I‚Äôm gonna leave it for now. grouped = sequenced.groupby([\"route\", \"direction\", \"scheduled_interval\"]) grouped[\"actual\"].apply(simulate_wait_times) route direction scheduled_interval C northbound 10.0 (349, 348) 12.0 (1434, 1434) 15.0 (1377, 1376) southbound 10.0 (483, 482) 12.0 (1312, 1311) 15.0 (1453, 1452) D northbound 10.0 (441, 440) 12.0 (1413, 1412) 15.0 (1507, 1506) southbound 10.0 (395, 394) 12.0 (1300, 1299) 15.0 (1391, 1390) E northbound 10.0 (1869, 1868) 12.0 (516, 515) 15.0 (590, 589) southbound 10.0 (1342, 1341) 12.0 (1087, 1086) 15.0 (1049, 1048) Name: actual, dtype: object ","date":"2020-01-14","objectID":"/waiting-time-paradox/:9:0","tags":["datascience"],"title":"Waiting time paradox","uri":"/waiting-time-paradox/"},{"categories":null,"content":"Cool stuff picked up while working through this # Simulate arrivals and compare interval distribution to an exponential distribution import scipy.stats as stats N = 1000000 x = np.linspace(0, 10, 100) diffs = np.diff(np.sort(N * np.random.rand(N))) plt.hist(diffs, bins=x, density=True, alpha=0.5) plt.plot(x, stats.expon.pdf(x, scale=1), \"-k\"); # Playing around with poisson and visualising normal approximation to poisson fig, ax = plt.subplots(1, 2, figsize=(18, 6)) x = np.arange(20) for l in range(1, 8): ax[0].plot(x, poisson.pmf(x, l), \"-o\", label=f\"Lambda = {l}\") ax[0].legend() x = np.arange(1, 50) end = 30 for l in range(1, end): ax[1].plot(x, poisson.pmf(x, l), \"-o\", alpha=0.5, label=f\"Lambda ={l}\") ax[1].plot(x, norm.pdf(x, end, np.sqrt(end)), color=\"b\", linewidth=4) ","date":"2020-01-14","objectID":"/waiting-time-paradox/:10:0","tags":["datascience"],"title":"Waiting time paradox","uri":"/waiting-time-paradox/"},{"categories":null,"content":"##¬†Setup We‚Äôre interested in estimating $P(L|features)$, the probability of a label given a collection of features. Bayes‚Äôs formula helps us express this in terms of things we can measure in the data as: $$P(L|features) = \\frac{P(features|L) * P(L)}{P(features)}$$ To decide which label is more probable given a set of features, we can compute the posterior ratio of likelihoods: $$\\frac{P(L_1|features)}{P(L_2|features)} = \\frac{P(features|L_1) * P(L_1)}{P(features|L_2) * P(L_2)}$$. To be able to do this, we need to specify a generative model (a hypothetical random process that generates the data) for each label in the data. Generating these models is the main task of training a Bayesian classifier. A naive Bayesian classifier comes about by making naive assumptions about the nature of these models, which can, in principle, be quite complex. ","date":"2019-11-23","objectID":"/naive-bayes/:0:0","tags":["datascience"],"title":"Naive Bayes","uri":"/naive-bayes/"},{"categories":null,"content":"Gaussian Naive Bayes A simple approach is to assume that data from each label is drawn from a simple Gaussian distribution (with no covariance between them). We can then estimate the mean and standard deviation for each set of labelled points, which is all we need to estimate the distribution. Once we have the distribution, we can calculate the likelihood of being drawn from the distribution for each new data-point, and thus compute the posterior ratio above. import matplotlib.pyplot as plt import numpy as np import seaborn as sns from sklearn.datasets import make_blobs from sklearn.model_selection import train_test_split sns.set() X, y = make_blobs(500, 2, centers=2, random_state=1223, cluster_std=1.5) Xtrain, Xtest, ytrain, ytest = train_test_split(X, y) plt.scatter(Xtrain[:, 0], Xtrain[:, 1], c=ytrain, cmap=\"Paired\"); from sklearn.naive_bayes import GaussianNB model = GaussianNB().fit(Xtrain, ytrain) # New data to show decision boundary rng = np.random.RandomState(1223) Xnew = [-5, -14] + [20, 20] * rng.rand(2000, 2) ynew = model.predict(Xnew) plt.scatter(Xtrain[:, 0], Xtrain[:, 1], c=ytrain, cmap=\"Paired\", label=\"Training\") plt.scatter(Xtest[:, 0], Xtest[:, 1], c=ytest, marker=\"D\", label=\"Testing\") lim = plt.axis() plt.scatter(Xnew[:, 0], Xnew[:, 1], c=ynew, s=50, cmap=\"Paired\", alpha=0.1) plt.axis(lim) plt.legend() plt.title(\"Training and testing data with decision boundary\"); ","date":"2019-11-23","objectID":"/naive-bayes/:1:0","tags":["datascience"],"title":"Naive Bayes","uri":"/naive-bayes/"},{"categories":null,"content":"Sources Python Data Science Handbook ","date":"2019-11-23","objectID":"/naive-bayes/:2:0","tags":["datascience"],"title":"Naive Bayes","uri":"/naive-bayes/"},{"categories":null,"content":" Trees Are an easy and intuitive way to split data within a sample.\u2028Problem, they are not good at predicting out-of-sample (with different datasets)\u2028Random Forests\u2028Random forests remedy this by combining the simplicity of decision trees with flexibility, which leads to a large improvement in predictive accuracy.\u2028How to make a random forest:\u2028Create bootstrapped sample from data (i.e.¬†sample observations of the original sample with replacement)\u2028Create a decision tree using only a subset of randomly selected variables at each step (e.g.¬†only 2 out of 4 for root, then 2 out of remaining 3 at next node, ect.)\u2028Repeat above two steps many times (e.g.¬†1000) to build many trees (and build a random forest)\u2028To predict outcome for new observation, do the following:\u2028Feed data into each tree in the forest and keep score of the predictions (either Yes or No for each tree). The outcome with the most scores is the prediction.\u2028The process is called ‚ÄúBagging‚Äù because we Bootstrap the data and rely on the AGGregate to make a decision.\u2028How can we test how good a tree is at out-of sample prediction without having another sample?\u2028Bootstrapping relies on randomly sampling from data with replacement, hence, not all observations will be used to create a tree.\u2028The unused observations are called the ‚ÄúOut-of-bag Dataset‚Äù. We can use these test whether our Forest is any good at predicting.\u2028We simply take the out-of-bag dataset from each tree, run through the entire Forest and check whether the Forest accurately classifies the observation. We then repeat this for each out-of-bag dataset.\u2028The proportion of incorrectly classified out-of-bag samples is called the ‚Äúout-of-bag error‚Äù.\u2028The out-of-bag error is what helps us determine how many variables to use when building our random trees above. The algorithm builds different forests with different numbers of variables (typically starting with the square-root of the total number of variables ‚Äì e.g.¬†2 if we have 4 variables ‚Äì and then calculating a few above and below that) and then picks the one with the smallest out-of-bag error.¬†Ada boosts\u2028When building random forests, trees vary in their depth.\u2028When using Ada boost to create a Random Forest, each tree is usually just one node and two leaves. (A tree with one node and two leaves is a ‚Äústump‚Äù). So, Ada boost produces a Forest of Stumps.\u2028Because a stump only makes use of a single variable, they are generally poor predictors.\u2028Main ideas of ada boost\u2028Take Forest of Stumps\u2028Stumps have different weights (mounts of say) in the calculation of the out-of-bag error (with the weights being proportional to the gravity of the prediction errors they make. Loosely speaking, for how many observations they get the prediction wrong).¬†Each stump takes the errors of the previous stump into account (it does this by treating as more important those observations that the previous stump misclassified).¬†Process\u2028Create first Stump using the variable that best classifies outcomes\u2028Then calculate classification error\u2028The size of that error determines the weight this stump gets in the overall classification (i.e.¬†in the Forest of Stumps).\u2028The next stump will be build using the variable that best classifies outcomes in a dataset that over-emphasizes the observations that the previous stump misclassified.\u2028As in a Random Forst, we run all obseravtions through all Stumps and keep track of the classification. Instead of adding up the Yes and No, we add up the amount of say of the Yes Stumps and No Stumps and classify as Yes if total amount of say of yes Stumps is larger.\u2028Gradient boosting (most used configuration)\u2028Comparison to Ada boost\u2028Like Ada boost builds fixed size trees, but they can be larger than a Stump (in our specification, we use trees with a depth of 5)\u2028GB also scales trees, but all by same amount\u2028Also builds tree based on error of previous tree\u2028Algorithm\u2028Predict based on average and calculate (pseudo residuals)\u2028Then build a tree to predict residuals\u2028Scale predicted residual by the ","date":"2019-11-21","objectID":"/tree-based-methods/:0:0","tags":["stats"],"title":"Tree-based methods","uri":"/tree-based-methods/"},{"categories":null,"content":"Sources Brilliant series of videos on StatQuest ","date":"2019-11-21","objectID":"/tree-based-methods/:1:0","tags":["stats"],"title":"Tree-based methods","uri":"/tree-based-methods/"},{"categories":null,"content":"Research Levelling Down and the COVID-19 Lockdowns: Uneven Regional Recovery in UK Consumer Spending (with John Gathergood, Benedict Guttman-Kenney, Edika Quispe-Torreblanca, and Neil Stewart) CEPR Covid Economics, vol. 67, pp. 24-52. pdf Matching in the Corporate Loan Market MPhil thesis at the University of Oxford, 2017. pdf It‚Äôs Politics, Stupid! Political Constraints Determined Governments‚Äô Reactions to the Great Recession (with Jan-Egbert Sturm ‚Äì this is a version of my undergraduate thesis) Kyklos, 2016, vol. 69(4), pp. 584-603. pdf | online appendix | Reprint in IMF Fiscal Politics book Essays on the Political Consequences of Unemployment MSc thesis at the University of Bern, 2015. pdf ","date":"0001-01-01","objectID":"/research/:0:0","tags":null,"title":"","uri":"/research/"}]